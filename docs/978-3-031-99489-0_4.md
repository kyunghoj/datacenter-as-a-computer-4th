# 4장. WSC 소프트웨어: 분산 시스템 인프라

## 4.1 소프트웨어 스택 개요

이전 장에서 우리는 창고 규모 컴퓨터(WSC)에서 실행되는 애플리케이션과 그것이 시스템 트레이드오프 및 설계 결정에 미치는 영향에 초점을 맞췄습니다. 이번 장에서는 스택의 하위 부분에 초점을 맞출 것입니다. 간단한 요약으로, **그림 4.1**은 다양한 계층을 보여줍니다.

## 4.2 분산 시스템

단일 시스템에서 운영 체제 계층은 자원을 관리하고 해당 단일 컴퓨터에 기본 서비스를 제공합니다. 이와 마찬가지로, 수천 대의 컴퓨터, 네트워킹, 스토리지로 구성된 WSC 시스템 또한 더 큰 규모에서 유사한 기능을 제공하는 소프트웨어가 필요합니다. 이 섹션에서는 클러스터 수준 시스템, 스토리지, WAN 및 엣지(edge) 네트워크와 같은 해당 소프트웨어의 가장 중요한 구성 요소에 대해 논의합니다.

### 4.2.1 클러스터 수준 인프라

많은 WSC는 글로벌 네트워크로 연결되어 글로벌 서비스를 제공합니다. 클러스터는 하나의 단위로 관리되는 가장 작은 도메인을 나타냅니다. 클러스터(때로는 셀(cell)이라고도 함)는 Jupiter(6장)와 같은 단일 고대역폭 클러스터 네트워크로 연결된 머신들로 구성됩니다. 일반적으로 클러스터는 단일 데이터 센터 홀이나 건물(즉, 10-30 MW 전력)에 위치한 수만 대의 서버를 포함합니다. 클러스터 내의 모든 서버는 단일 제어 평면(control plane)을 가진 동일한 네트워크에 연결되어 있으므로, 동시에 모두 사용 불가능해질 가능성이 있습니다.

**그림 4.1 창고 규모 컴퓨터를 위한 Google 소프트웨어 스택**

클러스터는 존(Zone, 2장 참조)보다 하위 수준인데, 하나의 존이 여러 건물에 걸쳐 있는 여러 클러스터를 포함할 수 있기 때문입니다. 예를 들어, 새로운 존은 단일 클러스터로 시작하여 시간이 지남에 따라 추가 클러스터를 더할 수 있습니다.

#### 4.2.1.1 자원 관리 (Resource Management)

자원 관리는 아마도 클러스터 수준 인프라 계층에서 가장 필수 불가결한 구성 요소일 것입니다. 이는 사용자와 작업을 하드웨어 자원에 매핑하는 것을 제어하고, 우선순위와 할당량을 집행하며, 기본적인 태스크 관리 서비스를 제공합니다. 자원 관리에는 네 가지 뚜렷한 책임이 있습니다.

*   **하드웨어 관리(Hardware management)**는 하드웨어를 추가, 업그레이드, 수리 및 제거할 수 있게 합니다. 하드웨어 자원을 효과적으로 관리하려면 재고 관리(예: 현장에 무엇이 있는지, 어디에 있는지, 내부에 어떤 구성 요소가 있는지), 상태 관리(예: 하드웨어 전원이 켜져 있고 건강한지 등), 워크플로 도구(예: 기술자를 서버로 안내하고, 교체할 부품을 식별하고, 구형 및 신형 부품의 바코드를 기록하는 등)가 필요합니다.
*   **머신 상태 관리(Machine state management)**는 펌웨어 이미지, 하드웨어 구성 매개변수(예: 클럭 속도 설정), 장치 ID(예: 인증서)와 같이 각 장치에 로컬로 저장된 비트를 다룹니다.
*   **작업 관리(Job management)**는 워크로드를 개별 머신에 스케줄링하고 공급(예: 머신을 사용할 수 없게 됨)이나 수요(예: 작업이 추가 부하를 처리하기 위해 확장하려 함)의 변화에 대응합니다. 사용자에게 제공되는 추상화 수준에 따라 스케줄러는 원시 머신(베어 메탈 서버), 가상 머신(일반적으로 부분적인 머신 점유), 또는 컨테이너(가상 머신조차도 추상화함)를 할당할 수 있습니다.
*   **용량 관리(Capacity management)**는 자원 관리를 물리적 공급망으로 확장하여, 너무 많이 보유하는 비용(유휴 자원에 대한 비용 지불)과 너무 적게 보유하는 비용(중요 자원의 재고 부족으로 인한 애플리케이션 영향) 간의 균형을 맞춥니다. 1초 미만의 할당 결정을 내리는 작업 관리와 달리, 용량 관리는 하드웨어 자원의 리드 타임(lead time)에 따라 며칠에서 몇 달까지의 기간으로 운영됩니다.

가장 단순한 형태(소규모 기업에서는 여전히 실행됨)에서는 자원이 특정 사용자나 작업에 수동으로 정적으로 할당되며, 이는 일반적으로 낮은 활용률로 이어집니다. 반면, WSC 자원 관리는 필요성(수백만 개의 자원을 수동으로 추적하는 것은 불가능함)과 기회(1%의 효율성 증가가 수백만 달러의 절감을 나타낼 수 있음)에 의해 고도로 자동화되어야 합니다.

Google의 Borg 스케줄러[1], [2]는 이질적인 서버 풀에 작업을 스케줄링합니다. 각 작업(job)은 여러 태스크(task)로 구성된 서비스(예: 검색 백엔드)를 나타냅니다. 각 태스크는 단일 머신에서 실행되는 단일 바이너리(함께 제공되는 구성 파일 포함)를 나타냅니다. 따라서 검색 백엔드 작업을 스케줄링하는 일은 검색 인덱스의 샤드(shard)를 처리하는 수천 개의 태스크를 스케줄링하는 것으로 구성될 수 있습니다. 모든 머신은 공유되므로 태스크 스케줄링은 내부 단편화(internal fragmentation), 즉 좌초된(stranded) 자원을 최소화하면서 머신 풀에 모든 태스크(모든 작업의)를 스케줄링하는 것을 목표로 하는 빈 패킹(bin packing) 문제가 됩니다. 예를 들어, 태스크가 32코어와 40GB 메모리를 필요로 하는데, 32개의 유휴 코어와 100GB 메모리가 남은 서버에 이를 스케줄링하는 것은 바람직하지 않습니다. 왜냐하면 이렇게 하면 남은 60GB 메모리가 좌초되기 때문입니다.

작업은 스케줄링 제약 조건을 지정할 수 있습니다. 어피니티(Affinity, 친화성) 또는 안티-어피니티(anti-affinity, 반-친화성) 제약 조건은 특정 태스크들이 동일한 머신, 스위치 또는 전원 도메인에 함께 스케줄링되어야 하는지(또는 말아야 하는지)를 지정합니다. 예를 들어, 두 개의 복제본(replica)을 가진 데이터베이스 작업은 2차 복제본이 1차 복제본과 다른 머신, 다른 스위치에 있기를 원할 것입니다. 그래야 단일 장애가 두 복제본 모두에 영향을 미치지 않기 때문입니다. 다른 제약 조건으로는 효율성과 성능을 최적화하기 위한 머신 유형이 포함될 수 있습니다. 이러한 제약 조건 중 일부는 프로파일링 정보를 기반으로 자동 생성될 수 있습니다. 예를 들어, Borg 생태계에는 작업이 특정 하드웨어에서 특히 잘 실행되거나(또는 잘 실행되지 않거나) 하는 것을 인식하고 스케줄러가 이를 고려하도록 편향시키는 도구가 포함되어 있습니다.

Kubernetes[3]는 Borg의 아이디어와 개념을 기반으로 하는 인기 있는 오픈 소스 컨테이너 관리자입니다(실제로 초기 Kubernetes 팀은 주로 이전에 Borg에서 작업했던 엔지니어들로 구성되었습니다). Kubernetes는 높은 이식성과 확장성을 목표로 하며, 많은 퍼블릭 및 프라이빗 클라우드와 물리적 및 가상 머신에서 실행됩니다. Borg와 달리 머신 자체의 수명 주기를 다루지 않으므로 VM을 제공하는 기본 인프라를 가정합니다.

많은 Kubernetes 사용자가 이를 클라우드 API 추상화로 취급하지만, 일부는 그 이상을 추상화하려고 합니다. Kubernetes에는 API 내부를 포함하여 많은 확장 포인트가 포함되어 있어 조직이 내부 사용자를 위한 맞춤형(bespoke) 플랫폼을 만들 수 있습니다. 이러한 내부 개발자 플랫폼을 통해 기업은 서비스 실행 방법에 대한 자체적인 의견을 포함하고 강제할 수 있습니다. Kubernetes의 확장성은 또한 여러 오픈 소스 PaaS 및 FaaS(Function as a Service) 프로젝트를 탄생시켰으며 다른 컨테이너 지향 제품에 영향을 미쳤습니다. 예를 들어, GCP의 Cloud Run[4]은 VM뿐만 아니라 대부분의 운영 세부 정보를 숨기는 Kubernetes와 유사한 추상화를 제공하여 사용자의 컨테이너만 입력으로 받아 서비스로 실행합니다.

오늘날 자원 관리자는 전력, 에너지 및 탄소 최적화도 통합합니다. 예를 들어, 전력 인프라의 오버서브스크립션(oversubscription, 9장에서 논의됨)을 허용하여 필요할 때 우선순위가 낮은 워크로드를 제한하고[5], 지역 간(cross-regional) 스케줄러는 탄소 에너지가 가장 낮은 지역으로 워크로드를 이동할 수 있습니다[6].

#### 4.2.1.2 기반 인프라 소프트웨어 (Foundational infrastructure software)

거의 모든 대규모 분산 애플리케이션에는 소규모의 기본 기능 세트가 필요합니다. 신뢰할 수 있는 분산 스토리지, 원격 프로시저 호출(RPC), 클러스터 수준 동기화 및 잠금(locking) 등이 그 예입니다. 대규모 클러스터에서 고성능과 고가용성을 갖춘 이러한 기능을 올바르게 구현하는 것은 복잡합니다. 각 애플리케이션에 대해 이렇게 까다로운 코드를 다시 구현하는 것을 피하고 대신 재사용할 수 있는 모듈이나 서비스를 만드는 것이 현명합니다. Colossus(GFS의 후속)[7], [8], Dynamo[9], Chubby[10]는 대규모 클러스터를 위한 신뢰할 수 있는 스토리지 및 잠금 서비스의 예입니다.

소규모 배포에서는 간단한 수동 프로세스인 많은 작업들이 대규모 시스템에서는 상당한 양의 인프라를 필요로 합니다. 소프트웨어 이미지 배포 및 구성 관리, 모니터링, 비상 상황에서의 경보 선별(triage) 등이 그 예입니다. Autopilot[11]은 Windows Live 데이터 센터의 이러한 기능 중 일부에 대한 설계 예시를 제공합니다. 하드웨어 전체(fleet)의 전반적인 상태를 모니터링하려면 세심한 모니터링, 자동화된 진단 및 수리 워크플로의 자동화도 필요합니다. Google의 시스템 상태 인프라(System Health Infrastructure)[12]는 효율적인 상태 관리에 필요한 소프트웨어 인프라의 예입니다. 마지막으로, 이 규모의 시스템에서 성능 디버깅 및 최적화에도 전문적인 솔루션이 필요합니다. Dapper[13]는 성능 문제의 디버깅을 가능하게 하기 위해 RPC 추적을 수집하는 최초의 실제 시스템 중 하나였으며, 오늘날의 OpenTelemetry 생태계[14]에 영감을 주었습니다.

### 4.2.2 비정형 스토리지 시스템 (Unstructured storage systems)

Google의 GFS[7]는 단순한 파일과 같은 추상화를 갖춘 최초의 클러스터 규모 스토리지 시스템이었습니다. 처음에 GFS는 단일 사용 사례인 웹 검색 인덱싱 시스템(크롤링된 웹 페이지를 웹 검색에 사용하기 위해 인덱스 파일로 변환하는 시스템)을 지원하도록 설계되었습니다. 따라서 수천 명의 동시 읽기/쓰기 사용자를 위한 높은 처리량과 높은 디스크 장애율 하에서의 견고한 성능에 중점을 두었습니다. GFS 사용자는 일반적으로 대량의 데이터를 조작했으므로 GFS는 대규모 읽기 및 쓰기에 더욱 최적화되었습니다.

시스템 아키텍처는 메타데이터 작업을 처리하는 프라이머리(primary) 서버와, 해당 드라이브의 데이터 청크(chunk)를 관리하기 위해 디스크 드라이브가 있는 모든 서버에서 실행되는 수천 개의 청크서버(보조) 프로세스로 구성되었습니다. GFS에서 내결함성(fault tolerance)은 기존 RAID 시스템처럼 머신 내부가 아니라 머신 간의 복제를 통해 제공되었습니다. 머신 간 복제를 통해 시스템은 머신 및 네트워크 장애를 견딜 수 있었고, 주어진 디스크나 머신에 대한 복제본이 수천 대의 다른 머신에 분산되어 있었기 때문에 빠른 복구가 가능했습니다.

GFS는 배포를 가속화하기 위해 몇 가지 선택을 했습니다. 가장 눈에 띄는 점은 파일 시스템의 메타데이터가 단일 서버의 메모리에 맞아야 한다는 것이었습니다. 이 제한은 구현 노력을 크게 줄였지만 스토리지 셀의 규모와 중앙 메타데이터 서버가 실패했을 때 달성할 수 있는 가동 시간(uptime)을 제한했습니다. GFS에 대한 요구가 확대됨에 따라(예: Gmail 및 YouTube 사용 사례), GFS는 후속 제품인 Colossus[15]로 대체되었습니다.

Colossus 셀은 많은 폴더로 구성됩니다. 각 폴더는 사실상 자체적인 파일 시스템입니다. 각 폴더의 메타데이터는 파일 메타데이터를 보유하는 BTree의 독립적인 계층 구조로 저장됩니다. 이 최상위 디렉터리 계층을 Metadata1 또는 MD1이라고 합니다. MD1 BTree 파일의 메타데이터는 MD0라고 하는 또 다른 수준의 BTree에 저장되며, 해당 트리(MD0 파일의 파일 설명자)의 메타데이터는 Paxos[16]로 구현된 분산 로그 시스템에 저장됩니다. (단순함을 위해 **그림 4.2**에서는 MD0 및 MD1이 생략되었습니다.)

**그림 4.2 Colossus 파일 시스템의 단순화된 아키텍처**
(그림 설명: Client가 Curator(leader/read replica), CFSv2 folder, Folder cache service 등과 상호작용하며, 데이터는 D 서버에 저장되고 메타데이터는 Paxos 로그에 의해 지원됨을 보여주는 다이어그램)

큐레이터(Curator)는 고가용성의 수평적으로 확장 가능한 메타데이터 서버를 구현합니다. 이들은 네임스페이스, 파일 시스템 구조 및 관련 메타데이터(액세스 제어 목록, 타임스탬프 등)를 관리할 책임이 있습니다. 큐레이터는 파일 작업(생성, 삭제, 이름 변경 등)을 처리하지만 읽기 및 쓰기에는 관여하지 않습니다. Paxos는 이 폴더의 리더(메타데이터를 변경할 수 있는 서버)가 될 큐레이터를 결정합니다.

모든 데이터는 네트워크 연결 디스크와 유사한 간단한 API를 구현하는 D라는 하위 수준 시스템에 저장됩니다. D는 분산 파일 시스템이 아닙니다. 각 D 서버는 로컬 저장 장치에 있는 파일에 대한 네트워크 액세스만 제공합니다. D는 파일별 체크섬 외에는 심각한 신뢰성 보장을 제공하지 않습니다. 모든 종류의 복제, 오류 수정 또는 기타 백업 시스템은 이를 사용하는 시스템(즉, Colossus)에서 구현해야 합니다.

Colossus 클라이언트 라이브러리는 복제, 로드 밸런싱, 응답하지 않는 D 서버 처리 등 전체 기능의 상당 부분을 구현합니다. 예를 들어 파일이 3방향 복제(r=3)를 사용하는 경우, 클라이언트는 세 개의 복제본에 쓰기 작업을 보내야 할 책임이 있습니다. 이 기능을 클라이언트에 넣으면 효과적인 확장성을 제공합니다. 파일 시스템에 읽기 및 쓰기 사용자가 많을수록 읽기 및 쓰기 작업에 더 많은 CPU 및 네트워크 대역폭을 사용할 수 있으며, Colossus 서버는 파일 열기와 같은 일부 작업에만 관여하므로 전체 시스템 성능이 실제 Colossus 서버에서 병목 현상을 일으킬 가능성이 줄어듭니다.

GFS의 초기 버전은 단순 복제만 지원했지만, Colossus는 더 공간 효율적인 리드-솔로몬(Reed-Solomon) 코드를 지원하여 동일한 수준의 가용성에 대해 단순 복제보다 복제 공간 오버헤드를 약 2배 줄였습니다(3.3.2절 참조). 또한 Colossus는 상관관계가 있는 장애로 인해 데이터 손실이 발생할 가능성이 극히 낮도록 전체 클러스터에 파일 청크를 배포합니다(예: 복제본은 서로 다른 랙과 서로 다른 네트워크 블록에 저장됨).

전체 클러스터의 장치에 청크를 광범위하게 배포하면 복구 속도도 빨라집니다. 주어진 장치에 저장된 청크의 복제본이 스토리지 클러스터의 많은 머신에 분산되어 있기 때문에 손실된 데이터 청크의 재구성은 고속으로 병렬 수행됩니다. 긴 복구 시간 창은 추가 오류가 클러스터에 발생할 경우 복제본이 부족한 청크를 데이터 손실에 취약하게 만들 수 있으므로 빠른 복구가 중요합니다. Google의 분산 파일 시스템 가용성에 대한 포괄적인 연구는 Ford et al.[17]에서 찾을 수 있습니다. Google의 파일 시스템 설계 진화에 대한 좋은 논의는 McKusick과 Quinlan[15]에서도 찾을 수 있습니다.

Colossus 클라이언트에 많은 기능을 넣으면 한 가지 주목할 만한 단점이 있습니다. 버그를 수정하거나 새로운 기능을 제공하려면 모든 Colossus 클라이언트를 다시 컴파일한 다음 다시 시작해야 한다는 것입니다! 파일 시스템을 사용하는 애플리케이션의 업그레이드 속도를 제어하는 것은 일반적으로 불가능하기 때문에 이는 매우 바람직하지 않은 속성입니다. 그러나 단일 회사(클라우드 제공업체, 즉 Google) 내의 순전히 내부적인 사용의 경우 이는 실용적인 선택입니다. Google에서는 다양한 이유로 모든 바이너리를 정기적으로 다시 빌드해야 하는 표준 정책이 있으며, 이 정책은 Colossus 클라이언트도 최신 상태로 유지합니다. 그러나 Colossus의 대중적인 사촌 격인 GCS[18]는 클라우드 고객에게 애플리케이션을 적시에 업데이트하도록 강요하는 것이 불가능하기 때문에 반대되는 선택을 했습니다. 따라서 GCS 클라이언트 라이브러리는 상당히 단순하며 실제 Colossus 클라이언트가 포함된 API 서버로 요청을 전달합니다.

### 4.2.3 구조화된 WSC 스토리지 (Structured WSC storage)

Colossus와 GCS의 단순한 파일 추상화는 대용량 데이터 블롭(blob)을 조작하는 시스템에는 충분할 수 있지만, 애플리케이션 개발자는 데이터 세트를 구조화하고 색인화하여 소규모 업데이트나 복잡한 쿼리를 쉽게 수행할 수 있는 데이터베이스 같은 기능도 필요로 합니다. Google의 Bigtable[19]과 Amazon의 DynamoDB[9]와 같은 구조화된 분산 스토리지 시스템은 이러한 요구를 충족하도록 설계되었습니다. 기존 데이터베이스 시스템과 비교하여 Bigtable과 DynamoDB는 대규모에서의 더 높은 성능과 가용성을 위해 스키마 표현의 풍부함과 강력한 일관성과 같은 일부 기능을 희생합니다. 예를 들어 Bigtable은 여러 열(column)에 연관된 행 키(문자열)로 구성된 단순한 다차원 정렬 맵을 제공하여 분산 희소 테이블 공간을 형성합니다. 열 값은 버전 관리 및 시계열을 지원하기 위해 타임스탬프와 연관됩니다. 최고 성능의 기존 SQL 데이터베이스와 비교하더라도 Bigtable은 훨씬 더 높은 처리량(초당 수십억 쿼리)에 도달할 수 있습니다.

그러나 Bigtable과 DynamoDB에서 최종적 일관성(eventual consistency)만 제공하는 것은 완전한 일관성을 달성하는 부담을 이러한 시스템을 사용하는 애플리케이션으로 전가합니다. 특히 다단계 작업은 작성하기 어려워져 오류가 발생하기 쉽습니다. 예를 들어, 최종적 일관성 시스템에서는 애플리케이션이 새 데이터를 썼지만 나중에 다시 읽을 때 두 번째 읽기가 초기 쓰기와 다른 복제본으로 이동하면 이전 데이터를 볼 수 있습니다. Megastore[20]와 특히 Spanner[21]와 같은 2세대 구조화된 스토리지 시스템은 이러한 우려를 해결하기 위해 설계되었습니다. Megastore와 Spanner는 더 단순하고 강력한 일관성 모델을 제공하면서 더 풍부한 스키마와 SQL 유사 기능을 제공합니다. Megastore는 동기식 복제를 제공하기 위해 쓰기 처리량을 희생하여 유용성과 채택이 제한되었습니다.

Spanner는 수평적 확장성을 포기하지 않으면서 전역적으로 분산된 트랜잭션을 효율적으로 직렬화하기 위해 TrueTime이라는 새로운 시간 기반 API를 사용하여, 내결함성을 위해 원활한 광역 복제가 필요한 애플리케이션에 간단한 일관성 모델을 제공합니다. TrueTime은 GPS와 원자 시계를 기반으로 하는 전역적으로 분산된 시계로, 시계 불확실성에 대한 엄격한 제한을 제공합니다. TrueTime은 단일 시점을 반환하는 대신 실제 시간이 놓여 있는 간격 [earliest, latest]을 각 머신에 제공합니다. Spanner는 TrueTime을 사용하여 트랜잭션에 타임스탬프를 할당합니다. 이러한 전역적으로 일관된 타임스탬프에 의존함으로써 Spanner는 전 세계의 서로 다른 서버에서 발생하더라도 트랜잭션의 순서를 결정할 수 있습니다.

Spanner는 트랜잭션 처리 시스템에 대한 가장 엄격한 일관성 속성인 외부 일관성(external consistency)을 제공합니다. 파티션 내의 트랜잭션뿐만 아니라 Spanner의 모든 트랜잭션이 이 일관성 속성을 만족합니다. 외부 일관성은 Spanner가 트랜잭션이 직렬로 실행되는 시스템과 구별할 수 없는 방식으로 트랜잭션을 실행한다고 명시합니다. 또한 한 트랜잭션이 다른 트랜잭션이 커밋을 시작하기 전에 완료되면 시스템은 클라이언트가 두 번째 트랜잭션의 효과는 포함하지만 첫 번째 트랜잭션의 효과는 포함하지 않는 상태를 결코 볼 수 없음을 보장합니다[22].

Spanner는 수평적 확장성과 매우 높은 가용성(99.999% SLO)을 갖춘 완전 일관성 SQL 유사 데이터베이스를 제공합니다. 이는 시스템이 일관성(Consistency), 가용성(Availability), 네트워크 분할(Partition) 허용이라는 세 가지 속성 중 두 가지만 달성할 수 있다는 CAP 정리[23]에 의해 부과된 제한을 위반하는 것처럼 보입니다. NoSQL 시스템은 다른 두 가지 속성을 얻기 위해 일관성을 희생해야 하는데, Spanner는 어떻게 세 가지를 모두 제공할 수 있을까요? 실제로 Spanner는 CP(가용성 포기)만 제공하지만 실제로는 가용성이 너무 높아 사용자가 이를 CA 시스템으로 간주할 수 있습니다[24]. 이 조합은 네트워크를 완전히 제어할 수 있는 경우 실제로 가능한데, 이는 WAN을 통해서는 드물지만 Google의 글로벌 네트워크에서는 달성됩니다. 그렇더라도 네트워크 경로의 상당한 중복성, 상관관계가 있는 장애를 관리하기 위한 아키텍처 계획, 그리고 특히 업그레이드 시 매우 신중한 운영이 필요합니다. 중단이 발생하면 Spanner는 가용성보다 일관성을 선택합니다.

일관성과 신뢰성의 장점 때문에 Spanner는 거의 모든 WSC 사용자가 선택하는 트랜잭션 스토리지 시스템이 되었습니다. 예를 들어 Google에서는 Gmail과 Google 포토가 모든 데이터를 Spanner에 저장하며, Google의 클릭스트림 및 광고 데이터 관리 시스템도 마찬가지입니다. 후자의 경우 Google의 F1 시스템[25]은 Spanner를 데이터 저장소로 사용하고 분산 SQL 쿼리, 트랜잭션적으로 일관된 보조 인덱스, 비동기 스키마 변경, 낙관적 트랜잭션, 자동 변경 기록 및 게시와 같은 데이터베이스 기능으로 모든 AdWords 데이터를 관리합니다.

구조화된 스토리지 스펙트럼의 반대편에는 거의 전적으로 고성능을 목표로 하는 시스템이 있습니다. 이러한 시스템은 트랜잭션이나 지리적 복제에 대한 지원이 부족하고 단순한 키-값 데이터 모델을 사용하며 내구성 보장이 느슨할 수 있는 경향이 있습니다. 분산 DRAM 기반 객체 캐싱 계층으로 개발된 Memcached[26]는 단순 캐싱에 널리 사용됩니다. 빠른 키-값 저장소는 그 가치가 명확해지자 학계에서 상당한 관심을 끌었습니다. 예를 들어 스탠포드 RAMCloud[27] 시스템은 분산 DRAM 기반 데이터 저장소를 사용했지만 스토리지 노드 장애 발생 시 훨씬 더 높은 노드당 성능과 내구성을 목표로 했습니다. FAWN-KV[28] 시스템 역시 키-값 고성능 스토리지 시스템을 제시했지만 저장 매체로 NAND 플래시를 사용했으며 에너지 효율성에 추가적인 중점을 두었는데, 이는 9장에서 더 광범위하게 다룰 주제입니다. 오늘날에는 Redis([29])를 포함하여 여러 오픈 소스 키-값 스토리지 시스템이 인기가 있습니다.

### 4.2.4 WAN 및 엣지 네트워킹

이 책은 주로 WSC를 대규모 빌딩 블록으로 논의하지만, 대부분의 WSC는 하이퍼스케일러 또는 퍼블릭 클라우드 공급자가 운영하는 글로벌 인프라의 일부입니다. 여기서는 WSC 자체 외부의 인프라를 스케치합니다. 훨씬 작지만 WSC 자체의 가치에 지대한 영향을 미칩니다.

독일 사용자가 www.google.com에 요청하는 예시를 사용하여 이 인프라를 설명하겠습니다. 첫 번째 단계로 Google의 DNS 서버는 해당 엔드포인트에 대한 IP 주소를 반환해야 하며, 가급적 사용자 근처에 위치한 것이 좋습니다. 오늘날 이는 Anycast DNS[30]를 통해 수행됩니다. 동일한 IP 주소(예: 1.2.3.4)가 해당 서버가 있는 모든 지역에서 알려지고, 인터넷의 표준 경계 경로 프로토콜(BGP) 라우팅은 사용자의 요청을 네트워크 홉이 가장 적은 지역(예: 함부르크)으로 안내합니다. Google은 DNS 서버 자체와 요청을 수신하는 프런트엔드 IP 모두에 Anycast를 사용합니다.

대안적인 접근 방식은 스마트 DNS 서버를 사용하여 요청의 소스 IP를 사용하여 요청에 가장 적합한 위치를 계산하는 것입니다. 이 접근 방식은 IP 범위를 위치에 매핑하는 글로벌 네트워크 데이터베이스에 의존합니다. 예를 들어 사용자의 IP가 함부르크의 Deutsche Telekom에서 사용하는 서브넷에 있다는 것을 알면 함부르크가 요청을 보낼 올바른 지역임을 알 수 있습니다. Google의 DNS는 Anycast로 전환하기 전에 이러한 접근 방식을 사용했습니다. IP 기반 이름 확인의 주요 단점은 네트워크 지도를 구축하고 유지 관리하는 복잡성과 DNS 요청 시점에 알려진 정보가 너무 적다는 고유한 한계에 있습니다. 특히 Google DNS에 도달하는 DNS 요청은 사용자가 직접 보내는 것이 아니라 사용자의 ISP나 회사 소유의 중간(재귀) DNS 리졸버가 보내는 경우가 많으며, 이 리졸버는 사용자로부터 멀리 떨어져 있을 수 있습니다. 따라서 DNS 요청의 소스 IP 주소는 사용자의 실제 위치에 대한 열악한 프록시가 될 수 있습니다. (HTTP/3 사양에는 이제 서버가 현재 IP와 다른 선호 IP 주소를 지정할 수 있는 옵션이 포함되어 있지만[31], 2024년 현재 이 기능은 아직 널리 구현되지 않았습니다.)

그런 다음 사용자의 요청은 엣지(edge) 위치에 도달합니다. 네트워크의 코어(예: 실제 WSC)가 아니라 가장자리(외부 경계)에 있기 때문에 그렇게 불립니다. "엣지"는 문맥에 따라 다르게 사용되는 부정확한 용어입니다. 예를 들어 때로는 클라이언트 장치(즉, 엣지 컴퓨팅이 사용자의 휴대폰이나 노트북에서 발생함)를 의미하기도 하고, 때로는 동네 네트워크 캐비닛이나 타워와 같이 고도로 분산된 네트워크 위치를 의미하기도 하며, 대도시의 네트워크 POP(접속점)를 의미하기도 합니다.

Google의 네트워크에는 두 개의 엣지 서빙 계층이 있습니다. Google Global Cache(GGC)[32]는 엣지 캐싱, 콘텐츠 압축 및 지능형 라우팅을 사용하여 콘텐츠를 빠르고 효율적으로 전달합니다. 대부분의 GGC 노드는 몇 대의 서버로만 구성되며 파트너 ISP의 네트워크 내부에 위치합니다. 그런 의미에서 그들은 Google의 네트워크에 직접 연결되어 있지 않고 Google의 공간에 호스팅되지도 않기 때문에 Google의 네트워크 엣지 너머에 위치합니다.

GGC는 정적 콘텐츠(이미지, 비디오)를 제공하거나 라이브 콘텐츠를 스트리밍하도록 특별히 설계된 네트워크인 콘텐츠 전송 네트워크(CDN)의 예입니다. GGC는 캐싱을 통해 ISP의 인프라 비용을 줄이면서 콘텐츠 전달(예: YouTube 비디오 또는 Spotify 노래) 속도를 크게 높일 수 있습니다. 전 세계적으로 수만 개의 GGC 위치가 있습니다. **그림 4.3**의 지도는 Cloud Media CDN의 일부인 더 큰 캐싱 노드만 보여줍니다.

두 번째 엣지 계층은 실제 네트워크 엣지, 일반적으로 인터넷 교환소(IX)나 네트워킹 POP[33]에 있습니다. 전 세계적으로 이러한 위치는 100개가 훨씬 넘습니다. 예시를 계속하면, 이 글을 쓰는 시점(2024년)에 Google은 함부르크를 포함하여 독일에서 5개의 엣지 위치를 운영하고 있으므로 사용자의 요청은 함부르크 엣지 POP에서 처리될 가능성이 높습니다.

**그림 4.3 미디어 콘텐츠 배포 네트워크(CDN) 위치**

엣지 POP는 서비스 지역에 따라 다양한 크기로 제공되며 일반적으로 요청을 처리하거나 라우팅하는 프런트엔드 서버와 인기 있는 콘텐츠에 대한 2차 캐싱 수준을 형성하는 캐싱 서버를 모두 포함합니다. 로컬에서 처리할 수 없는 요청은 해당 서비스에 사용할 수 있는 서버가 있는 가장 가까운 데이터 센터로 전달됩니다. 예를 들어 GCP 요청은 프랑크푸르트나 베를린 GCP 리전으로 전달될 수 있지만, 검색 요청은 검색을 제공하는 가까운 데이터 센터(예: 덴마크나 네덜란드)로 전달될 수 있습니다.

네트워크 POP는 요청을 처리하는 것 외에도 Google 네트워크와 해당 위치에 있는 ISP의 네트워크를 연결하는 피어링(peering) 라우터도 수용합니다. "피어링"은 두 네트워크가 어느 쪽도 상대방에게 비용을 지불하지 않고 직접 상호 연결하고 트래픽을 교환하기로 합의하는 방식을 말합니다. 피어링은 트래픽 수준에 대한 직접적인 가시성을 제공하므로 양쪽 모두의 네트워크 성능을 향상시킬 수 있습니다. 예를 들어 함부르크의 ISP A로 가는 포트가 과부하되면 Google은 트래픽을 혼잡하지 않은 다른 포트로 리디렉션할 수 있습니다.

네트워크마다 라우팅 및 피어링 정책이 다릅니다. "뜨거운 감자(Hot potato)" 라우팅은 패킷을 가능한 한 빨리 네트워크 밖으로 내보냄으로써 네트워크 비용을 최소화합니다. 예를 들어 패킷이 ISP B로 향하지만 B로 라우팅할 수 있는 로컬 ISP A가 있는 경우 패킷은 즉시 A로 전달된 다음 공용 인터넷을 통해 여러 ISP를 거쳐 B에 도달합니다. "차가운 감자(Cold potato)" 라우팅은 그 반대인데, 패킷을 사용자와 가까운 위치에서 B로 전달될 때까지 내부 백본에 유지합니다. 차가운 감자 라우팅 정책, 개방형 피어링 정책, 대부분의 피어링 위치에 존재하는 덕분에 Google 패킷의 98% 이상이 Google 네트워크에서 최종 사용자의 ISP 또는 최종 사용자의 기업 네트워크로 직접 단 한 번의 핸드오프만 거칩니다.

엣지 POP는 서로 연결되어 있으며 백본 네트워크(**그림 4.4**)를 통해 주 데이터 센터에도 연결됩니다. 이 백본 네트워크는 계층적으로 구성됩니다. 예를 들어 대도시 지역의 여러 POP는 메트로 네트워크를 통해 서로 연결되며, 해당 네트워크는 하나 이상의 위치에서 지역 백본에 연결되고, 지역 백본은 다시 글로벌 백본에 연결됩니다(**그림 4.5**). 많은 백본 네트워크에서 트래픽의 대부분은 데이터 센터와 사용자 간의 트래픽이 아니라 존과 리전 간의 데이터 복제로 인한 데이터 센터 간의 트래픽입니다.

2장에서 논의한 바와 같이 리전에는 여러 캠퍼스에 위치한 여러 존이 있으므로 네트워크 백본은 데이터 센터를 직접 연결하는 것이 아니라 리전을 연결합니다. 지역 백본 네트워크는 한 리전 내의 모든 캠퍼스를 연결합니다. 일반적으로 이들은 수십 킬로미터 떨어져 있을 뿐이므로 지역 백본은 저렴한 비용으로 고용량 대역폭을 제공할 수 있습니다. 여러 건물로 구성된 대규모 캠퍼스에는 이러한 건물을 훨씬 더 빠른 속도로 더 낮은 비용으로 연결하는 캠퍼스 네트워크가 추가로 있습니다. 이러한 네트워크 계층(예: 캠퍼스 및 지역) 각각은 그들 사이의 트래픽을 라우팅하는 게이트웨이로 연결됩니다. 기능은 비슷하지만 게이트웨이는 규모가 매우 다릅니다. 단일 피어링 라우터로 구성된 게이트웨이는 여러 ISP를 네트워크에 연결할 수 있는 반면, 수 멀티 페타비트 캠퍼스 네트워크와 수 멀티 페타비트 지역 네트워크 사이의 게이트웨이는 인터넷 POP보다는 클러스터 네트워크와 더 유사한, 수천 개의 포트가 있는 수십 개의 섀시로 구성됩니다.

우리의 예시를 다시 요약하자면, 사용자의 패킷은 ISP 네트워크를 통해 피어링 라우터가 ISP와 Google을 연결하는 Google 엣지 위치로 이동합니다. 로컬에서 처리할 수 없는 경우 Google의 메트로 네트워크를 통과하여 백본 게이트웨이로 이동한 다음, 서비스를 제공하는 데이터 센터 근처의 다른 백본 게이트웨이로 이동합니다. 거기서 목적지의 메트로 네트워크를 계속 이동하여 캠퍼스 네트워크 게이트웨이로 가고, 거기서 서비스를 호스팅하는 건물로 이동하여 또 다른 게이트웨이가 실제 서버를 보유한 클러스터 네트워크로 라우팅합니다.

**그림 4.4 Google 글로벌 네트워크의 구조**
(그림 설명: GGC가 Edge에 연결되고, Edge는 다시 WSC(Warehouse-Scale Computer)들에 연결되며, 이들이 상호 연결된 구조를 보여줌)

**그림 4.5 Google의 글로벌 해저 네트워크**

## 4.3 인프라 관리

WSC 클러스터에는 수백만 개의 물리적 부품, 수천만 개의 컴퓨팅 코어, 최소 수만 개의 바이너리가 포함되어 있습니다. 이 규모의 자원을 관리하는 것은 그 자체로 예술이 됩니다. 오늘날 소규모 배포를 관리하는 노고(toil)를 줄이는 많은 관행은 원래 WSC를 위해 개발된 관행에서 파생되었습니다.

### 4.3.1 자원 프로비저닝 (Resource provisioning)

퍼블릭 클라우드 배후의 WSC를 사용하는 대부분의 개발자는 사용 중인 VM, 컨테이너 또는 스토리지의 기본 물리적 인프라를 다룰 필요가 없습니다. 새 자원을 프로비저닝하는 데는 API 호출만 하면 되고, 몇 초 후에 자원을 사용할 수 있습니다. 이면에서는 항상 사용 가능한 용량이라는 환상이 물리적 세계의 현실과 만납니다. 고객이 요청할 때마다 여유 자원을 사용할 수 있도록 어떻게 보장할까요?

먼저 현재 수요(오늘 주문해야 할 것)를 계산하고 미래 수요(몇 달에서 몇 년 후에 필요한 것)를 예측하는 계획 시스템이 필요합니다. 여기서 "현재" 수요는 지금 당장 필요한 것을 의미하는 것이 아니라, 지금 당장 주문해야 하는 것, 즉 공급 리드 타임에 있는 수요를 의미합니다. 예를 들어 서버를 주문, 배송 및 설치하는 데 일주일이 걸린다면 오늘 우리는 지금부터 일주일 후의 수요가 얼마나 될지 알아야 합니다.

따라서 리드 타임은 프로비저닝의 핵심 지표입니다. 리드 타임 내에 구체화되는 수요는 일반적으로 충족될 수 없으므로 리드 타임이 짧을수록 좋습니다. 개별 서버와 같은 일부 자원의 경우 며칠의 리드 타임을 실현하는 것이 제한된 범위 내에서 가능합니다(서버 1대에 대한 수요는 서버 1000대에 대한 수요와 매우 다릅니다!). 다른 자원의 경우 공급망이 훨씬 더 길 수 있습니다. 예를 들어 데이터 센터를 짓는 데는 약 1년이 걸리고 유틸리티 규모 변압기와 같은 일부 구성 요소는 주문에서 배송까지 1년 이상 걸립니다. 칩과 같은 작은 구성 요소조차 리드 타임이 길 수 있습니다. 2024년 현재 TSMC의 5nm 공정에서 제조된 칩은 제조하는 데 약 5개월이 걸립니다!

리드 타임을 단축하거나 숨기는 한 가지 방법은 미리 주문하고 어딘가에 재고 버퍼를 유지하는 것입니다. 예를 들어 5nm 칩의 분기별 물량을 버퍼에 보관하면 예상치 못한 수요를 5개월보다 훨씬 더 빨리 충족할 수 있습니다. 하지만 그 반대도 마찬가지입니다. 이제 우리는 이 칩에 대한 수요가 끝난다는 것을 적어도 3개월 전에 결정해야 버퍼를 비울 수 있습니다. 그리고 그 예측이 완벽하더라도 버퍼는 비용이 많이 들 수 있습니다. 예비로 보유한 재고를 구매하는 데 지출된 모든 달러는 선반에 앉아 있는 무언가에 지출된 달러이므로 효과적인 돈 사용이라고 하기 어렵습니다. 따라서 버퍼링은 필요하지만 완전한 해결책은 아닙니다.

계획 프로세스의 출력은 이행(fulfillment)을 위한 입력이 됩니다. 공급망 세계에서 이행은 주문을 수신, 준비 및 전달하는 프로세스로 정의됩니다. WSC 공급망의 경우 여기에는 구성 요소 주문(일반적으로 "A" 부품, 즉 가장 비싼 부품), 제조업체(B 및 C 부품 구매)에 할당, 제조 및 운송 용량 예약, 배포 계획(다음 주 화요일에 이 위치에 기술자가 충분한가?) 등이 포함될 수 있습니다.

WSC에서 공급은 신규 구매에서만 나오는 것이 아닙니다. 예를 들어 대규모 사용자가 방금 동일한(또는 인근) 데이터 센터 밖으로 이동하여 해당 머신을 사용하여 새로운 수요를 충족할 수 있습니다. 또는 위치 A의 서비스를 유휴 자원이 있는 B로 이동하여 A의 새로운 수요를 위해 자원을 확보할 수 있습니다. 또는 수요를 재구성할 수도 있습니다. 100개의 A 유형 GPU를 요청하는 작업이 150개의 B 유형 GPU에서 똑같이 잘 실행되어 A 유형 GPU의 추가 구매를 피할 수 있습니다. 일반적으로 용량을 공유할 수 있는 풀이 클수록, 그리고 용량 수요가 위치 및 특정 하드웨어 측면에서 유연할수록 총 이행 비용은 낮아집니다. 좋은 이행과 훌륭한 이행 사이의 비용 차이가 클 수 있으므로 계획 및 이행은 매우 복잡하고 다차원적인 최적화 문제로 발전했습니다. 누군가 말했듯이 "계획과 이행은 직접 해보기 전까지는 쉽다"는 것입니다.

### 4.3.2 구성 및 관측 가능성 (Configuration and observability)

워크로드와 하드웨어 인프라의 규모와 복잡성으로 인해 관리 및 관측 가능성 프레임워크는 모든 WSC 배포의 기본 구성 요소가 되므로 여기에서 더 자세히 설명합니다.

#### 4.3.2.1 구성 관리 (Configuration management)

역사적으로 애플리케이션 구성은 규율이라기보다는 나중에 덧붙인 생각이었습니다. 바이너리는 명령줄 플래그, 셸 환경 변수 및 일부 구성 파일을 통해 구성을 받았습니다. 현대 소프트웨어 개발은 이를 변경하여 구성 관리를 소프트웨어 엔지니어링의 필수적인 부분으로 만들었습니다[34].

구성 관리는 구성 데이터를 자동으로 관리하는 도구를 통해 엔지니어링 팀이 견고하고 안정적인 시스템을 구축하도록 돕습니다. 구성 데이터에는 낮은 수준의 머신 데이터(예: N 코어 및 M GB RAM이 있는 특정 머신 유형), 애플리케이션 매개변수, API 키와 같은 자격 증명, 애플리케이션에 필요한 서비스를 나타내는 이름 또는 엔드포인트 등이 포함될 수 있습니다.

구성 관리는 변경 사항 검토, 잘못된 구성 롤백, 기록 문서화("작년에 어떤 설정을 사용했지?")를 허용하는 버전 제어 시스템에 이 모든 데이터를 저장하는 것을 목표로 합니다. 종종 이러한 시스템은 매우 유사한 목적으로 구축되었기 때문에 소스 제어 시스템 위에 구현됩니다. 이것이 바로 이 접근 방식을 종종 "코드로서의 구성(configuration as code)"이라고 부르는 이유입니다. 구성 파일을 소스 파일처럼 취급하십시오.

"코드로서의 인프라(IAC, Infrastructure as code)"는 한 단계 더 나아가 서비스 인스턴스 설정 또는 삭제가 완전히 자동화되어야 한다고 제안합니다. 이 접근 방식은 코드로서의 구성을 필요로 하지만 VM 또는 컨테이너 이미지를 생성하고, 해당 VM 또는 컨테이너를 시작하고, 네트워킹 및 스토리지를 설정하는 등의 모든 단계를 추가로 포함합니다. IAC 도구의 예로는 Ansible[35], Chef[36], Puppet[37], Terraform[38]이 있습니다.

#### 4.3.2.2 관측 가능성 (Observability)

모니터링(즉, 시스템 상태를 정의하는 메트릭 수집 및 보고)을 넘어, 관측 가능성은 모니터링 데이터를 이해하여 문제의 근본 원인을 찾는 시스템을 필요로 합니다. 모니터링이 개별 구성 요소에 대한 데이터를 수집하는 반면, 관측 가능성은 분산 시스템 전체를 봅니다.

시스템 운영자는 인터넷 서비스가 서비스 수준 지표(SLI)를 얼마나 잘 충족하고 있는지 추적해야 합니다. SLI는 특정 API 호출 C가 성공적으로 실행되는 시간의 백분율과 같은 특정 서비스 측면을 측정합니다. 서비스 수준 목표(SLO)는 서비스가 도달해야 하는 목표 SLI를 명시합니다("C는 99.9%의 시간 동안 성공적으로 실행될 것이다"). 마지막으로 서비스 수준 계약(SLA)은 SLO 및 SLO에 도달하지 못한 경우 지불해야 할 벌금을 명시하는 법적 계약입니다("이 SLO를 충족하지 못하면 위약금 없이 계약을 해지할 수 있습니다").

모니터링 정보는 운영자(또는 자동화된 시스템)가 신속하게 시정 조치를 취하고 심각한 혼란을 피할 수 있도록 매우 최신 상태여야 합니다(이상적으로는 몇 분이 아니라 몇 초 이내). 때로는 필요한 가장 중요한 정보가 사용자 요청에 대한 지연 시간 및 처리량 통계와 같이 프런트엔드 서버에서 수집할 수 있는 몇 가지 신호로 제한됩니다. 가장 단순한 형태의 모니터링 시스템은 단순히 모든 프런트엔드 서버를 몇 초마다 폴링하여 적절한 신호를 받고 대시보드에 운영자에게 표시하는 스크립트일 수 있습니다. Google Cloud Operations 도구[39]는 클라우드 서비스를 모니터링, 문제 해결 및 운영하기 위한 유사한 제품을 제공합니다. 로드 밸런서는 종종 트래픽을 정상 서버 풀로 우회하거나 풀에 추가 서버를 추가하여 몇 초 내에 특정 문제를 자동으로 해결하는 데 사용됩니다.

대규모 서비스는 프런트엔드 수가 상당히 많을 수 있고 서비스 상태를 특성화하는 데 더 많은 신호가 필요하므로 더 정교하고 확장 가능한 모니터링 지원이 필요한 경우가 많습니다. 예를 들어 신호 자체뿐만 아니라 시간 경과에 따른 미분(derivatives)을 수집하는 것이 중요할 수 있습니다. 시스템은 지연 시간 및 처리량 외에도 다른 비즈니스별 매개변수("구매 버튼 클릭이 실패하고 있습니까?")를 모니터링해야 할 수도 있습니다. 모니터링 시스템은 일반적으로 운영자가 모니터링되는 기준 신호를 기반으로 파생 매개변수를 만들 수 있는 표현 언어를 지원합니다. 마지막으로 시스템은 모니터링되는 값과 임계값에 따라 대기(on-call) 운영자에게 자동 경고를 생성합니다. 경고(또는 알람) 시스템을 미세 조정하는 것은 까다로울 수 있습니다. 오탐(false positives)으로 인해 너무 자주 울리는 알람은 운영자가 실제 알람을 무시하게 만드는 반면, 극단적인 경우에만 울리는 알람은 운영자의 주의를 너무 늦게 끌어 근본적인 문제를 원활하게 해결하지 못하게 할 수 있기 때문입니다. SRE 책[40]에는 모니터링 및 경고에 대한 심층적인 논의가 포함되어 있습니다.

WSC 개발자는 문제의 원인을 파악하기 위해 애플리케이션 및 충돌 로그도 필요합니다. 기존(단일 머신) 애플리케이션에서는 로컬 파일 시스템에 저장되지만, WSC에서는 개발자가 서비스의 모든 인스턴스에 걸쳐 애플리케이션 로그를 볼 수 있도록 중앙 저장소에서 수집 및 집계되어야 합니다. 일반적으로 이러한 파일은 먼저 로컬 스토리지에 기록되고 로그 수집 프로세스가 중앙 저장소로 복사하여 일정 기간 동안 보관합니다. 종종 이 프로세스는 로그를 압축하고 충돌 덤프에 존재할 수 있는 암호화 키와 같은 민감한 데이터를 제거하여 "위생 처리(sanitizes)"합니다.

#### 4.3.2.3 추적 도구 (Tracing tools)

서비스 수준 대시보드는 운영자가 서비스 수준 문제를 신속하게 식별하는 데 도움이 되지만, 서비스가 느리거나 요구 사항을 충족하지 못하는 이유를 알기 위해 필요한 상세 정보는 부족한 경우가 많습니다. 운영자와 서비스 설계자 모두 수백 대의 서버에서 실행될 수 있는 많은 프로그램 간의 복잡한 상호 작용을 이해하여 성능 이상 현상의 근본 원인을 파악하고 병목 현상을 식별하는 데 도움이 되는 도구가 필요합니다.

분산 시스템 추적 도구는 이러한 요구를 해결하고 모니터링에서 관측 가능성으로 이동하는 중요한 단계를 형성합니다. 이러한 도구는 주어진 시작자(예: 사용자 요청)를 대신하여 분산 시스템에서 수행된 모든 작업을 결정하고 관련된 다양한 구성 요소 간의 인과적 또는 시간적 관계를 자세히 설명하려고 시도합니다. 예를 들어 사용자가 "구매" 버튼을 클릭하면 애플리케이션 프런트엔드는 구매 완료 과정에서 수백 개의 마이크로서비스를 전이적으로 호출할 수 있습니다. 실패하면 왜 실패할까요? 추적 도구는 그 목적에서 이름을 따왔습니다. 요청이 완료될 때까지 모든 하위 요청을 통해 요청을 추적하여 관련된 다른 모든 서비스와 그 성능에 대한 전체 기록을 생성하는 것을 목표로 합니다.

추적 도구는 블랙박스 모니터링 시스템과 애플리케이션/미들웨어 계측(instrumentation) 시스템이라는 두 가지 광범위한 범주로 나뉩니다. WAP5[41]와 Sherlock 시스템[42]은 블랙박스 모니터링 도구의 예입니다. 이들의 접근 방식은 시스템 구성 요소 간의 네트워킹 트래픽을 관찰하고 통계적 추론 방법을 통해 인과 관계를 추론하는 것으로 구성됩니다. 모든 시스템 구성 요소(네트워킹 인터페이스 제외)를 블랙박스로 취급하기 때문에 이러한 접근 방식은 애플리케이션이나 소프트웨어 인프라 구성 요소의 지식이나 도움 없이 작동한다는 장점이 있습니다. 그러나 모든 관계를 통계적으로 추론해야 하므로 이 접근 방식은 본질적으로 정보 정확성을 희생합니다. 더 많은 메시징 데이터를 수집하고 분석하면 정확도는 향상될 수 있지만 모니터링 오버헤드가 높아지는 비용이 듭니다.

Pip[43], Magpie[44], X-Trace[45]와 같은 학계 시스템에서 개척된 계측 기반 추적 방식은 머신 간 및 머신 내 모듈 경계 간에 추적 정보를 전달하기 위해 애플리케이션이나 미들웨어 라이브러리를 명시적으로 수정할 수 있는 기능을 활용합니다. 주석이 달린 모듈은 일반적으로 외부 성능 분석 프로그램에 의한 후속 수집을 위해 로컬 디스크에 추적 정보를 기록합니다. 이러한 시스템은 추론이 필요 없으므로 매우 정확할 수 있지만 포괄적인 데이터를 수집하려면 분산 시스템의 모든 구성 요소를 계측해야 합니다. Dapper 시스템[13]은 메시징, 제어 흐름 및 스레딩 라이브러리와 같이 모든 애플리케이션과 일반적으로 연결된 몇 가지 주요 모듈을 계측하여 애플리케이션 수준 소프트웨어에 효과적으로 투명하게 유지되는 주석 기반 추적 도구입니다. GCP의 Cloud Trace[46]는 OpenTelemetry 호환 소스의 추적을 수락하는 Dapper 시스템의 공개적으로 사용 가능한 구현입니다.

지금까지는 RPC 수준에서의 추적을 고려했지만, 이 개념은 함수 호출에도 적용될 수 있습니다. 예를 들어 LLVM 컴파일러의 기능인 XRay는 컴파일러 계측을 사용하여 모든 함수 진입 및 종료 시 추적 지점을 추가하여 활성 상태일 때 매우 미세한 지연 시간 세부 정보를 제공하고 비활성 상태일 때 매우 낮은 오버헤드를 제공합니다. 다른 도구는 더 깊은 내부 검사를 제공할 수 있습니다. 예를 들어 사용자가 다시 컴파일하거나 다시 배포하지 않고도 프로그램에 동적으로 로깅을 추가할 수 있습니다.

#### 4.3.2.4 성능 도구 (Performance tools)

추적 도구는 성능 최적화에도 매우 유용할 수 있습니다. 추적에는 타이밍 정보가 주석으로 달려 있으므로 구매 버튼 클릭과 같은 중요한 서비스의 지연 시간에 기여하는 요인을 잘 이해할 수 있습니다. 또한 추적이 항상 수집될 때 다양한 부하에서 서비스의 성능을 이해하거나 지역 A의 서비스 인스턴스 성능이 지역 B와 다른 이유를 이해하는 데 매우 도움이 될 수 있습니다.

하드웨어 성능 카운터의 샘플링을 기반으로 하는 CPU 프로파일러는 프로그래머가 마이크로아키텍처 성능 현상을 이해하는 데 매우 성공적이었습니다. WSC 애플리케이션은 많은 머신에서 실행되므로 이러한 도구는 여러 머신을 프로파일링해야 합니다. Google-Wide Profiling(GWP)[47]은 무작위로 머신의 하위 집합을 선택하여 짧은 전체 머신 및 프로세스별 프로필 데이터를 수집하고 모든 Google 바이너리에 대한 심볼 정보 저장소와 결합하여 클러스터 전체의 프로필 데이터 뷰를 생성합니다. GWP는 다음과 같은 질문에 답합니다. Google에서 가장 자주 실행되는 절차는 무엇입니까? 또는 메모리를 가장 많이 사용하는 프로그램은 무엇입니까? Google Cloud Operations 도구 제품[39]은 GWP에서 영감을 받았습니다.

성능 도구는 관찰만 하는 것이 아니라 능동적으로 최적화할 수도 있습니다. 예를 들어 프로필 기반 최적화(PGO)[48]는 프로덕션 사용의 프로파일링 데이터를 사용하여 동일한 바이너리의 미래 버전을 최적화합니다. 최적화된 메모리 할당자는 캐시 및 TLB 미스를 줄입니다[49]. 이 공간에서의 발전은 최적화의 진정한 성능 영향이 규모가 커져야만 가시화되고 더 작고 실험적인 설정에서의 이점과는 매우 다를 수 있기 때문에 특히 어렵습니다[50]. 예를 들어 할당자를 과도하게 인라인하거나 특수화하면 더 작은 벤치마크에서는 상당한 속도 향상을 제공할 수 있지만, 수백만 개의 인라인된 할당자로 인한 코드 크기 증가가 너무 많은 명령어 캐시 미스를 유발할 수 있기 때문에 프로덕션 바이너리의 속도를 실제로 늦출 수 있습니다.

### 4.3.3 관행: DevOps 및 SRE

WSC, 그리고 나중의 퍼블릭 클라우드는 애플리케이션의 설계 및 구현뿐만 아니라 실행 방식에도 새로운 접근 방식을 요구했습니다. 가장 인기 있는 두 가지 새로운 접근 방식은 DevOps(Development와 Operations의 합성어)와 SRE(Site Reliability Engineering)입니다.

DevOps는 개발과 운영을 하나의 팀으로 통합합니다. 개발자는 더 이상 완성된 제품을 운영하고 유지 관리하는 운영 팀에 넘기지 않으며, 더 이상 소프트웨어 엔지니어와 운영 엔지니어 사이에 공식적인 구분이 없습니다. DevOps의 목표는 같은 팀이 두 가지 모두를 책임지게 함으로써 새 코드를 커밋하는 것부터 해당 코드가 프로덕션에서 실행되는 것까지의 시간을 단축하고 양쪽의 요구 사항 균형을 맞추는 것입니다. 예를 들어 애플리케이션 수준의 메트릭이나 로그가 충분하지 않으면 디버깅이 어려워질 것이고, 동일한 엔지니어가 프로덕션의 문제나 중단을 다루고 있기 때문에 높은 관측 가능성을 갖춘 애플리케이션을 구축할 가능성이 더 큽니다.

DevOps에 대한 가능한 반론은 두 직종이 너무 다른 기술 세트를 필요로 한다는 것입니다. 훌륭한 소프트웨어 엔지니어가 신뢰성, 운영의 엄격함 또는 용량 계획에 뛰어나지 않을 수 있습니다. 따라서 SRE는 동일한 문제를 다른 방식으로 해결하는 것을 목표로 합니다. 여전히 두 팀이 있지만 "운영" 팀은 신뢰성 및 기타 운영 속성(확장성, 효율성, 보안 등)을 전문으로 하는 소프트웨어 엔지니어로 구성됩니다.

SRE 책[40]의 1장에는 SRE 원칙에 대한 훌륭한 개요가 포함되어 있으며, 다음의 간단한 문장으로 요약할 수 있습니다. SRE는 소프트웨어 엔지니어에게 운영 팀을 설계하도록 요청할 때 발생하는 일입니다. SRE는 엔지니어링(예: 증상 해결이 아닌 솔루션 자동화 또는 근본 원인 해결)에 지속적으로 중점을 두고 서비스의 SLO를 달성하면서 최대 변경 속도를 추구합니다. 즉, 목표는 100% 가동 시간을 달성하는 것이 아닙니다. 그렇게 하면 SRE가 변경을 거부하도록 장려할 수 있으며, 이는 중단의 가장 큰 원인이기 때문입니다. 대신 SRE 철학은 "다운타임 예산을 잘 사용하라"(그리고 그 예산 내에 머무르라, 즉 SLO 달성)고 요청합니다. 중단이 발생하면 "단순한 실수"가 아니라 새로운 문제로 인해 발생해야 합니다. 예를 들어 손상된 구성 파일로 인해 중단이 발생했다면 이는 서비스가 구성을 제대로 검증하지 않는다는 신호입니다.

WSC 규모에서 운영하려면 장애를 학습 기회로 보는 강력한 문화도 필요합니다. 수백만 개의 코어와 수천 명의 소프트웨어 엔지니어가 있는 시스템에서는 규모 면에서 *무언가* 잘못될 것입니다. 최고의 팀, 최고의 기술, 최고의 관행이 있더라도 말이죠. 따라서 목표는 불완전한 하드웨어, 소프트웨어 및 사람에도 불구하고 매우 신뢰할 수 있는 시스템을 만드는 것입니다. 높은 수준의 신뢰성에 도달하기 위한 핵심 메커니즘 중 하나는 비난 없는 포스트모텀(blameless postmortem)[40]입니다.

> 비난 없는 포스트모텀은 SRE 문화의 핵심입니다. 포스트모텀이 진정으로 비난이 없으려면 나쁘거나 부적절한 행동에 대해 개인이나 팀을 기소하지 않고 사고의 기여 원인을 식별하는 데 초점을 맞춰야 합니다. 비난 없는 포스트모텀은 사고에 관련된 모든 사람이 좋은 의도를 가졌고 그들이 가진 정보로 옳은 일을 했다고 가정합니다. "잘못된" 일을 했다고 개인이나 팀을 손가락질하고 수치심을 주는 문화가 만연하다면 사람들은 처벌을 두려워하여 문제를 밝히지 않을 것입니다(자세한 내용은 [40]의 15장 참조).

이 책은 주로 하드웨어 및 소프트웨어 주제를 다루지만 올바른 엔지니어링 *문화*를 만들고 유지하는 것은 종종 훨씬 더 어렵고 또한 더 중요합니다! 우리는 11장에서 WSC의 25년을 되돌아볼 때 이에 대해 더 이야기할 것입니다.

## 4.4 서버 수준 소프트웨어

### 4.4.1 OS 및 펌웨어

WSC 서버 노드에서 실행되는 기본 소프트웨어 시스템 이미지는 일반 엔터프라이즈 서버 플랫폼에서 예상되는 것과 크게 다르지 않습니다. 그러나 WSC 서버의 펌웨어, 장치 드라이버 또는 운영 체제 모듈은 범용 엔터프라이즈 서버보다 더 크게 단순화될 수 있습니다. WSC 서버 하드웨어 구성의 동질성이 높기 때문에 장치 조합이 적어 펌웨어 및 장치 드라이버 개발과 테스트를 간소화할 수 있습니다. 또한 WSC 서버는 비교적 잘 알려진 환경에 배포되어 성능 향상을 위한 최적화가 가능합니다. 예를 들어 WSC 서버의 네트워킹 연결 대부분은 동일한 건물 내의 다른 머신으로 연결되며 장거리 인터넷 연결보다 패킷 손실이 적습니다. 따라서 더 높은 통신 효율성을 위해 전송 또는 메시징 매개변수(타임아웃, 윈도우 크기 등)를 튜닝할 수 있습니다.

WSC 서버는 실행되는 소프트웨어의 무결성을 포함하여 보안에 높은 중점을 둡니다(보안은 10장에서 더 자세히 논의할 것입니다). 서버가 변조되지 않았음을 보장하기 위해 부팅 프로세스와 펌웨어 업데이트를 제어하는 작고 전문화된 보안 칩인 실리콘 RoT(Root of Trust)를 사용합니다. RoT는 CPU와 완전히 독립적이므로 공격자가 서버에 대한 전체 제어(루트) 권한을 가지고 있어도 보안 조치를 우회할 수 없습니다.

RoT는 서버나 장치가 올바른 펌웨어로 부팅되고 낮은 수준의 맬웨어에 감염되지 않았음을 확인하여 하드웨어 인프라와 그 위에서 실행되는 소프트웨어가 의도된 신뢰할 수 있는 상태로 유지되도록 돕습니다. 또한 암호학적으로 고유한 머신 ID를 제공할 수 있어 운영자가 서버나 장치가 합법적인지 확인할 수 있습니다. 또한 물리적 접근이 가능한 사람(예: 서버나 장치가 배송되는 동안)에게도 변조 방지 방식으로 암호화 키와 같은 비밀을 보호합니다. 마지막으로 중요한 것은 권위 있고 변조가 명백한 감사 기록 및 기타 런타임 보안 서비스를 제공한다는 것입니다.

실리콘 RoT는 클라우드 제공업체에서 흔히 볼 수 있습니다. Google의 Titan 칩[51], Amazon의 Nitro[52], 오픈 소스 노력인 OpenTitan[53] 및 Caliptra[54]가 그 예입니다.

### 4.4.2 컨테이너화 및 가상화

컨테이너화와 가상화는 서로 격리된 애플리케이션을 배포하고 관리하기 위한 뚜렷하지만 상호 보완적인 두 가지 기술입니다.

컨테이너화는 여러 격리된 사용자 공간 인스턴스가 공유 운영 체제 커널에서 실행될 수 있도록 하는 가벼운 형태의 운영 체제 가상화입니다. 각 컨테이너는 애플리케이션과 종속성을 캡슐화하여 일관되고 이식 가능한 런타임 환경을 제공합니다. 이 격리는 다른 컨테이너 내에서 실행되는 애플리케이션이 서로 간섭하지 않도록 하여 종속성 관리를 단순화하고 보안을 강화합니다. 컨테이너는 기본 OS 커널을 공유하므로 효율적이며, 기존 가상 머신에 비해 시작 시간이 빠르고 리소스 소비가 적습니다.

반면 가상화는 하드웨어, 운영 체제 및 애플리케이션을 포함한 전체 컴퓨터 시스템을 가상 머신(VM) 내부에서 에뮬레이트합니다. 이 에뮬레이션은 가상 머신과 물리적 하드웨어 사이에 있는 소프트웨어 계층인 하이퍼바이저를 통해 달성됩니다. 각 가상 머신은 고유한 전용 자원과 자체 "게스트" 커널을 사용하여 독립적으로 운영됩니다. 가상화는 단일 물리적 서버에서 여러 운영 체제와 애플리케이션을 실행할 수 있도록 하여 더 큰 유연성을 제공합니다.

두 기술 모두 WSC 인프라에서 중요한 역할을 하며, 동일한 서버에 여러 워크로드를 스케줄링하여 효율적인 자원 활용을 가능하게 하고, 이러한 워크로드를 서로 격리하여 애플리케이션 관리를 강화합니다. 이 격리는 상당히 강력하지만 완벽하지는 않습니다. 별도의 워크로드가 서로의 성능을 간섭할 수 있기 때문입니다("시끄러운 이웃 문제"). 예를 들어 한 워크로드가 DRAM 대역폭을 포화시키거나 로컬리티가 좋지 않아 많은 L3 캐시 라인을 대체하면 다른 워크로드는 성능 저하를 겪게 됩니다. 최신 CPU 아키텍처에는 성능 격리를 위해 특별히 설계된 하드웨어 기능이 포함될 수 있습니다. 예를 들어 Intel의 CAT(Cache Allocation Technology)는 코어별로 캐시 할당을 세밀하게 제어할 수 있으며, RDT(Resource Director Technology)는 캐시 및 메모리 대역폭과 같은 공유 자원을 모니터링하고 제어할 수 있습니다[55].

또한 시끄러운 이웃 효과는 한 프로세스가 메모리나 레지스터에 직접 액세스할 수 없음에도 불구하고 다른 프로세스의 정보를 재구성("읽기")할 수 있는 사이드 채널 공격을 통해 보안 문제를 일으킬 수 있습니다. 이러한 종류의 최초의 고영향 취약점(Spectre 및 Meltdown[56])은 2017년 Google과 그라츠 대학(University of Graz)의 연구원들에 의해 발견되었으며 그 이후로 많은 유사한 취약점이 발견되었습니다. 하드웨어 보안 취약점에 대해서는 10장에서 더 자세히 논의할 것입니다.

## 참고 문헌 (References)

1.  A. Verma, L. Pedrosa, M. Korupolu, et al., “Large-scale cluster management at Google with Borg,” in *Proceedings of the Tenth European Conference on Computer Systems*, ser. EuroSys ’15, New York, NY, USA: Association for Computing Machinery, Apr. 17, 2015, pp. 1–17.
2.  M. Tirmazi, A. Barker, N. Deng, et al., “Borg: The next generation,” in *Proceedings of the 15th European Conference on Computer Systems*, ser. EuroSys ’20, New York, NY, USA: Association for Computing Machinery, Apr. 17, 2020, pp. 1–14.
3.  Kubernetes Authors, Kubernetes, https://www.kubernetes.io.
4.  Google Cloud, Cloud Run - Serverless Container Platform.
5.  S. Li, X. Wang, F. Kalim, et al., “Thunderbolt: Throughput-optimized, quality-of-service-aware power capping at scale,” in *14th USENIX Symposium on Operating Systems Design and Implementation (OSDI20)*, 2020, pp. 1241–1255.
6.  A. Radovanovic, Our data centers now work harder when the sun shines and wind blows, The Keyword, https://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows/, 2020.
7.  S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,” in *Proceedings of the nineteenth ACM Symposium on Operating systems principles*, ser. SOSP ’03, New York, NY, USA: Association for Computing Machinery, Oct. 19, 2003, pp. 29–43.
8.  D. Serenyi, “Cluster-level storage @Google,” in *Keynote at the 2nd Joint International Workshop on Parallel Data Storage and Data Intensive Scalable Intensive Computing Systems*, 2017.
9.  G. DeCandia, D. Hastorun, M. Jampani, et al., “Dynamo: Amazon’s highly available key-value store,” in *Proceedings of twenty-first ACM SIGOPS Symposium on Operating Systems Principles*, ser. SOSP ’07, New York, NY, USA: Association for Computing Machinery, Oct. 14, 2007, pp. 205–220.
10. M. Burrows, “The Chubby lock service for loosely-coupled distributed systems,” in *Proceedings of the 7th Symposium on Operating Systems Design and Implementation*, ser. OSDI ’06, USA: USENIX Association, Nov. 6, 2006, pp. 335–350.
11. M. Isard, “Autopilot: Automatic data center management,” 2, vol. 41, New York, NY, USA: Association for Computing Machinery, Apr. 1, 2007, pp. 60–67.
12. E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large disk drive population,” in *Proceedings of the 5th USENIX Conference on File and Storage Technologies*, ser. FAST ’07, San Jose, CA: USENIX Association, 2007.
13. B. H. Sigelman, L. A. Barroso, M. Burrows, et al., “Dapper, a large-scale distributed systems tracing infrastructure,” 2010.
14. OpenTelemetry Authors, OpenTelemetry, https://opentelemetry.io/.
15. M. K. McKusick and S. Quinlan, “GFS: Evolution on Fast-forward: A discussion between Kirk McKusick and Sean Quinlan about the origin and evolution of the Google File System,” 7, vol. 7, New York, NY, USA: Association for Computing Machinery, Aug. 1, 2009, pp. 10–20.
16. L. Lamport, “The part-time parliament,” 2, vol. 16, New York, NY, USA: Association for Computing Machinery, May 1, 1998, pp. 133–169.
17. D. Ford, F. Labelle, F. I. Popovici, et al., “Availability in globally distributed storage systems,” in *Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation*, ser. OSDI’10, USA: USENIX Association, Oct. 4, 2010, pp. 61–74.
18. Google Cloud, Cloud Storage - Object Storage, https://cloud.google.com/storage.
19. F. Chang, J. Dean, S. Ghemawat, et al., “Bigtable: A distributed storage system for structured data,” in *Proceedings of the 7th Symposium on Operating Systems Design and Implementation*, ser. OSDI ’06, USA: USENIX Association, Nov. 6, 2006, pp. 205–218.
20. J. Baker, C. Bond, J. C. Corbett, et al., “Megastore: Providing scalable, highly available storage for interactive services.,” in *CIDR*, vol. 11, 2011, pp. 223–234.
21. J. C. Corbett, J. Dean, M. Epstein, et al., “Spanner: Google’s globally-distributed database,” in *Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation*, ser. OSDI’12, USA: USENIX Association, Oct. 8, 2012, pp. 251–264.
22. Google Cloud, TrueTime and external consistency, Cloud Spanner Documentation: https://cloud.google.com/spanner/docs/true-time-external-consistency.
23. E. Brewer, “CAP twelve years later: How the ”rules” have changed,” *Computer*, vol. 45, no. 2, pp. 23–29, 2012.
24. E. Brewer, Spanner, TrueTime and the CAP theorem, https://research.google/pubs/spanner-truetime-and-the-cap-theorem/, 2017.
25. J. Shute, R. Vingralek, B. Samwel, et al., “F1: A distributed SQL database that scales,” 11, vol. 6, VLDB Endowment, Aug. 1, 2013, pp. 1068–1079.
26. memcached team, memcached - a distributed memory object caching system, https://memcached.org/about.
27. J. Ousterhout, P. Agrawal, D. Erickson, et al., “The case for RAMClouds: scalable high-performance storage entirely in DRAM,” 4, vol. 43, New York, NY, USA: Association for Computing Machinery, Jan. 27, 2010, pp. 92–105.
28. D. G. Andersen, J. Franklin, M. Kaminsky, et al., “Fawn: A fast array of wimpy nodes,” 7, vol. 54, New York, NY, USA: Association for Computing Machinery, Jul. 1, 2011, pp. 101–109.
29. Redis, Redis, https://redis.io.
30. Cloudflare, What is anycast DNS? Cloudflare Learning Center: https://www.cloudflare.com/learning/dns/what-is-anycast-dns/.
31. J. Iyengar, M. Thomson, et al., “QUIC: A UDP-based multiplexed and secure transport,” in *RFC 9000*, Internet Engineering Task Force (IETF) Fremont, CA, USA, 2021.
32. Google, Google Peering Infrastructure Map, https://peering.google.com/#/infrastructure.
33. Google Cloud, Cloud CDN Locations, https://cloud.google.com/vpc/docs/edge-locations.
34. Atlassian, Configuration management in microservices, https://www.atlassian.com/microservices/microservices-architecture/configuration-management.
35. Red Hat, Ansible, Website: https://ansible.io.
36. Chef, Website: https://chef.io.
37. Puppet, Website: https://puppet.com.
38. HashiCorp, Terraform by HashiCorp.
39. Google Cloud, Cloud Operations Suite (formerly Stackdriver), https://cloud.google.com/products/observability.
40. B. Beyer, N. R. Murphy, D. K. Rensin, K. Kawahara, and S. Thorne, *The Site Reliability workbook: practical ways to implement SRE*. ”O’Reilly Media, Inc.”, 2018.
41. P. Reynolds, J. L. Wiener, J. C. Mogul, M. K. Aguilera, and A. Vahdat, “WAP5: Black-box performance debugging for wide-area systems,” in *Proceedings of the 15th International Conference on World Wide Web*, ser. WWW ’06, New York, NY, USA: Association for Computing Machinery, May 23, 2006, pp. 347–356.
42. P. Bahl, R. Chandra, A. Greenberg, et al., “Towards highly reliable enterprise network services via inference of multi-level dependencies,” in *Proceedings of the 2007 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications*, ser. SIGCOMM ’07, New York, NY, USA: Association for Computing Machinery, Aug. 27, 2007, pp. 13–24.
43. P. Reynolds, C. E. Killian, J. L. Wiener, et al., “Pip: Detecting the unexpected in distributed systems.,” in *NSDI*, vol. 6, 2006, pp. 9–9.
44. P. Barham, R. Isaacs, R. Mortier, and D. Narayanan, “Magpie: Online modelling and performance-aware systems,” in *9th Workshop on Hot Topics in Operating Systems (HotOS IX)*, 2003.
45. R. Fonseca, G. Porter, R. H. Katz, and S. Shenker, “X-Trace: A pervasive network tracing framework,” in *4th USENIX Symposium on Networked Systems Design & Implementation (NSDI 07)*, 2007.
46. Google Cloud, Cloud Trace API Reference, https://cloud.google.com/trace/docs/reference.
47. G. Ren, E. Tune, T. Moseley, et al., “Google-Wide Profiling: A continuous profiling infrastructure for data centers,” *IEEE Micro*, vol. 30, no. 4, pp. 65–79, 2010.
48. H. Shen, K. Pszeniczny, R. Lavaee, et al., “Propeller: A profile guided, relinking optimizer for warehouse-scale applications,” in *Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2*, ser. ASPLOS 2023, New York, NY, USA: Association for Computing Machinery, Jan. 30, 2023, pp. 617–631.
49. Z. Zhou, V. Gogte, N. Vaish, et al., “Characterizing a memory allocator at warehouse scale,” in *Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3*, ser. ASPLOS ’24, vol. 3, New York, NY, USA: Association for Computing Machinery, Apr. 27, 2024, pp. 192–206.
50. A. H. Hunter, C. Kennelly, P. Turner, et al., “Beyond malloc efficiency to fleet efficiency: A hugepage-aware memory allocator,” in *15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)*, 2021, pp. 257–273.
51. U. Savagaonkar, N. Porter, N. Taha, B. Serebrin, and N. Mueller, “Titan in depth: Security in plaintext,” Google Cloud Identity and Security Blog, 2017.
52. Amazon Web Services, “Security Design of the AWS Nitro System,” Tech. Rep.
53. OpenTitan, OpenTitan - Open Source Silicon Root of Trust, https://opentitan.org/.
54. Chips Alliance, Caliptra - Hardware Root of Trust, GitHub Repository: https://github.com/chipsalliance/Caliptra.
55. Intel, Intel Resource Director Technology (Intel RDT), https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html.
56. Spectre Attack, https://spectreattack.com/, 2018.

**오픈 액세스 (Open Access)** 이 챕터는 크리에이티브 커먼즈 저작자 표시 4.0 국제 라이선스(http://creativecommons.org/licenses/by/4.0/)의 조건에 따라 라이선스가 부여됩니다. 이 라이선스는 원저작자와 출처에 대해 적절한 크레딧을 제공하고, 크리에이티브 커먼즈 라이선스에 대한 링크를 제공하며, 변경 사항이 있는 경우 이를 표시하는 한, 어떠한 매체나 형식으로든 사용, 공유, 각색, 배포 및 복제를 허용합니다.

이 챕터의 이미지나 기타 제3자 자료는 자료에 대한 크레딧 라인에 달리 명시되지 않는 한 챕터의 크리에이티브 커먼즈 라이선스에 포함됩니다. 자료가 챕터의 크리에이티브 커먼즈 라이선스에 포함되지 않고 귀하의 의도된 사용이 법적 규정에 의해 허용되지 않거나 허용된 사용을 초과하는 경우, 저작권 소유자로부터 직접 허가를 받아야 합니다.