다음은 **Barroso et al., *The Data Center as a Computer* (2026 Edition)**의 8장 "Performance and Costs"를 한국어로 번역하여 마크다운 형식으로 정리한 내용입니다.

---

# 8장. 성능과 비용 (Performance and Costs)

## 8.1 성능

### 8.1.1 광범위한 시스템 성능 특성 분석

#### 8.1.1.1 성능과 효율성을 위한 시스템 균형
컴퓨터 아키텍트들은 WSC(Warehouse-Scale Computer, 창고형 컴퓨터)를 구성하는 다양한 구성 요소(building blocks)들로부터 성능과 용량의 균형 잡힌 조합을 찾도록 훈련받습니다. 아래에서 우리는 전체 WSC 시스템을 고려할 때만 명확해지는 올바른 구성 요소의 몇 가지 예를 논의합니다. 균형은 이 수준에서도 반드시 다루어져야 합니다.

세 가지 중요한 관찰은 사용 가능한 옵션들의 틀을 더 잘 잡는 데 도움이 됩니다.

*   **워크로드는 불변하는 것은 아니지만, 관성(inertia)을 가지고 있습니다.** 개발자들은 새로운 설계 대안에 더 잘 맞도록 주요 서비스를 재구성할 수 있습니다. 동시에, 그러한 변경에는 높은 비용(그리고 기회비용)이 따르기 때문에, 이러한 최적화는 가장 리소스를 많이 사용하는 서비스에 대해서만 일어나는 경향이 있습니다. 또한, 컴파일러를 통해서조차 프로그래밍하기 너무 복잡한 머신이 되지 않도록 주의해야 합니다. 이 원칙은 앞서 6장에서 논의된 TPU나 VCU와 같은 맞춤형 가속기에 영향을 미칩니다.
*   **가장 비용 효율적이고 균형 잡힌 하드웨어 구성이 특정 단일 워크로드에는 완벽하게 맞지 않을 수 있지만, 모든 워크로드의 결합된 리소스 요구 사항에는 여전히 가장 적합할 수 있습니다.** 예를 들어, 탐색(seek)이 제한적인 애플리케이션은 초대형 디스크 드라이브의 용량을 완전히 사용하지 못할 수 있지만, 주로 아카이브 목적으로 공간이 필요한 애플리케이션과 그 공간을 공유할 수 있습니다. 이 관찰은 7장에서 논의된 소프트웨어 정의 인프라(software-defined infrastructure)의 동기가 됩니다.
*   **대체 가능한(Fungible) 리소스는 더 효율적으로 사용되는 경향이 있습니다.** 두 가지 대안을 비교할 때, 둘 다 동일한 가동률(utilization)을 달성할 것이라고 가정하지 마십시오! 여러 서버 풀(pool)에 걸쳐 리소스를 유연하게 활용할 수 있는 시스템이 종종 더 효율적입니다. 이 관찰은 자주 과소평가됩니다.

#### 8.1.1.2 WSC의 데이터/스토리지 계층 구조
그림 8.1은 가상의 WSC 데이터/스토리지 계층 구조에 대한 프로그래머의 관점을 보여줍니다. 서버는 하나 또는 두 개의 프로세서 소켓으로 구성되며, 각각 멀티코어 CPU와 내부 캐시 계층 구조, 로컬 공유 및 일관성(coherent) 있는 DRAM, 그리고 NIC 및 선택적 SSD에 대한 접근 권한을 가집니다. 랙(Rack)은 가속기 랙, 디스크 또는 SSD 랙 등 여러 다른 형태로 제공됩니다. (랙은 동종이 아닐 수 있으므로, 단일 랙에 다른 종류의 트레이가 포함될 수 있습니다.)

**[그림 8.1 설명: WSC의 스토리지 계층 구조]**
*   **서버 구조:** L1$+L2$, Last-level cache, Local DRAM, Flash(SSD)로 연결됨.
*   **랙 스위치(Rack switch):** 서버들을 연결.
*   **데이터 센터 패브릭(Data center fabric):** 랙 스위치들을 연결하여 서버, 가속기 랙, 디스크 어플라이언스, 플래시 어플라이언스를 통합.

**표: 계층별 사양 (예시)**
| 계층 | 사양 |
| :--- | :--- |
| **단일 서버** | **DRAM:** 1TB, 80ns, 500GB/s<br>**Flash:** 12TB, 100us, 10GB/s<br>**네트워크:** 랙 내 200 GB/s, 클러스터로는 1/3~1/2 수준 |
| **서버 랙** | 20-40대 서버 |
| **가속기 랙** | 4-20개의 가속기 트레이 |
| **디스크 랙** | 각각 1.2PB를 가진 10개의 어플라이언스 |
| **플래시 랙** | 각각 128TB를 가진 40개의 어플라이언스 |
| **데이터 센터 패브릭** | 10K(1만 대) 머신 규모. 200GB/s 링크로 연결된 랙들의 혼합 |

마지막으로, 이 공간에서의 워크로드 변동(churn)은 WSC 아키텍트들에게 도전 과제를 제시합니다. 소프트웨어 기반이 서버의 수명(6년 이상) 동안 서버 설계가 차선책(suboptimal)이 될 정도로 빠르게 진화할 가능성이 높습니다. 예를 들어, 예비 DIMM 슬롯을 추가하여 서버의 초기 비용을 높이면 나중에 메모리 업그레이드를 허용하여 수명 기간 총 소유 비용(TCO)을 줄일 수 있습니다. 하지만 미래의 DIMM이 각 DIMM의 용량을 두 배로 늘려서 추가 슬롯이 필요 없게 된다면 그러한 예비 슬롯은 불필요한 것이 될까요? 혹은 이중 용량 DIMM이 나오더라도 가격이 얼마나 비쌀지 모릅니다.

이 문제는 전체 WSC에 있어 더욱 중요합니다. 데이터 센터 시설의 수명은 일반적으로 서버 수명의 몇 배인 20년 정도에 걸쳐 있기 때문입니다. 이러한 경우, WSC 시스템의 수명 동안 필요할 수 있는 머신 또는 시설 업그레이드의 종류를 고려하고 시설의 설계 단계에서 이를 감안하는 것이 유용합니다.

#### 8.1.1.3 지연 시간, 대역폭, 용량 정량화
클러스터의 모든 리소스는 클러스터 수준 스위치 패브릭을 통해 접근 가능합니다. 리소스의 상대적 균형은 대상 애플리케이션의 요구에 따라 달라지며 시간이 지남에 따라 변합니다.

그림 8.2는 WSC의 지연 시간, 대역폭 및 용량 특성을 정량화합니다. 설명을 위해 3:1로 오버구독(oversubscribed)된 200 Gbps 네트워크 패브릭이 있는 클러스터를 가정합니다. 네트워크 지연 시간 수치는 TCP/IP 전송을 가정하며, 네트워킹 대역폭 값은 오버구독된 업링크 세트 뒤에 있는 각 서버가 사용 가능한 클러스터 수준 대역폭의 공정한 몫을 사용하고 있다고 가정합니다.

그래프는 각 리소스 풀의 상대적 지연 시간, 대역폭 및 용량을 보여줍니다. 예를 들어, 2-4개의 로컬 SSD에서 사용할 수 있는 대역폭은 약 10 GB/s입니다. 디스크는 성능이 지역성(locality)과 무관하므로 클러스터 수준에서만 표시됩니다.

단일 랙에 들어갈 수 있는 것보다 훨씬 많은 서버를 필요로 하는 대규모 애플리케이션은 지연 시간, 대역폭 및 용량의 이러한 큰 불일치를 효과적으로 처리해야 합니다. 이러한 불일치는 단일 머신에서 볼 수 있는 것보다 훨씬 커서 WSC 프로그래밍을 더 어렵게 만듭니다. WSC 아키텍트의 핵심 과제는 이러한 불일치를 비용 효율적인 방식으로 완화하는 것입니다. 반대로, 소프트웨어 아키텍트의 핵심 과제는 애플리케이션 개발자로부터 이러한 복잡성의 대부분을 숨기는 클러스터 인프라와 서비스를 구축하는 것입니다.
예를 들어, 원래 휴대용 전자 제품용으로 개발된 NAND 플래시는 이제 WSC 시스템에서 흔하며, 그림 8.2에 표시된 것처럼 DRAM과 디스크 사이의 비용 및 성능 격차를 메울 수 있습니다. 그러나 플래시와 같은 중간 비휘발성 메모리 계층은 전통적인 시스템 설계 및 소프트웨어 최적화가 마이크로초 규모의 지연 시간을 지원하지 못하기 때문에 새로운 성능 및 효율성 도전 과제와 기회를 제시하기도 합니다. 높은 성능을 달성하기 위한 간단한 프로그래밍 모델을 제공하기 위해 새로운 하드웨어 및 소프트웨어 기술 세트가 필요합니다 [1].

**(그림 8.2는 WSC 스토리지 계층 수준의 지연 시간과 대역폭을 그래프로 보여줌)**

#### 8.1.1.4 모든 엔지니어가 알아야 할 지연 시간 수치
Google 내에서 우리의 동료인 Jeff Dean은 "모든 엔지니어가 알아야 할" 주요 지연 시간 수치 요약을 유지 관리합니다 [2]. 이러한 대략적인 작업 지연 시간은 엔지니어가 1차 근사값 내에서 처리량, 지연 시간 및 용량을 추론하는 데 도움을 줍니다. 우리는 2025년 기준 현재 하드웨어 속도를 반영하여 그림 8.3의 수치를 업데이트했습니다.

**그림 8.3: 모든 WSC 엔지니어가 알아야 할 지연 시간 수치 (2025년 기준)**

| 작업 (Operation) | 시간 (Time) |
| :--- | :--- |
| L1 캐시 참조 (L1 cache reference) | 1 ns |
| 분기 예측 실패 (Branch misprediction) | 3 ns |
| L2 캐시 참조 (L2 cache reference) | 4 ns |
| 경합 없는 뮤텍스 잠금/해제 (Uncontended mutex lock/unlock) | 16 ns |
| L3 캐시 참조 (L3 cache reference) | 25 ns |
| DRAM 참조 (낮은 가동률) (DRAM reference (low utilization)) | 80 ns |
| DRAM 참조 (높은 가동률) (DRAM reference (high utilization)) | 250 ns |
| Snappy로 1KB 압축 해제 (Decompress 1 KB with Snappy) | 500 ns |
| "Far memory" (CXL) 참조 ("Far memory" (CXL) reference) | 1,000 ns (1 µs) |
| Snappy로 1KB 압축 (Compress 1 KB with Snappy) | 2,000 ns (2 µs) |
| 메모리에서 1MB 순차 읽기 (Read 1 MB sequentially from memory) | 3,000 ns (3 µs) |
| 400Gbps 네트워크에서 1MB 순차 읽기 (Read 1 MB sequentially from 400Gbps network) | 20,000 ns (20 µs) |
| SSD 랜덤 읽기 (SSD random read) | 80,000 ns (80 µs) |
| 로컬 SSD에서 1MB 바이트 순차 읽기 (Read 1 MB bytes sequentially from local SSD) | 250,000 ns (250 µs) |
| 디스크에서 1MB 읽기 (Read 1 MB sequentially from disk) | 500,000 ns (500 µs) |
| 디스크 탐색 (Disk seek) | 10,000,000 ns (10 ms) |
| 캘리포니아→네덜란드→캘리포니아 패킷 전송 | 150,000,000 ns (150 ms) |

### 8.1.2 전체적인 워크로드 특성 분석
WSC는 지배적인 바이너리가 없는 광범위한 워크로드를 실행합니다 (3.2절 참조). 소수의 제품이 전체 부하의 큰 부분을 차지하고(예: Google Cloud 및 검색), 일부 인프라 서비스는 정말 거대하지만(예: Spanner 및 Colossus), 그 각각은 더 큰 시스템 내에서 마이크로서비스를 구현하는 많은 다른 바이너리로 구성됩니다. 따라서 이들 서비스 전반에 걸쳐 공통점을 살펴보는 것이 합리적입니다.

그림 8.4는 모든 서비스에서 사용되는 공통 기능으로 나누어 시간 경과에 따른 Google의 모든 워크로드에 대한 프로필(4장에서 논의된 GWP를 사용하여 생성됨)을 보여줍니다. 우리는 이러한 공통 기능을 "데이터 센터 세금(data center tax)" [3]이라고 명명했는데, 이는 서비스 개발자가 직접 작성한 코드가 아니라 분산 대규모 WSC 컴퓨팅에 필수적인 "세금"과 같은 기능을 수행하기 때문에 실제로 생각하지 않는 기능들이기 때문입니다.

**(그림 8.4 설명: "데이터 센터 세금"에 소비된 전체 사이클 비율. 커널, 보안, RPC, 압축, 해시함수, TCMalloc, Memfunc 등이 포함됨)**

대부분의 사이클은 커널(kernel)에서 소비되며, 이는 대부분 서비스의 메모리 및 I/O 집약적 특성을 반영합니다. "Memfunc"은 메모리 이동 및 복사와 같은 메모리 기능을 나타내며, TCMalloc은 7.1.2절에서 설명한 최적화된 메모리 할당자입니다. RPC 관련 기능은 나머지 "세금" 중 가장 큰 비중을 차지합니다.

다년간의 최적화 노력에도 불구하고, 이 기능 세트에서 소비되는 사이클의 전체 비율은 수년 동안 놀라울 정도로 안정적으로 유지되었습니다. 일부는 증가했고(특히 RPC), 일부는 감소했습니다(특히 커널). 이러한 안정성이 최적화가 효과가 없었음을 의미하는 것은 아니며, 단지 최적화 이득이 사용량 증가로 인해 거의 상쇄되었을 뿐입니다. 사실, 지속적인 최적화가 없었다면 전체 사이클은 상당히 증가했을 것입니다. 그럼에도 불구하고, 안정적인 소비량은 소비되는 사이클을 오프로드(offload)하여 다른 범용 작업을 위해 CPU 사이클을 확보할 수 있는 가속기를 구축할 기회를 강조합니다 [4].

동일한 프로파일링 시스템은 비디오 관련 기능 및 선형 대수(ML용)와 같은 애플리케이션별 라이브러리의 리소스 소비도 추적합니다. 이 두 범주에 대한 ASIC 투자를 감안할 때, CPU에서의 점유율(footprint)은 낮습니다.

### 8.1.3 성능 모델링
하이퍼스케일 인프라 설계자들은 인프라에서 실행되는 사내 애플리케이션에 대한 깊은 이해를 통해 이점을 얻습니다. 이러한 애플리케이션 지식은 일반적인 창고형 컴퓨터에 들어가는 다양한 하드웨어 구성 요소의 아키텍처 및 설계에 대한 중요한 피드백을 제공할 수 있습니다. 이러한 지식의 이점을 활용하기 위해 많은 하드웨어 인프라 제공업체들이 자체 SoC 및 하드웨어 가속기를 설계하기 시작했습니다. 강력한 성능 모델링 및 시뮬레이션 인프라는 설계 단계에서의 아키텍처 탐색(pathfinding), 개발 단계에서의 워크로드 성능 추정, 그리고 테이프아웃(tapeout) 후 최종 하드웨어의 성능 검증을 가능하게 합니다.

#### 8.1.3.1 실리콘에서 클러스터까지 시뮬레이션
성능 시뮬레이션 인프라는 맞춤형 실리콘을 구축하기 위한 핵심 요구 사항이며 시스템 설계의 여러 측면을 지원할 수 있습니다. 애플리케이션에 대한 사내 지식과 하이퍼스케일 인프라 설계자가 누리는 수직적 통합을 고려할 때, 그들은 일반적으로 계층화된 성능 시뮬레이션 방법론(그림 8.5)에 의존합니다. 이는 마이크로아키텍처 또는 코어 수준 시뮬레이션(트레이스를 사용하여 노드 수준 성능의 초기 추정을 수행)부터 시작하여, 실제 운영 수준의 성능을 결정하기 위해 많은 다른 애플리케이션의 전체 클러스터 수준 시뮬레이션에 이르기까지 성능 특성을 모델링하고 시뮬레이션할 수 있습니다. 다음 섹션에서는 이러한 시뮬레이션 각 수준의 고수준 특성을 요약합니다.

*   **코어 시뮬레이터(Core simulator)**는 제안된 CPU 코어를 사이클 수준 정확도로 모델링하고 코어 아키텍처 기능, 마이크로아키텍처 최적화, 트레이드오프 및 사이징 연구를 위한 탐색과 경로 찾기를 용이하게 합니다. 이 시뮬레이터는 또한 RTL 및 실리콘 초기 구동(bring-up) 중에 필요한 부품의 기능적 및 성능적 준비 상태를 평가하는 데 도움을 주어, 실제 하드웨어를 사용할 수 있기 전에 시뮬레이션된 전원 켜기 부팅을 허용합니다.
*   **SoC 시뮬레이터(SoC simulator)**는 다양한 옵션과 그 성능 및 비용 이점을 훑어봄으로써 아키텍처 공간의 설계 및 탐색을 가능하게 합니다. 이는 시스템 및 플랫폼 설계자에게 전체 시스템 운영 비용의 균형을 맞추면서 워크로드 성능 요구 사항을 충족하도록 시스템을 구성해야 하는 다양한 IP(코어, 언코어, 메모리 에이전트, I/O 인터페이스 등)를 선택하는 데 필요한 입력을 제공합니다. 또한 이 시뮬레이터는 라스트 레벨 캐시(LLC), 메시 패브릭, 칩렛 간 통신, 메모리 컨트롤러와 같은 다양한 SoC 구성 요소의 탐색 및 경로 찾기를 가능하게 합니다.
*   **노드 시뮬레이터(Node simulator)**는 미래 하드웨어를 연구하기 위한 전체 시스템 수준 에뮬레이션, 시뮬레이션 및/또는 모의 환경을 제공합니다. 이는 대표적인 워크로드에 대한 네트워킹 및 메모리 대역폭과 메모리 용량 프로비저닝에 대한 현실적인 트레이드오프를 가능하게 합니다. 이 시뮬레이터는 또한 기존 머신과 비교하여 성능 및 비용 이점을 평가하기 위한 워크로드 예측 방법론을 제공합니다. 또한 시뮬레이션 환경 내에서 예비 데이터시트 또는 사양을 구현하고 연구함으로써 업계 및 파트너 장치와 기술, 그리고 그들의 통신 인터페이스 및 프로토콜을 평가할 수 있게 합니다.
*   **클러스터 시뮬레이터(Cluster simulator)**는 마지막으로 창고형 애플리케이션 및 서비스의 특성화를 가능하게 합니다. 클러스터 수준 스케줄링과 머신 수준 동작을 결합하여 실제 운영 클러스터를 시뮬레이션합니다. 이 시뮬레이터는 또한 이질적인 워크로드 및 이질적인 하드웨어 유형에 대한 클러스터 수준 성능 예측을 가능하게 합니다.

#### 8.1.3.2 벤치마킹과 트레이싱
벤치마킹과 트레이싱은 새로운 하드웨어 플랫폼의 성능 모델링 및 평가에 중요한 역할을 합니다. 이들은 초기 플랫폼 성능을 평가하고 최적화를 위한 중요한 입력을 제공하며, 시스템 소프트웨어 변경(BIOS/커널 업데이트) 또는 플랫폼 차이로 인한 성능 회귀(regression)를 식별하고 진단하는 데 도움을 줍니다. 또한 아키텍트가 아키텍처, 형태 및 기능에 대해 데이터에 기반한 결정을 내림으로써 미래 플랫폼의 성능과 비용을 분석할 수 있게 합니다.

창고형 컴퓨터 워크로드는 SPEC과 같은 전통적인 벤치마크가 포착하는 것과는 근본적으로 다른 특성을 가지고 있습니다. 창고형 워크로드에 대한 최적의 효율성을 달성하려면 현대 컴퓨터 아키텍처 및 설계에 변화가 필요합니다. 예를 들어, Google 워크로드는 현대 CPU 캐시의 용량을 초과하는 데이터 및 명령어 풋프린트(footprint)를 가지고 있어, CPU가 코드와 데이터를 기다리는 데 상당한 시간을 소비합니다(7.1.1절). 따라서 SPEC 벤치마크가 유용하긴 하지만, 현재 또는 미래 하드웨어에 대한 더 나은 성능 평가를 위해 다른 데이터 소스로 이를 보완합니다.

**Fleetbench** [5]는 Google 워크로드를 위한 벤치마킹 모음으로, Google 전체의 핫(hot) 함수에 대한 선별된 마이크로벤치마크 세트로 구성됩니다. 벤치마크 실행에 사용되는 데이터 세트 분포는 실제 운영 환경에서 수집된 데이터에서 파생됩니다. 이는 칩 벤더, 컴파일러 연구원 및 Google과 유사한 창고형 워크로드에 유익한 성능 최적화를 수행하는 데 관심이 있는 사람들이 사용하도록 의도되었습니다. 더 최근에는 업계에서 ML 시스템의 성능을 벤치마킹하기 위해 MLPerf가 사용되었습니다. Google과 같은 대규모 사용자는 종종 워크로드를 더 정확하게 나타내는 다른 내부 벤치마크로 MLPerf를 보완합니다.

실제 창고형 애플리케이션의 워크로드 트레이스 [6]는 시스템 설계자가 WSC 워크로드를 더 잘 이해하는 데 중요한 역할을 하는 또 다른 도구입니다. 이는 또한 프로세서 프론트엔드, 온-다이(on-die) 상호 연결, 캐시 및 메모리 서브시스템 등 WSC 워크로드에 큰 영향을 미치는 모든 영역에 대한 연구를 가능하게 합니다. 예를 들어, 메모리 트레이스는 프론트엔드 병목 현상을 추적하고 시스템 설계자가 새로운 최적화를 설계할 수 있도록 하는 상시(always-on) 전체 모니터링 시스템인 AsmDB(7.1.1절)의 핵심 입력입니다.

#### 8.1.3.3 프로덕션 성능 모델링
창고형 규모 인프라의 성능 평가에는 실제 운영 환경에서 창고형 애플리케이션의 특성을 포착하고 표현할 수 있는 가볍고 비침해적인(non-intrusive) 도구가 필요합니다. 예를 들어, **WSMeter** [7]는 실제 운영 환경에서의 실제 작업(job) 동작을 사용하여 WSC를 평가합니다. 이는 WSC 성능과 관련된 두 가지 주요 과제를 해결합니다. 첫째, 수천 개의 워크로드를 통합하고 성능을 요약하는 전체론적 지표의 부재, 둘째, 라이브 환경에서 평가를 수행하는 데 따르는 높은 비용과 위험입니다. Google에서는 각 작업의 MIPS(Million Instructions Per Second) 등급의 할당량 가중 합계를 사용하는 새로운 WSC 성능 지표(내부적으로 GCU 또는 Google Compute Units라고 함)를 사용하여 이러한 문제를 해결합니다. 이 가중 합계는 작업의 다양성과 분포를 포착하여 라이브 운영 환경에서 성능을 보다 정확하고 비용 효율적으로 평가할 수 있게 합니다.

WSC 규모에서의 평가 비용을 해결하기 위해, 작업 간의 독립성과 중심 극한 정리에 기반한 통계적으로 근거 있는 샘플링 전략이 신중하게 선택된 소수 샘플에 대해 제약 조건 최적화기를 실행하여 낮은 오버헤드(때로는 전체의 1% 미만)로 정확한 추정치를 얻습니다. 또 다른 최적화인 **Sage** [8]는 비지도 ML 모델을 사용하여 트레이스 라벨링 비용을 우회하고 창고형 애플리케이션의 다양한 서비스 간의 종속성을 학습합니다. Sage는 영리한 기술을 사용하여 서로 다른 마이크로서비스가 서로 어떻게 영향을 미치는지 이해하고 실제 서비스를 중단하지 않고 "what-if" 시나리오를 테스트함으로써 다양한 마이크로서비스 전반의 성능 문제에 대한 근본 원인을 정확하게 식별합니다. 특정 마이크로서비스와 리소스(CPU 또는 네트워크 등)가 문제를 일으키는 것을 찾아내고 해결하는 데 도움을 주어 더 예측 가능하고 성능이 뛰어나며 효율적인 WSC로 이어집니다. Sage와 같은 도구는 애플리케이션 성능 특성을 모델링할 뿐만 아니라 성능 및 QoS 회귀를 신속하게 디버깅하고 근본 원인을 파악하며 수정하기 위해 운영 환경에서 사용할 수 있는 귀중한 도구 역할도 합니다.

---

## 8.2 비용

### 8.2.1 총 소유 비용(TCO) 모델

2장에서 설명했듯이, 창고형 컴퓨터(WSC)의 정의적인 특징 중 하나는 규모에 따른 비용 효율성을 강조한다는 점입니다. 이를 더 잘 이해하기 위해 데이터 센터의 총 소유 비용(TCO)을 살펴보겠습니다. 최상위 수준에서 비용은 자본 지출(Capex)과 운영 지출(Opex)로 나뉩니다. Capex는 선불로 이루어져야 하고 일정 기간 동안 감가상각되는 투자를 말합니다. 데이터 센터 건설 비용이나 서버 구매 가격이 그 예입니다. Opex는 감가상각을 제외하고 장비를 실제로 운영하는 데 드는 반복적인 월별 비용을 말합니다. 전기 요금, 수리 및 유지 보수, 현장 인력 급여 등이 있습니다. 따라서 우리는 다음과 같은 식을 갖게 됩니다.

$$TCO = DC\_depreciation + DC\_opex + server\_depreciation + server\_opex$$

우리는 이 장에서 최상위 추정치(top-line estimates)에 초점을 맞추고 적절한 경우 모델을 단순화합니다. 학술적인 목적을 위해 우리의 단순화된 모델은 모든 주요 비용을 모델링하기에 충분히 정확합니다. 실제 데이터 센터와 비교했을 때 부정확성의 주요 원인은 건설 비용과 같은 모델 입력값이 될 것입니다.

#### 8.2.1.1 자본 비용 (Capital costs)
데이터 센터 건설 비용은 설계, 규모, 위치 및 원하는 건설 속도에 따라 크게 달라집니다. 놀랍지 않게도, 신뢰성과 이중화(redundancy)를 추가하면 데이터 센터 비용이 더 비싸지며, 소규모 데이터 센터는 고정 비용을 많은 와트(watt)로 상각할 수 없기 때문에 더 비싼 경향이 있습니다.

2023년 기준으로 대규모 데이터 센터의 건설 비용은 위치에 따라 $6/W에서 $13/W 사이였습니다 [9]. 이 비용은 인플레이션으로 인해 지난 10년 동안 상승했으며 건물 외피(shell)와 내부 설비(fitout)를 모두 포함하지만, 토지 비용, 변전소, 전력 및 수도 유틸리티 연결과 같은 부지 수준 비용은 제외합니다. 이러한 비용은 지역 상황에 따라 $1-2/W를 추가할 수 있습니다.

비용은 지리적 지역에 따라 크게 다릅니다. 특히 인건비와 생산성은 상당히 다를 수 있습니다. 예를 들어, 소규모 시장에는 자격을 갖춘 전기 기술자가 충분하지 않아 먼 곳에서 전기 기술자를 채용해야 할 수 있으며, 이는 더 높은 임금을 지불하거나 주거 지원을 제공해야 할 수 있습니다. 경험칙(rule of thumb)으로 볼 때, 데이터 센터 건설 비용의 약 50%는 인건비입니다.

또한 세금과 건설 표준(예: 지진 위험)이 비용에 영향을 미칩니다. 소규모 시장에서는 비용 지수가 고객 구성(mix)에 의해 영향을 받을 수도 있습니다. 예를 들어, 맞춤형 데이터 센터를 짓는 은행 대 표준화된 설계를 짓는 하이퍼스케일러의 차이입니다.

비용을 와트당 달러로 특성화하는 것은 대규모 데이터 센터(규모 독립적인 고정 비용이 전체 비용에서 상대적으로 작은 부분을 차지하는 경우)에 합리적입니다. 데이터 센터의 주요 구성 요소인 전력, 냉각, 공간이 대략 와트와 선형적으로 비례하기 때문입니다. 일반적으로 전체 건설 비용의 약 60-80%는 전력 및 냉각 인프라에 사용되며, 나머지 20-40%는 일반 건물 및 부지 건설에 사용됩니다.

비용은 원하는 이중화 및 가용성 정도에 따라 달라지므로, 우리는 항상 비용을 **임계 와트(critical watt)당 달러**로 표현합니다. 즉, IT 장비가 실제로 사용할 수 있는 와트입니다. 예를 들어, 20MW 발전기를 갖춘 데이터 센터가 $2N$ 구성으로 구축되어 6MW의 임계 전력만 제공할 수 있습니다(플러스 4MW는 냉각기 전력 공급). 따라서 건설 비용이 1억 2천만 달러라면 $6/W가 아니라 $20/W입니다. 업계 보고서에서는 임계 전력이라는 용어를 올바르게 사용하지 않는 경우가 많아, 예시 데이터 센터가 20MW 데이터 센터로 설명되거나, 30MW를 공급할 수 있는 변전소에서 공급받는 경우 30MW 데이터 센터라고 설명될 수도 있습니다. 마찬가지로, 지난 장에서 소개된 전력 오버구독(9.4절에서 더 논의됨)도 프로비저닝된 와트(provisioned watts)와 임계 와트를 구분합니다. 2억 4천만 달러가 드는 30MW 데이터 센터를 40MW로 오버구독할 수 있다면 비용은 $8/W가 아니라 $6/W가 됩니다.

충분히 낮은 신뢰성 표준을 적용하면 건설 비용을 획기적으로 줄일 수 있습니다. 예를 들어, 암호화폐 채굴 데이터 센터는 $1-2/W로 구축할 수 있습니다. 훨씬 낮은 비용(더 짧은 건설 시간과 함께 제공됨)을 대가로, 이러한 데이터 센터는 일반적으로 이중화가 없으며 단일 사용 사례(고밀도 ASIC 또는 GPU 랙)에 최적화되어 있고, 더 높은 주변 온도 변화와 더 높은 장비 고장률을 용인합니다. 채굴자들은 최소 비용을 위해 직접 공기 냉각을 사용하거나 특수 칩을 위해 침수 냉각(immersion cooling)을 사용하는 등 다양한 설계를 사용합니다. 예를 들어, 그림 8.6의 컨테이너형 데이터 센터 오두막(hut)은 환경에 개방되어 있으며 각각 최대 1.2MW의 용량을 가지고 정가가 100만 달러입니다 [10]. 컨테이너 비용 외에도 외부 변압기 및 전력선, 부지 작업(정지 작업 및 자갈), 울타리 등에 대한 추가 비용이 포함되므로 최종 비용은 $1/W 이상이 될 것입니다.

때때로 건설 비용은 평방피트당 달러로 인용되지만, 그 지표는 프로젝트를 적절하게 비교할 수 없고 와트당 달러로 표현된 비용보다 훨씬 더 일관성 없이 사용되기 때문에 덜 유용합니다. 특히 계산에 포함하거나 제외할 공간에 대한 표준 정의가 없으며, 이 지표는 데이터 센터 건설의 주요 비용 동인인 임계 전력과 잘 상관되지 않습니다. 따라서 대부분의 업계 전문가는 비용을 표현할 때 평방피트당 달러를 사용하는 것을 피합니다.

초기 건설 비용에서 발생하는 월별 감가상각 비용(또는 상각 비용)은 투자가 상각되는 기간(예상 수명과 관련됨)과 가정된 이자율에 따라 달라집니다. 대부분의 데이터 센터 장비는 15-25년, 건물 외피의 경우 최대 40년에 걸쳐 감가상각됩니다. 미국 회계 규칙에 따르면 자산 가치가 매달 일정 금액씩 감소하는 정액법 감가상각을 사용하는 것이 일반적입니다. 예를 들어, $12/W 데이터 센터를 12년에 걸쳐 감가상각하면 감가상각 비용은 월 $0.08/W입니다. 8%의 이자율로 건설 자금을 조달하기 위해 대출을 받았다면 관련 월별 이자 지급액은 $0.05/W가 추가되어 총 월 $0.13/W가 됩니다. 이자율은 시간에 따라 변하지만, 많은 회사는 7–12% 범위의 자본 비용 비율을 사용합니다.

서버 비용도 비슷하게 계산되지만 서버의 수명이 더 짧다는 점이 다릅니다. 최근 몇 년 동안 모든 주요 클라우드 공급자는 관찰된 서버 수명 증가에 따라 서버 감가상각 기간을 6년으로 전환했습니다. 여기서 수명은 기술적 수명(서버가 고장 나기 전까지 얼마나 오래 작동할 수 있는가)이 아닌 경제적 수명(서버를 실행하는 것이 경제적으로 얼마나 타당한가)을 나타냅니다. 수명이 늘어난 것은 서버의 품질이 높아져서 더 오래 지속되기 때문이 아니라 무어의 법칙(Moore’s Law)이 둔화되었기 때문입니다. 이유를 알아보기 위해 서버 성능이 매년 두 배로 증가하고 가격은 동일하게 유지된다고 가정해 보겠습니다. 3년 후, 구형 서버는 성능의 1/8에 불과하므로 완전히 감가상각되어("공짜") 있더라도 이 서버를 실행하는 비용(전력, 공간 등)이 최신 서버 비용의 1/8보다 크기 때문에 더 이상 경제적이지 않습니다. 반면, 서버 가격/성능이 연간 10%만 향상된다면 5년 된 서버도 여전히 가격/성능 경쟁력이 있습니다.

서버 및 데이터 센터 비용을 정규화하기 위해, 서버의 실제 피크 전력 소비를 분모로 사용하여 서버 비용을 와트당으로 특성화하는 것이 유용합니다. 예를 들어, 피크 전력 소비가 1100W인 $16,000 서버는 $14.5/W입니다. 6년에 걸쳐 감가상각하면 서버 비용은 월 $0.20/W이고 연 8%의 이자가 월 $0.05/W를 추가하여 총 월 $0.25/W가 됩니다.

이전 장에서 논의했듯이, 무어의 법칙 둔화로 인해 WSC는 와트당 성능을 개선하기 위해 점점 더 하드웨어 가속기로 눈을 돌리고 있습니다. 내부적으로 개발된 가속기의 Capex에는 ASIC을 설계하고 제조하는 비용인 비반복 엔지니어링(NRE) 비용뿐만 아니라 주변 인프라 비용도 포함됩니다. 100,000개의 가속기를 개발하고 배포하는 데 일회성 비용이 5,000만 달러가 든다면 NRE 비용은 유닛당 $500이므로, 각 칩은 순수 제조 비용보다 $500 더 비쌉니다. 따라서 맞춤형 하드웨어는 대량으로 생산되고 상당한 비용/성능 이점을 제공하는 경우에만 경제적으로 합리적이며, 그렇지 않으면 개발 비용과 위험이 이점보다 큽니다.

#### 8.2.1.2 운영 비용 (Operational costs)
데이터 센터 운영 비용(Opex)은 운영 표준(예: 보안 요원이 동시에 몇 명 근무하는지, 발전기를 얼마나 자주 테스트하고 서비스하는지)과 데이터 센터 규모에 따라 크게 달라집니다. 대규모 데이터 센터는 고정 비용이 더 잘 상각되기 때문에(MW당) 더 저렴합니다. 비용은 또한 지리적 위치(기후, 세금, 급여 수준 등)와 데이터 센터의 설계 및 연령에 따라 달라질 수 있습니다. 간단하게 하기 위해, 우리는 운영 비용을 보안 요원, 유지 보수, 전기와 같은 항목을 나타내는 와트당 월별 요금으로 나눌 것입니다. 미국의 대규모 데이터 센터에 대한 일반적인 운영 비용은 감가상각, 이자 및 실제 전기 비용을 제외하고 월 $0.05에서 $0.10/W 범위입니다. (이 숫자는 고객에게 청구되는 시장 가격이 아니라 비용을 반영한다는 점에 유의하십시오.)

마찬가지로 서버에도 운영 비용이 있습니다. 서버 유지 보수 비용은 서버 유형과 유지 보수 표준(예: 응답 시간이 4시간 대 2영업일) 및 데이터 센터 규모에 따라 크게 다릅니다.

전통적인 IT 환경에서 서버 비용은 매우 다릅니다. 운영 비용의 대부분이 애플리케이션, 즉 소프트웨어 라이선스와 시스템 관리자, 데이터베이스 관리자, 네트워크 엔지니어 등의 비용에 있기 때문입니다. 우리는 WSC 인프라를 실행하는 비용에 초점을 맞추고 있고, 상황에 따라 애플리케이션 비용이 크게 다르기 때문에 여기서는 이러한 비용을 제외합니다. 소규모 기업 환경에서는 수십 대의 서버당 한 명의 시스템 관리자를 보는 것이 드문 일이 아니며, 이는 상당한 머신당 연간 비용을 초래합니다 [11].

### 8.2.2 TCO 세부 내역 및 사례 연구

관련된 변수가 많으므로, 다양한 배포 종류를 나타내는 시나리오를 통해 비용 요소를 설명하는 것이 가장 좋습니다.

먼저, 미국의 일반적인 신규 멀티 메가와트 기업 데이터 센터를 고려합니다(5장에서 논의된 Uptime Institute의 Tier III 분류에 가까운 것). 나머지 기본 사례 매개변수는 다음과 같습니다.

*   전기 비용은 2024년 평균 미국 산업 요금인 8센트/kWh입니다.
*   기업이 대출에 대해 지불해야 하는 이자율은 8%이며, 서버는 6년 리스로 자금을 조달합니다. 이 가정은 대부분의 기업이 자본 비용에 대해 더 높은 이율을 사용하므로 온프레미스 사례에 유리합니다.
*   데이터 센터 건설 비용은 $10/W이며, 20년에 걸쳐 상각되고 동일한 8% 이자율로 자금을 조달합니다.
*   데이터 센터 Opex는 월 $0.05/W입니다.
*   데이터 센터의 전력 사용 효율(PUE)은 현재 업계 평균인 1.5입니다.
*   서버 수명은 6년이며, 서버 수리 및 유지 보수는 연간 Capex의 5%입니다.
*   서버의 평균 전력 소모량은 피크 전력의 75%입니다.

**[그림 8.7 설명: 사례 연구 A(왼쪽)와 B(오른쪽)의 비용 분석]**
*   구성 요소: DC 상각, DC 이자, DC Opex, 서버 상각, 서버 이자, 서버 Opex, 서버 전력, PUE 오버헤드.

이 값들과 아래의 값들은 실제 비용을 대략적으로 대표하도록 의도되었습니다.

**사례 A**의 경우, 일반적인 기업이 구매할 만한 서버를 선택합니다. 두 개의 빠른 16코어 CPU, 512GB RAM, 2TB SSD가 있는 2소켓 서버입니다. 이 서버가 피크 시 950W를 소비하고 2024년 기준으로 약 $29,000의 비용이 든다고 가정해 보겠습니다. 데이터 센터가 완전히 채워졌다고 가정할 것입니다(비현실적인 가정이지만 일단 보류).

그림 8.7의 왼쪽은 사례 A에 대한 연간 TCO 분석을 데이터 센터 및 서버 관련 Opex 및 Capex 구성 요소로 나누어 보여줍니다. 고전적인 데이터 센터의 전형인 이 예에서, 높은 서버 자본 비용이 전체 TCO를 지배하며, 월별 비용의 71%가 서버 구매와 관련되고 추가로 11%가 서버 Opex와 관련됩니다.

일용품(Commodity) 기반의 저비용(아마도 낮은 신뢰성) 서버나 더 높은 전력 가격은 상황을 상당히 바꿀 수 있습니다. **사례 B**(그림 8.7 오른쪽)의 경우, 피크 시 1100W를 소비하고 비용은 $16,000에 불과한 더 저렴하고, 빠르고, 고전력인 서버를 전기 비용이 $0.10/kWh이고 PUE가 1.1인 위치에서 가정합니다. 이 경우 데이터 센터 관련 비용은 전체의 29%로 증가하고 에너지 비용은 12%로 증가합니다. 즉, 서버의 호스팅 비용(즉, 이를 수용하기 위한 모든 인프라 및 전력 비용)이 서버 구매 비용에 근접하기 시작합니다.

가정된 더 높은 전력 가격과 더 높은 서버 전력에도 불구하고, 서버가 더 저렴하기 때문에 사례 B의 절대적인 6년 TCO는 사례 A보다 낮습니다($36,000 대 $51,000).

### 8.2.3 실제 비용: 가동률이 중요하다!

사실, 실제 데이터 센터 비용은 지금까지 모델링한 것보다 훨씬 높습니다. 지금까지 제시된 모델은 데이터 센터가 100% 꽉 차 있고 서버가 상당히 바쁘다고(피크 전력의 75%는 약 50%의 CPU 사용률에 해당함; 9장 참조) 가정합니다. 실제로는 데이터 센터와 서버 모두에서 이렇게 높은 가동률은 드뭅니다. 예를 들어, 데이터 센터 공간은 구축하는 데 시간이 걸리기 때문에 미래의 성장을 수용하기 위해 일부 빈 공간을 유지해야 할 수 있습니다. 또한 서버 레이아웃은 종종 지나치게 높은(최악의 경우) 전력 소비를 가정합니다. 서버는 모든 옵션(최대 메모리, 디스크, PCI 카드 등)을 설치하면 최대 1100W를 소비할 수 있지만, 실제로 배포된 구성은 500W만 사용할 수 있습니다. 서버 레이아웃이 1100W라는 명판 정격을 가정하면 가동률이 45%에 불과하게 되고, 따라서 서버당 실제 데이터 센터 비용은 두 배 이상이 됩니다. 따라서 실제로 서버당 월별 실제 비용은 데이터 센터 관련 비용이 데이터 센터 전력 가동률에 반비례하여 증가하기 때문에 사례 A와 B에 표시된 것보다 훨씬 높은 경우가 많습니다.

나중에 9장에서 논의하겠지만, 높은 데이터 센터 전력 가동률을 달성하는 것은 생각만큼 간단하지 않습니다. 공급업체가 특정 구성에 대한 실제 최대 전력 소모량을 계산하는 전력 계산기를 제공하더라도 그 값은 100% CPU 사용률을 가정합니다. 해당 값을 기준으로 서버를 설치하고 평균 30% CPU 사용률(1100W 대신 400W 소비)로 실행한다면, 우리는 데이터 센터 전력의 절반 이상을 낭비한 것이고 비용은 두 배 이상이 된 것입니다! 그러나 평균값인 400W를 기준으로 서버를 설치했는데 월말에 서버가 잠시 동안 거의 전체 용량으로 실행된다면 데이터 센터가 과열되거나 차단기가 내려갈 수 있습니다. 마찬가지로, 나중에 서버에 추가 RAM이나 디스크를 추가하기로 선택할 수 있는데, 전력 소비 계산에 여유를 두지 않았다면 서버 랙의 물리적 압축 해제(decompaction)가 필요할 것입니다.

따라서 실제 민간 데이터 센터 운영자는 이러한 문제에 대비하기 위해 상당한 양의 여유 공간을 남겨둡니다. 20–50%의 예비(Reserves)가 일반적이며, 이는 실제 데이터 센터가 정격 용량 근처에서 실행되는 경우가 거의 없음을 의미합니다. 다시 말해, 10MW의 임계 전력을 가진 데이터 센터는 종종 월 평균 실제 임계 전력이 4–6MW에 불과합니다(플러스 PUE 오버헤드).

부분적으로 채워진 데이터 센터를 모델링하기 위해, 우리는 Capex 및 Opex 비용(전력 제외)을 점유율의 역수로 확장합니다. 예를 들어, 절반만 채워진 데이터 센터는 와트당 단가가 100% 더 높습니다. 위의 사례 B를 가져와 50% 점유율을 적용하면, 데이터 센터 비용은 이제 TCO의 45%(그림 8.8)를 차지하며, 이는 29%에서 증가한 수치입니다. 이 경우가 실제 비용에 더 가깝습니다.

부분적으로 사용된 서버는 전력을 덜 사용하기 때문에 운영 비용에 긍정적인 영향을 미칩니다. 물론 이러한 절감액은 해당 서버에서 실행되는 애플리케이션이 비즈니스 가치를 덜 생산할 가능성이 높기 때문에 의심스럽습니다. 우리의 TCO 모델은 물리적 인프라 비용에 기반하고 이 하드웨어에서 실행되는 애플리케이션을 제외하므로 이 효과를 포착할 수 없습니다. 이 엔드투엔드 성능을 측정하기 위해 애플리케이션 가치에 대한 대리 지표(예: 완료된 은행 거래 수 또는 웹 검색 수)를 측정하고 TCO를 해당 숫자로 나눌 수 있습니다. 예를 들어, 월 100만 달러 비용이 드는 데이터 센터에서 월 1억 건의 거래를 완료한다면 거래당 비용은 1센트입니다. 반면, 한 달에 트래픽이 낮아 5천만 건의 거래만 완료한다면 거래당 비용은 2센트로 두 배가 됩니다. 이 장에서는 하드웨어 비용에만 전적으로 집중했지만, 궁극적으로 소프트웨어 성능과 서버 가동률이 그만큼 중요하다는 점을 명심하는 것이 중요합니다. 이러한 문제는 더 높은 가치를 제공하지만 소프트웨어 생태계 지원을 위한 추가 비용도 발생하는 가속기의 맥락에서 더욱 악화됩니다.

### 8.2.4 퍼블릭 클라우드의 비용 이점

자체 데이터 센터와 서버를 구축하는 대신, 기업은 Google의 Compute Engine이나 Amazon의 EC2와 같은 퍼블릭 클라우드 제공업체로부터 가상 머신(VM)을 임대할 수 있습니다. 이전 예제에서 사용된 서버는 대략 GCE c3d-highmem-60 인스턴스와 비슷하며, 온디맨드 인스턴스의 경우 시간당 $3.67, 스팟 인스턴스의 경우 시간당 $1.47, 3년 약정의 경우 시간당 $1.65(인스턴스 스토리지 비용 무시)로 가정합니다. 예제의 2TB SSD를 고려하면, 시간당 클라우드 인스턴스 비용은 $3.94(온디맨드), $1.74(스팟) 또는 $1.92(3년 약정)가 됩니다. 이 모든 수치는 2024년 중반 기준입니다.

이를 비용 모델과 비교하기 전에, 세 가지 매우 다른 요금제를 고려해 보십시오. 온디맨드 요금은 "사용한 만큼 지불(pay-as-you-go)"하는 유연성에 최적화되어 있습니다. 언제든지 VM을 시작하고 중지할 수 있으므로 1년에 며칠만 필요한 경우 온디맨드 요금이 다른 대안보다 훨씬 저렴할 것입니다. 예를 들어, 평일에 하루 4시간 동안의 피크 부하를 처리하기 위해 두 대의 서버가 필요하고 나머지 시간에는 한 대의 서버만 필요한 경우가 있습니다. 온디맨드 인스턴스를 사용하면 서버를 소유했을 때의 168시간에 비해 주당 20시간에 대해서만 비용을 지불하므로 8:1의 절감 효과가 있습니다.

온디맨드 인스턴스는 더 비쌉니다. 시간당 $3.94로 3년 동안 사용하면 $103,500가 듭니다. 하지만 정상 상태(steady-state) 사용은 온디맨드 VM이 사용되는 용도가 아닙니다. 온디맨드 서버가 주당 20시간만 필요한 위의 예를 사용하면 3년 동안의 총 온디맨드 비용은 $12,300입니다.

3년 동안 지속적으로 서버가 필요한 경우, 퍼블릭 클라우드 제공업체는 이 장기 약정에 대한 대가로 시간당 가격을 할인해 줍니다. 3년 약정 시, GCP는 예제 서버를 55% 이상 할인하여 3년 비용이 $50,500가 됩니다.

온디맨드 인스턴스는 왜 상대적으로 비쌀까요? 클라우드 제공업체는 당신이 서버를 필요로 할지 알 수 없으므로, 누군가 원할 경우를 대비해 추가 서버를 준비해 두어야 합니다. 따라서 온디맨드 인스턴스에 사용되는 서버 풀의 가동률은 상당히 낮으며, 따라서 월별 수익도 낮습니다. 예를 들어, 일반적인 온디맨드 인스턴스가 트래픽이 최고조에 달하는 하루 6시간을 커버한다면 가동률은 25%가 되고, 따라서 시간당 비용은 24시간 실행되는 "기저 부하(base load)" 인스턴스보다 4배 더 높습니다.

온디맨드 인스턴스가 하루 종일 활용되지 않기 때문에, 대부분의 클라우드 제공업체는 유휴 용량을 "스팟(spot)" 또는 "선점형(preemptible)" VM으로 할인된 요금에 제공합니다. 이러한 VM은 가용성이 보장되지 않으며(얻으려고 해도 실패할 수 있음) 짧은 통지로 선점(즉, 종료)될 수 있지만 저렴합니다. 물론 워크로드는 이러한 형태의 선점을 견딜 수 있어야 합니다(예: 많은 배치 작업).

가상의 그러나 현실적인 예를 사용하여 클라우드 VM을 온프레미스 서버와 비교해 보겠습니다. 2024년 8월 현재, 비슷하게 구성된 Dell PowerEdge R7525의 정가는 $29,000(대규모 고객에게는 상당히 할인될 것임)이고 동등한 SuperMicro AS-1125HS-TNR 서버는 $11,000입니다. 서버 수명 6년 동안의 이자, 데이터 센터 및 전력 비용을 추가하면 총 비용은 각각 $49,000와 $26,900가 됩니다.

따라서 언뜻 보기에 온프레미스 비용이 훨씬 저렴해 보입니다. 6년 비용이 GCP에서의 3년 비용과 거의 같거나(Dell), 약 절반(SuperMicro)입니다. 하지만 현실은 좀 더 복잡합니다. 퍼블릭 클라우드는 서버 호스팅과 같지 않기 때문입니다. 하드웨어 차이점부터 시작하여 더 자세히 살펴보겠습니다.

*   온프레미스 서버의 데이터 센터 비용은 1.5의 PUE(대규모 기업 데이터 센터에 탁월함)와 비현실적인 100% 데이터 센터 가동률을 가정했습니다. 보다 현실적인 PUE 1.8과 데이터 센터 가동률 50%를 사용하면 6년 동안 약 $10,500가 추가됩니다.
*   하드웨어에 장애가 발생하면 클라우드에서는 교체 VM을 즉시 사용할 수 있습니다. 온프레미스에서는 4시간 응답(해결 아님!) SLA가 있는 서비스 계약 비용이 연간 장비 비용의 5-10%입니다. 보수적으로 잡아서 6년 동안 $5,000로 할인하겠습니다.
*   온프레미스 서버의 로컬 SSD는 중복성(redundancy)이 없는 원시(raw) 장치입니다. GCP의 동등한 "영구 디스크(persistent disk)"는 고가용성과 내구성을 갖춘 네트워크 연결 RAID SSD에 더 가깝습니다. 대부분의 기업 애플리케이션은 더 높은 신뢰성을 필요로 하며, 이는 최소한 온프레미스 스토리지 비용을 두 배로 늘립니다(미러링을 위한 두 번째 SSD 가정). 실제 등가물은 SSD NAS(백업용 스냅샷도 지원)가 될 것이며 이는 훨씬 더 비쌉니다. 그러나 보수적으로 잡아서 이것이 필요하지 않다고 가정하고 두 번째 SSD와 기본 백업 소프트웨어에 대해 $4,000의 수명 비용만 추가하겠습니다. 또한 위의 하드웨어 유지 보수 계약으로 장애가 무료로 커버되며 데이터 손실로 인한 후속 비용이 없다고 가정하겠습니다. (대담한 가정입니다!)
*   온프레미스 서버는 일단 구매하면 사용 여부와 관계없이 비용이 발생합니다. 따라서 회사가 간헐적인 컴퓨팅 수요를 가지고 있다면 온프레미스 설정은 좋은 대안을 거의 제공하지 못하는 반면, 퍼블릭 클라우드에서는 3년 약정을 온디맨드 또는 스팟 사용과 혼합하여 워크로드 수요에 맞출 수 있습니다. 마찬가지로 클라우드 VM은 추가 비용 없이 수명 동안 다른 용도로 변경하기 쉽습니다. 예를 들어, 다른 영역(zone)으로 이동하여 다른 서버의 장애 조치(failover) 서버가 될 수 있는데, 이는 온프레미스에서 수행하려면 수백 달러의 시간과 자재가 소요되는 작업입니다. 보수적으로 온프레미스 설정이 매우 안정적이어서 온프레미스 저활용(underutilization) 비용이 전체 비용의 10%에 불과하다고 가정하겠습니다. (기저 부하 용량을 호스팅하고 클라우드로 "버스트(bursting)"하는 개념이 나온 지 꽤 되었지만, 실제로는 인프라 스택이 너무 달라서 수행하기가 너무 복잡합니다.)
*   또한 클라우드 서버는 6년 온프레미스 소유권과 달리 3년 약정만 필요하므로 구식이 될 가능성이 훨씬 적습니다. 온프레미스 서버가 지금부터 4-6년 후의 미래 워크로드에 딱 맞을 가능성은 얼마나 될까요? 보수적으로 두 번째 3년 단계 동안의 진부화 위험이 33%에 불과하다고 가정하겠습니다. 즉, 평균 온프레미스 서버가 처음 5년 동안은 처음과 마찬가지로 이상적이며 6년 차에만 사용되지 않는다고 가정하는 것입니다.

따라서 하드웨어 차이와 현실적인 데이터 센터 가동률을 고려한 후, 3년 온프레미스 비용은 보수적으로 각각 $49,000와 $31,000로 증가하여 GCP의 3년 비용인 $50,500와 비교됩니다.

이제 소프트웨어 차이점을 살펴보겠습니다. 온프레미스 서버에 해당하는 GCP VM은 단순히 하드웨어와 호스팅만 제공하는 것이 아니라 가상화, 네트워킹, 로드 밸런싱, 네트워크 연결 스토리지, 머신 이미지 배포, 중앙화된 콘솔 로그, 기본 보안 기능, 관리형 OS 이미지, 모니터링 등 풍부한 서비스 스택과 함께 제공되기 때문입니다. 이러한 각 서비스는 온프레미스 시나리오에서 추가 비용이 듭니다.

*   일반적인 가상화 라이선스 비용은 코어당 연간 약 $50이며, 예제 서버의 경우 연간 약 $3000입니다. 오픈 소스 소프트웨어를 사용하더라도 직원이 설치, 구성 및 업그레이드해야 하므로 연간 수천 달러의 비용이 듭니다. 가상화 비용으로 연간 $2500를 보수적으로 가정하겠습니다.
*   클라우드 솔루션에는 네트워크 및 스토리지 관리, 중앙 집중식 로깅, 로드 밸런싱 및 방화벽, 기본 보안 및 백업 기능도 포함되며 모두 무료이거나 매우 저렴한 비용으로 제공됩니다. 해당 공간에는 비용이 매우 다양한 많은 온프레미스 제품이 있습니다. 기업이 서버 하드웨어보다 이러한 기능에 더 많은 비용을 지출하는 것은 드문 일이 아닙니다. 이러한 비용으로 연간 또 다른 $2000를 가정하겠습니다(매우 보수적인 추정).

이 첫 번째 소프트웨어 및 서비스 계층만 고려하더라도, 온프레미스 사례에 유리한 몇 가지 낙관적인 가정에도 불구하고 일용품 온프레미스 서버는 GCP VM보다 약간 저렴할 뿐입니다(그림 8.9). 특히 대부분의 기업은 저활용 및 진부화로 인해 훨씬 더 높은 비용을 보게 될 것이며, 자본 비용은 우리가 가정한 8%가 아니라 11-12%에 가깝습니다. 따라서 대부분의 기업은 이미 이 시점에서 클라우드 절감 효과를 볼 수 있습니다. 6년 후의 서버 요구 사항을 예측하는 것은 매우 어렵습니다. (주요 클라우드 중 어느 곳도 3년보다 긴 약정을 제공하지 않는다는 것은 그에 대한 수요가 거의 없음을 시사합니다.) 또한 가장 큰 기업을 제외한 모든 기업에서 인건비(시스템 및 데이터베이스 관리)는 여기에서 가정한 것보다 훨씬 높을 것입니다.

**그림 8.9: 온프레미스 대 클라우드 비교**
| 항목 | 온프레미스 (On-prem) | 클라우드 (Cloud) |
| :--- | :---: | :---: |
| 3년 하드웨어 TCO (현실적 가동률/PUE) | $23,000 | $50,500 |
| 하드웨어 수리 및 유지보수 | $2,500 | — |
| 온디맨드 VM 부재로 인한 온프레미스 저활용 | $6,000 | — |
| 처음 3년 이후의 온프레미스 진부화 비용 | $2,000 | — |
| 가상화 소프트웨어 | $7,500 | — |
| 기타 모든 시스템 소프트웨어 | $6,000 | — |
| **합계** | **$47,000** | **$50,500** |

그러나 비교가 끝난 것은 아닙니다. 퍼블릭 클라우드는 클라우드 제공업체 및 타사의 대규모 서비스 카탈로그(관리형 데이터베이스, 데이터 분석, 글로벌 WAN, 방화벽, 관리형 백업 등)에 대한 원활한 액세스를 제공합니다. 온프레미스 환경에서는 이들 각각을 구매, 설치 및 유지 관리하기 위해 상당한 추가 인력이 필요합니다. 이러한 노력 외에도 온프레미스 구현에는 기회비용이 발생합니다. 추가 컴퓨팅, 스토리지 또는 네트워킹이 필요한 경우 조달 및 설치에 몇 주 또는 몇 달이 걸릴 수 있으며, 그동안 업그레이드를 주도하는 비즈니스 요구는 충족되지 않습니다. 또한 퍼블릭 클라우드는 거의 모든 업종에 대한 SaaS 애플리케이션을 제공합니다. 기업의 경우 이러한 추가 기능과 "서비스로서(as a service)"의 관리 및 통합이 클라우드 접근 방식을 훨씬 저렴하게 만드는 결정적인 추가 요인입니다. 이러한 이점을 앞서 언급한 VM당 달러로 정량화하기 어렵기 때문에 마지막에 언급했지만, 많은 기업에게는 이것이 가장 우선입니다. 사실 대부분의 기업은 서버나 VM을 구매하고 싶어 하지 않으며, Cloud Run과 같은 관리형 PaaS 서비스를 선호하여 개발 및 유지 관리 비용을 더욱 절감할 수 있습니다.

이 비용 분석의 핵심 통찰력은 퍼블릭 클라우드 컴퓨팅이 서버 호스팅과 동일하지 않다는 것입니다. 유일한 요구 사항이 고정된 함대(fleet)의 관리형 하드웨어라면 코로케이션(colocation) 제공업체가 주로 하드웨어 솔루션이기 때문에 더 저렴한 솔루션입니다. 그러나 서버 호스팅만 필요한 회사는 거의 없습니다. 중요한 기능이 필요하고 데이터 센터 및 서버 가동률 50%조차 달성하기 어려운 기업 용도의 경우 퍼블릭 클라우드의 경제성을 능가하기 어렵습니다. 그리고 이 책이 데이터 센터와 하드웨어 계층에 초점을 맞추고 있지만, 퍼블릭 클라우드는 주로 서버 호스팅이 아니라 소프트웨어 스택과 생태계를 통해 가치를 제공하고 있습니다.

---

## 참고 문헌

1. L. Barroso, M. Marty, D. Patterson, and P. Ranganathan, “Attack of the killer microseconds,” *Commun. ACM*, vol. 60, no. 4, pp. 48–54, Mar. 2017.
2. J. Dean, “Designs, lessons and advice from building large distributed systems,” *Keynote from LADIS*, vol. 1, 2009.
3. S. Kanev, J. P. Darago, K. Hazelwood, et al., “Profiling a warehouse-scale computer,” in *Proceedings of the 42nd Annual International Symposium on Computer Architecture*, ser. ISCA ’15, New York, NY, USA: Association for Computing Machinery, Jun. 13, 2015, pp. 158–169.
4. S. Karandikar, A. N. Udipi, J. Choi, et al., “CDPU: Co-designing compression and decompression processing units for hyperscale systems,” in *Proceedings of the 50th Annual International Symposium on Computer Architecture*, ser. ISCA ’23, New York, NY, USA: Association for Computing Machinery, Jun. 17, 2023, pp. 1–17.
5. A. Abel, Y. Li, R. O’Grady, C. Kennelly, and D. Gove, “A profiling-based benchmark suite for warehouse-scale computers,” in *2024 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)*, 2024, pp. 325–327.
6. V. Lee, *Workload traces for google warehouse-scale computers*, Google Cloud Blog: https://cloud.google.com/blog/topics/systems/workload-traces-for-google-warehouse-scale-computers.
7. J. Lee, C. Kim, K. Lin, et al., “WSMeter: A performance evaluation methodology for Google’s production warehouse-scale computers,” 2, vol. 53, New York, NY, USA: Association for Computing Machinery, Mar. 19, 2018, pp. 549–563.
8. Y. Gan, M. Liang, S. Dev, D. Lo, and C. Delimitrou, “Practical and scalable ML-driven cloud performance debugging with Sage,” *IEEE Micro*, vol. 42, no. 4, pp. 27–36, 2022.
9. Turner & Townsend, *Data Centre Cost Index 2023*, https://reports.turnerandtownsend.com/dcci-2023/data-centre-cost-trends, 2023.
10. Bitfury, *BlockBox AC - Crypto Infrastructure*, https://bitfury.com/crypto-infrastructure/blockbox.
11. E. Bauer, *The total cost of ownership of open source software: A report for the UK Cabinet Office supported by OpenForum Europe*, https://evanbauer.com/essays/LinuxTCO.pdf, 2005.