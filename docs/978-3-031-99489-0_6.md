다음은 제공된 PDF 파일("WSC Hardware: Computing Building Blocks")의 내용을 번역하여 정리한 마크다운 문서입니다.

---

# 6장. WSC 하드웨어: 컴퓨팅 구성 요소 (Computing Building Blocks)

## 6.1 WSC 구성 요소 및 설계 고려 사항

### 6.1.1 전체 아키텍처 (Overall architecture)
앞서 논의한 데이터 센터 인프라를 바탕으로, 이제 웨어하우스 스케일 컴퓨터(WSC)의 아키텍처를 정의하는 개별 하드웨어 구성 요소에 초점을 맞춥니다. 논리 소자가 마이크로프로세서를 구성하고 칩셋이 서버를 만드는 것처럼, WSC 시스템은 개별 칩부터 마더보드, 분산 시스템에 이르기까지 컴퓨팅, 스토리지, 네트워킹 전반에 걸친 다양한 구성 요소로 이루어집니다. 이 장에서는 이러한 구성 요소의 선택과 그에 따른 하드웨어-소프트웨어 공동 설계(co-design) 결정에 대한 통찰력을 제공하는 것을 목표로 합니다.

### 6.1.2 스케일아웃 시스템 구축 (Building scale-out systems)
대규모 분산 하드웨어 시스템을 구축할 때 우리는 스케일업(대규모 공유 메모리 시스템)과 스케일아웃(대규모 클러스터)이라는 두 가지 선택지에 직면합니다. WSC는 역사적으로 여러 가지 이유로 저사양 또는 중급 서버들의 클러스터를 구축하는 스케일아웃 방식을 선택해 왔습니다[1]. 주된 동기는 고사양 공유 메모리 시스템에 비해 이러한 서버들이 가지는 비용 효율성 때문이었습니다. 코어 수가 증가하는 지속적인 추세는 대부분의 VM이나 작업 인스턴스가 2소켓(또는 1소켓) 서버에 편안하게 들어갈 수 있음을 의미합니다. 사실, 오늘날 일반적인 물리적 서버는 수십 개에서 수백 개의 VM을 호스팅합니다.

이 책의 이전 판에서는 고사양 서버와 저사양 서버 간의 TPC-C 벤치마크[2] 비교를 깊이 있게 다루었습니다. 예를 들어, 초판에서는 HP Integrity Superdome-Itanium2[2]와 HP ProLiant ML350 G5[2]를 비교하며 저사양 서버의 비용 효율성이 고사양 공유 메모리 시스템보다 4배 더 우수함을 보여주었습니다. 하지만 수년이 지나고 개정이 거듭되면서 고사양 서버 시장은 점점 틈새시장이 되었고, 하이퍼스케일 서버가 훨씬 더 일반적이고 많은 물량을 차지하게 되었습니다. 따라서 특정 개별 시스템 비교에 초점을 맞추기보다는, 아래에서는 스케일업 대 스케일아웃의 트레이드오프를 추론하기 위한 간단한 수학적 모델을 제시하고, "강력한 코어(brawny) 대 약한 코어(wimpy)" 논쟁[3]을 포함한 CPU 구성 요소에 대한 선택과 그것이 수년에 걸쳐 어떻게 진화했는지 논의합니다. 또한, 새롭게 부상하는 가속기 시스템에서 스케일업 대 스케일아웃 고려 사항이 어떻게 다른지도 논의합니다.

#### 6.1.2.1 스케일업 대 스케일아웃을 추론하기 위한 모델
단순한 프로세서 중심의 비용 효율성 분석은 용량 $N \times M$인 단일 서버가 용량 $M$인 $N$개의 서버보다 훨씬 비싸다는 것을 보여줍니다. 그러나 이는 대규모 공유 메모리 멀티프로세서(SMP)가 상용 패브릭(commodity fabrics)으로 연결된 소형 서버 클러스터보다 월등히 우수한 통신 성능을 제공한다는 점을 고려하지 못한 것입니다. 대형 SMP 내의 노드들은 약 100ns의 지연 시간으로 통신할 수 있는 반면, 서버 클러스터에 주로 배포되는 LAN 기반 네트워크는 약 100µs의 지연 시간을 경험할 수 있습니다. 단일 대형 SMP 내에 들어가는 병렬 애플리케이션(예: SAP HANA)의 경우, 효율적인 통신은 극적인 성능 향상으로 이어질 수 있습니다. 하지만 앞서 WSC 애플리케이션에 대한 논의(3장)에서 다루었듯이, 대부분의 대규모 워크로드는 단일 SMP 내에 들어갈 가능성이 낮습니다. 따라서 대형 SMP 클러스터와 (대형 SMP와 동일한 서버급 CPU 코어를 사용하더라도) 더 적은 수의 CPU 소켓이나 코어를 가진 저사양 서버 클러스터 간의 상대적인 성능을 이해하는 것이 중요합니다. 다음의 간단한 모델이 이러한 비교에 도움이 될 수 있습니다.

주어진 병렬 작업 실행 시간을 '고정된 로컬 연산 시간'과 '글로벌 데이터 구조 접근에 따른 지연 시간 페널티'의 합으로 모델링할 수 있다고 가정해 봅시다. 연산이 단일 대형 공유 메모리 시스템에 들어간다면, 글로벌 데이터 접근은 대략 DRAM 속도(~100ns)로 수행될 것입니다. 만약 연산이 여러 노드에 걸쳐야 한다면, 일부 글로벌 접근은 원격 분산 시스템 접근을 위해 훨씬 느려져 일반적인 LAN 속도(~100µs) 수준이 될 것입니다. 고정된 로컬 연산 시간이 1ms(높은 처리량의 인터넷 서비스에 합리적인 값) 수준이라면, 프로그램 실행 시간을 결정하는 식은 다음과 같습니다.

$$ time = 1ms + f \cdot (100ns \cdot LocalAccessesFraction + 100\mu s \cdot RemoteAccessesFraction) $$

여기서 변수 $f$는 작업 단위(1ms)당 글로벌 접근 횟수입니다.
또한 글로벌 저장소에 대한 접근이 모든 노드에 균일하게 분포되어 있다고 가정해 봅시다. 그러면 로컬 노드에 매핑되는 글로벌 접근의 비율은 시스템 내 노드 수($N$)에 반비례합니다. 따라서 식은 다음과 같이 됩니다.

$$ time = 1ms + f \cdot (\frac{1}{N} 100ns + \frac{N-1}{N} 100\mu s) $$

그림 6.1에서는 연산에 참여하는 노드 수(X축)가 증가함에 따른 병렬 워크로드의 실행 시간(Y축)을 보여줍니다. $f$ 값에 따라 세 가지 곡선이 표시되는데, 이는 통신이 적은 워크로드($f=1$), 중간 정도인 워크로드($f=10$), 통신이 많은 워크로드($f=100$)를 나타냅니다. 우리 모델에서 노드 수가 많을수록 원격 글로벌 접근 비율이 높아집니다. 두 축은 모두 로그 스케일입니다.

예상대로 통신이 적은 경우, 다중 노드 클러스터를 사용할 때 성능 저하는 미미합니다. 중간 및 높은 통신 패턴의 경우 페널티가 상당히 클 수 있는데, 특히 단일 노드에서 두 개의 노드로 이동할 때 가장 극적이며 클러스터 크기가 증가함에 따라 추가적인 페널티는 급격히 감소합니다. 이 모델을 사용하면 단일 128 프로세서 SMP가 32개의 4 프로세서 SMP 클러스터보다 10배 이상의 성능 이점을 가질 수 있습니다.

정의상 WSC 시스템은 수천 개의 CPU로 구성됩니다. 따라서 우리는 이 모델을 사용하여 대형 SMP 서버로 구축된 클러스터와 저사양 서버로 구축된 클러스터의 성능을 비교해 보고자 합니다. 여기서는 두 시스템의 코어당 성능이 동일하고 서버들이 이더넷급 패브릭으로 연결되어 있다고 가정합니다. 비록 우리 모델이 지나치게 단순하지만(예: 경합 효과를 고려하지 않음), 우리가 관심 있는 효과를 명확하게 포착합니다.

그림 6.2에서는 512개에서 4,192개 코어 사이의 클러스터에 모델을 적용했습니다. 우리는 대형 SMP 서버(단일 공유 메모리 도메인에 128 코어)를 사용하는 구현과 저사양 서버(4 코어 SMP)를 사용하는 구현의 성능 이점을 보여줍니다. 클러스터 크기가 커짐에 따라 고사양 서버 기반 클러스터의 성능 우위는 빠르게 약화됩니다. 애플리케이션이 2,000개 이상의 코어를 필요로 한다면, 512개의 저사양 서버 클러스터는 통신이 많은 패턴에서도 16개의 고사양 서버로 구축된 클러스터 성능의 약 5% 이내에서 수행됩니다. 성능 격차가 이렇게 적다면, 고사양 서버의 가격 프리미엄(4~20배 더 높음)은 이를 매력적이지 않은 옵션으로 만듭니다.

이 분석의 요점은 정성적인 것입니다. 단일 고사양 서버에 비해 너무 큰 애플리케이션을 위한 시스템을 설계할 때 기본 플랫폼 선택에 대해 다르게 추론해야 함을 보여줍니다. 큰 요점은 웨어하우스 규모에서 시스템에 이득이 되는 성능 효과가 가장 중요하다는 것입니다. 단일 노드에 국한된 연산에 가장 큰 영향을 미치는 성능 향상(예: 빠른 SMP 스타일 통신)은 여전히 중요합니다. 그러나 그것이 과도한 추가 비용을 수반한다면, 소규모 컴퓨터에서와 달리 WSC에서는 비용 효율성이 경쟁력이 없을 수 있습니다.

#### 6.1.2.2 스케일아웃의 입도: 강력한 코어(brawny) 대 약한 코어(wimpy)
위의 논리를 더 확장하여 오늘날의 서버급 CPU 코어보다 더 작은(또는 "더 약한(wimpy)"[4]) CPU 코어를 사용할 수도 있습니다. 실제로 여러 선행 연구가 이 방향을 탐구했습니다. Piranha 칩 멀티프로세서[5]는 엔터프라이즈급 서버 시스템에서 저사양 코어 사용을 주장한 초기 시스템 중 하나였습니다. Lim 등[6]은 서버 플랫폼에서 저사양 저전력 프로세서 코어의 필요성을 주장했습니다. 이 작업은 HP의 Moonshot 서버[7]로 이어졌는데, 이는 하나의 4.3U 인클로저에 45개의 ARM 기반 또는 모바일 x86 기반 블레이드 서버를 탑재했습니다. Hamilton[8]은 PC급 부품에 대해 유사한 주장을 펼쳤으며, 이는 Amazon의 Graviton 기반 ARM 서버 라인[9]의 토대가 되었습니다. FAWN (Fast Array of Wimpy Nodes) 프로젝트[4]는 플래시 메모리를 사용하는 에너지 효율적인 키-값 저장 시스템을 구축하기 위해 약한 코어의 효용성을 탐구했으며, 스토리지 서버의 I/O 및 메모리 집약적 연산에 초점을 맞췄습니다.

더 작고 느린 CPU를 사용하는 이점은 고사양 SMP 대신 중급 상용 서버를 사용하는 논거와 매우 유사합니다.
*   중급 서버의 멀티코어 CPU는 일반적으로 저사양 프로세서에 비해 가격 대비 성능 프리미엄이 붙기 때문에, 다수의 더 작은 CPU를 사용하면 동일한 처리량을 2~5배 더 저렴하게 구입할 수 있습니다.
*   많은 애플리케이션이 메모리 또는 I/O 중심이어서 더 빠른 CPU가 대규모 애플리케이션에서 잘 확장되지 않으므로, 단순한 CPU의 가격 이점이 더욱 강화됩니다.
*   느린 CPU는 전력 효율이 더 높은 경향이 있습니다. 일반적으로 CPU 주파수가 $k$배 감소하면 CPU 전력은 $O(k^2)$만큼 감소합니다.

동시에 이 경로를 너무 무분별하게 따르는 것에는 위험이 있습니다[3].
첫째, 많은 인터넷 서비스가 겉보기에 무한한 요청 및 데이터 수준 병렬성의 이점을 누리지만, 암달의 법칙(Amdahl's law)에서 자유롭지는 않습니다. 제공되는 병렬 스레드 수가 증가함에 따라 직렬화 및 통신 오버헤드를 줄이는 것이 점점 어려워져 속도 향상이나 규모 확장이 제한됩니다[10], [6]. 극한의 경우, 매우 느린 단일 스레드 하드웨어에서 수행되는 사용자 요청의 본질적으로 직렬화된 작업 양이 전체 실행 시간을 지배하게 됩니다.

또한 병렬화된 요청을 처리하는 스레드가 많을수록 이러한 병렬 작업들의 응답 시간 변동성이 커져, 앞서 3장에서 논의한 꼬리 지연(tail latency) 문제가 악화됩니다. 결과적으로 하드웨어 비용은 줄어들 수 있지만, 더 많은 애플리케이션을 명시적으로 병렬화하거나 최적화해야 하므로 소프트웨어 개발 비용이 증가할 수 있습니다. 예를 들어, 현재 웹 서비스가 사용자 요청당 1초의 지연 시간으로 실행되고 그중 절반이 CPU 시간이라고 가정해 봅시다. 단일 스레드 성능이 3배 느린 저사양 서버 클러스터로 전환하면 서비스 응답 시간은 2초로 두 배가 되며, 애플리케이션 개발자는 1초 지연 시간 수준으로 되돌리기 위해 코드를 최적화하는 데 상당한 노력을 기울여야 할 수 있습니다.

더 작은 서버는 활용률 저하로 이어질 수도 있습니다. 서버 풀에 애플리케이션 집합을 할당하는 작업을 '빈 패킹(bin packing)' 문제로 생각해 봅시다. 각 서버는 '빈(통)'이고, 가능한 많은 애플리케이션이 각 통 안에 채워집니다. 통이 작으면 많은 애플리케이션이 서버를 완전히 채우지 못하면서도 두 번째 애플리케이션이 같은 서버에 공존하기에는 CPU나 RAM을 너무 많이 사용하여, 빈 패킹 작업이 더 어려워집니다.

마지막으로, '당황스러울 정도로 병렬적인(embarrassingly parallel)' 알고리즘조차도 연산과 데이터가 더 작은 조각으로 분할될 때 본질적으로 덜 효율적일 수 있습니다. 예를 들어 병렬 연산의 중단 기준이 글로벌 정보에 기반할 때 그렇습니다. 비용이 많이 드는 글로벌 통신과 잠금 경합을 피하기 위해 로컬 작업은 로컬 진행 상황에만 기반한 휴리스틱을 사용할 수 있으며, 이러한 휴리스틱은 자연스럽게 더 보수적입니다. 결과적으로 로컬 하위 작업은 글로벌 진행 상황에 대한 더 좋은 힌트가 있었을 때보다 더 오래 실행될 수 있습니다. 당연히 이러한 연산이 더 작은 조각으로 분할될 때 이 오버헤드는 증가하는 경향이 있습니다.

또한 클라우드 애플리케이션의 관점에서 대부분의 워크로드는 단일 VM 성능을 강조하며 VM당 단일 스레드 성능에 중점을 둔 고성능 코어를 선호합니다. 예를 들어, 데이터베이스는 애플리케이션 성능을 제한하기 때문에 가장 빠른 서버에서 실행되기를 원합니다. 게다가 더 큰 시스템은 더 작은 VM 형태로 배포 및 판매하기 쉽지만, 그 반대는 성립하지 않습니다.

반면, 데이터 분석과 같은 처리량 중심의 워크로드는 연산 비용을 최소화하는 시스템을 선호하므로, 다소 느린 코어지만 더 높은 처리량을 제공하고 시스템 비용을 줄이기 위해 코어당 메모리가 적은 서버로 이어집니다.

최근 CPU 공급업체들은 이 두 가지 시나리오 각각에 최적화된 CPU 변형을 제공하기 시작했습니다. 예를 들어, ARM은 성능을 위한 "V" 시리즈, 스케일아웃을 위한 "N" 시리즈, 효율성을 위한 "E" 시리즈를 제공합니다. 인텔은 성능과 효율성에 초점을 맞춘 두 가지 제온 프로세서 라인을 제공합니다. 마찬가지로 AMD도 EPYC 시스템에서 다양한 성능 및 효율성 설계 지점에 최적화된 여러 프로세서 라인을 제공합니다.

클라우드 제공업체가 제공하는 ARM 프로세서(예: AWS의 Graviton 및 GCP의 Axion)의 성공은 "강력한(brawny)" 코어에 대한 수요를 보여줍니다. 현재의 ARM VM은 두 가지 이유로 더 나은 가격/성능을 제공합니다. 첫째, 해당 CPU는 하이퍼스레딩을 사용하지 않으므로 1 vCPU = 1 물리적 코어입니다. 2방향 하이퍼스레딩을 사용하는 x86 VM에서는 2 vCPU = 1 물리적 코어입니다. 따라서 vCPU당 성능을 비교할 때 ARM VM이 상당히 빠를 수 있습니다. 둘째, 코어당 물리적 DRAM이 절반이기 때문에 더 저렴합니다. 클라우드 VM은 (물리적 코어가 아닌) vCPU당 표준 양의 DRAM으로 프로비저닝되므로, ARM 서버는 메모리를 절반만 포함하여 서버당 비용을 20-25% 절감합니다.

더 미세한 입도의 장단점에 대한 위의 논의는 설계 결정을 위한 광범위한 지침을 제공합니다. 단일 코어 속도를 최대화하는 설계와 단일 코어 속도를 줄여 소켓당 처리량을 개선하는 설계 모두 서버 플릿(fleet)에서 자리를 잡고 있습니다. 그러나 속도 차이가 클수록 위에서 논의한 오버헤드로 인해 소켓당 가격 대비 처리량 이점이 실제 배포에서 발휘될 가능성은 낮아집니다.

#### 6.1.2.3 가속기를 위한 스케일업 대 스케일아웃
지금까지는 전통적인 컴퓨팅 서버에 초점을 맞추었지만, 대규모 신흥 WSC 가속기 시스템을 구축할 때도 유사한 트레이드오프에 직면합니다. 이 장의 뒷부분에서 머신러닝 워크로드를 위한 GPU와 TPU, 비디오 워크로드를 위한 VCU 등 다양한 가속기의 설계와 이를 WSC 규모로 확장하는 방법에 대해 논의할 것입니다. 가속기 시스템은 맞춤형으로 설계되고 특정 워크로드를 위한 수직 통합 솔루션을 목표로 하며 종종 기존 서버보다 생산량이 적기 때문에 설계 트레이드오프가 다를 수 있습니다.

이 장의 시작 부분에서 논의한 모델로 돌아가면, 맞춤형 설계는 네트워크를 최적화할 기회를 열어주어 원격 노드 접근 지연 변수가 이전 모델에서 가정한 100µs와 다를 수 있게 합니다. 예를 들어 TPU 시스템은 혁신적인 ICI 기반 네트워크(ICI는 Google의 Inter-Chip Interconnect 프로토콜을 의미)를 갖추고 있어 공유 메모리 추상화를 가진 더 큰 TPU 슈퍼컴퓨터(또는 "하이퍼컴퓨터"[11])를 가능하게 합니다. ML 가속기의 경우, TPU 설계자들은 9,216개 칩의 포드(pod) 크기로 스케일업을 상당히 깊이 있게 추진하기로 결정했습니다. NVIDIA는 최대 256개의 GPU를 허용하는 NVLink 인터커넥트를 제공합니다. 새로운 산업 컨소시엄은 유사한 고속 네트워킹을 산업 표준으로 만들기 위해 "Ultra Accelerator Link (UAL)"[12] 작업을 진행 중입니다. 대규모 애플리케이션의 경우, 이렇게 고도로 연결된 포드들을 이더넷 기반 스케일아웃 네트워크 패브릭을 통해 연결합니다.

반면 VCU 시스템(6.3.3절)은 비디오 코딩 작업이 서로 통신하지 않기 때문에 전통적인 WSC 스케일아웃 방식을 완전히 수용합니다. 따라서 대량 배포는 이더넷 기반 네트워킹과 스케일아웃 작업 스케줄링과의 긴밀한 통합을 사용합니다.

전반적으로 이 분야는 아직 상대적으로 초기 단계이고 진화하고 있지만, 앞서 논의한 원칙들이 이러한 시스템 설계에도 동일하게 적용될 것으로 예상합니다. 스케일업은 소규모 워크로드에 대해 상당한 성능 및 관련 비용 대비 성능 이점을 제공할 것이지만, 비용 효율성과 산업 하드웨어 및 분산 시스템 관리에 대한 대규모 소프트웨어 투자를 활용할 수 있는 능력 덕분에 훨씬 더 큰 워크로드에 대해서는 스케일아웃이 항상 중요할 것입니다.

## 6.2 컴퓨팅: 서버와 랙 (Computing: servers and racks)

### 6.2.1 서버 하드웨어
개별 랙에 호스팅되는 서버는 WSC의 기본 구성 요소입니다. 이들은 네트워크 계층으로 상호 연결되며 공유 전력 및 냉각 인프라의 지원을 받습니다.

각 서버 세대는 광범위한 WSC 워크로드에 대한 성능과 비용, 그리고 리소스 요구 사항을 지원하는 유연성을 최적화해야 합니다. 서버는 일반적으로 트레이(tray) 형태로 제작되며, 마더보드, 칩셋, 추가 플러그인 구성 요소를 수용합니다. 마더보드는 CPU, 메모리 모듈(DIMM), 관리 모듈, 로컬 스토리지(Flash, SSD, HDD 등), 네트워크 인터페이스 카드(NIC), 그리고 가속기 카드(GPU, TPU, VCU 등)를 설치할 수 있는 소켓과 플러그인 슬롯을 제공합니다. 플랫폼 컨트롤러 허브(PCH)는 USB, SATA, 이더넷과 같은 느린 비필수 I/O 작업을 관리하고, CPU는 PCIe 및 DRAM과 같은 빠른 연결을 처리합니다. 그림 6.3은 일반적인 서버의 구성 요소를 보여줍니다.

워크로드 성능, 총 소유 비용(TCO), 유연성에 따라 몇 가지 주요 설계 고려 사항이 서버의 폼 팩터와 기능을 결정합니다.

*   **CPU**: CPU 전력(종종 TDP 즉 열 설계 전력으로 정량화됨), CPU 패키징, CPU 소켓 수 및 NUMA(비균일 메모리 접근) 토폴로지, CPU 선택(예: 코어 수, 코어 및 언코어 주파수, 캐시 크기, 소켓 간 일관성 링크 수).
*   **메모리**: 메모리 채널 수, 채널당 DIMM 수, 지원되는 DIMM 유형(RDIMM, LRDIMM 등), 지원되는 DDR 표준(DDR4, DDR5, LPDDR 등).
*   **플러그인 I/O 카드**: SSD, NIC, 가속기에 필요한 PCIe 카드 수, 폼 팩터, PCIe 대역폭 및 전력, 리소스 공유 가능성(예: NIC quadfurcation), CXL(Compute Express Link)과 같은 새로운 I/O 장치 등.
*   **트레이 수준 전력 및 냉각**: 전압 조정기(VR), 배터리 백업, 냉각 옵션(액체 대 공랭) 및 패키징, 팬, 전력 관리 지원 등.
*   **장치 관리 및 보안 옵션**: 보드 관리 컨트롤러(BMC), 루트 오브 트러스트(Root-of-Trust) 보안 등.
*   **기계적 설계**: 서버가 어떻게 설계되고 조립되는지는 신중한 고려가 필요합니다. 폼 팩터(너비, 높이, 깊이)는 전력 밀도에 영향을 줄 수 있으며, 전면 또는 후면 부품 접근 방식은 서비스 용이성에 영향을 줄 수 있습니다.

그림 6.4는 다양한 서버 트레이의 사진을 보여줍니다. 가장 왼쪽 서버는 두 개의 Intel Sapphire Rapids CPU 소켓을 지원합니다. 각 CPU는 최대 350W TDP를 지원할 수 있습니다. 각 CPU에는 4개의 메모리 컨트롤러가 있으며, 각각 채널당 2개의 DIMM 슬롯(2DPC)으로 2개의 DDR5 메모리 채널을 제어하여 CPU당 총 16개의 DIMM, 트레이당 32개의 DIMM을 지원합니다. 통합 전압 조정기를 통해 플랫폼은 코어별 DVFS를 허용합니다. 이 시스템은 8개의 PCIe Gen5 연결을 지원하여 SSD, 200 Gbps NIC 및 특수 목적 가속기 배열을 위한 PCIe 카드를 호스팅할 수 있습니다. OCP DC-SCM 사양에 기반한 데이터 센터 보안 제어 모듈(DC-SCM)이 마더보드에 연결되어 Root-of-Trust(RoT), 보드 관리 컨트롤러(BMC), BMC EEPROM, BIOS EEPROM을 마더보드에서 분리할 수 있게 합니다.

두 번째 서버는 유사하지만, 더 높은 코어 수와 TDP, CXL 인터커넥트 지원, 채널당 하나의 DIMM 슬롯(1DPC)을 가진 12개의 메모리 채널을 갖춘 AMD Genoa CPU를 포함합니다. 추가 메모리 채널은 더 높은 수의 코어를 지원하기 위해 증가된 메모리 대역폭을 제공합니다. 하지만 모든 워크로드에 충분한 메모리 대역폭을 제공하기는 어려우므로 일부는 여전히 메모리 대역폭에 의해 제한될 수 있습니다. 트랜지스터 크기가 개선될 때마다 코어를 추가하는 것은 쉬워지지만, I/O 연결을 위해 사용 가능한 칩 면적("쇼어라인")이 늘어나지 않고 메모리 채널당 대역폭도 트랜지스터 크기 및 성능과 비례하여 확장되지 않기 때문에 메모리 대역폭을 비례적으로 확장하는 것은 점점 더 어려워집니다.

세 번째 서버는 ARM Neoverse V2 코어를 사용하는 Google의 맞춤형 Axion CPU를 사용합니다. 각 CPU는 10개의 DDR5 메모리 컨트롤러를 가지고 있으며 최대 350W TDP를 지원합니다. 트레이에는 두 개의 컴퓨팅 노드가 있으며 각각 자체 소켓을 가집니다. 이 설계는 코어 수, 주파수, 캐시 크기 측면의 컴퓨팅 용량과 사용 가능한 메모리 시스템 대역폭의 균형을 맞춥니다. 또한 단일 NIC가 4개의 ARM 서버(2개 트레이)에 의해 공유되는 NIC quad-furcation을 지원하여 Perf/TCO를 개선합니다.

그림 6.5와 6.6은 2소켓 서버와 분기된(bifurcated) 2x1소켓 시스템 설계의 기능적 아키텍처를 보여줍니다.

### 6.2.2 하드웨어 랙 (Hardware racks)
랙은 수십 개의 서버를 함께 고정하는 물리적 구조물입니다. 랙은 물리적 지지 구조를 제공할 뿐만 아니라 전력 전달, 배터리 백업, 전력 변환(예: AC에서 48V DC로)을 포함한 공유 전력 인프라를 처리합니다. 랙의 너비와 깊이는 WSC마다 다릅니다. 일부는 고전적인 19인치 너비, 48인치 깊이 랙인 반면, 다른 것들은 더 넓거나 얕을 수 있습니다. 랙 설계는 서버 트레이, 네트워킹 또는 가속기를 대상으로 하는지, 그리고 주 캠퍼스에서 사용되는지 엣지 또는 코로케이션(colo) 캠퍼스에서 사용되는지에 따라 달라질 수 있습니다. 랙 설계 고려 사항은 "포드(pods)" 또는 "클리크(cliques)"[13]에서 사용될 때와 다양한 냉각 고려 사항(공랭 대 수랭)에 따라서도 변경될 수 있습니다. 가속기 랙 설계는 다음 섹션에서 더 자세히 논의할 것입니다.

랙의 너비와 깊이는 트레이 폼 팩터를 제약합니다. 네트워크 케이블을 랙 상단에 연결하는 것이 편리한 경우가 많습니다. 이러한 랙 레벨 스위치를 적절하게 Top of Rack(ToR) 스위치라고 부릅니다. 네트워킹에 대해서는 6.4절에서 곧 자세히 설명할 것입니다.

그림 6.7은 Google 데이터 센터의 예시 랙을 보여줍니다. 이는 다양한 너비와 높이를 가진 서버 트레이를 위한 구성 가능한 물리적 구조를 지원합니다. 예를 들어, 행당 4개의 좁은 서버 트레이를 호스팅할 수 있습니다. 또한 IT 전력 부하에 맞춰 이중화를 포함한 구성 가능한 전력 변환 및 배터리 백업 지원을 제공합니다. ToR 스위치와 랙 관리 장치는 랙 상단에 위치하며 데이터 센터 패브릭에 추가로 연결되어 많은 랙을 연결합니다.

그림 6.8은 TPU 가속기와 관련 네트워킹을 포함한 고밀도 랙을 보여줍니다. 일반 랙에 비해 이 랙들은 랙당 대역폭과 전력이 훨씬 높습니다.

Open Compute Project[14]는 많은 WSC 하드웨어 구성 요소에 대한 상세 사양을 제공합니다.

### 6.2.3 분산 시스템으로서의 개별 서버 (Individual servers as distributed systems)
과거의 서버 설계는 그림 6.9 (a)와 같은 "모놀리스(monoliths)"였습니다. 즉, 하나의 주 컴퓨팅 복합체가 있고 단일 운영 체제에 의해 관리되는 단일 노드 머신이었습니다. 그러나 그림 6.9 (b)와 같이 오늘날의 시스템은 기본 CPU 외에도 관리 컨트롤러, 스마트 NIC, 가속기 등 여러 컴퓨팅 요소를 포함하며, 각각 자체 운영 체제를 실행합니다. 이 접근 방식은 보안과 관리 용이성을 향상시키며, 클라우드 고객이 CPU에 가상화되지 않은 접근 권한을 갖는 "베어 메탈" 서버도 가능하게 합니다. 분리형(Disaggregated) 설계는 서버를 가상 구조로 재정의하여, 물리적으로 로컬 SSD나 GPU를 포함하는 대신 고속 버스(PCIe, CXL)나 이더넷을 통해 랙 레벨 풀의 장치에 연결하는 등 개별 하드웨어 구성 요소의 네트워크를 통해 리소스 조각이 다중화되는 동적으로 재할당 가능한 풀로 구성합니다. 그림 6.9는 서버가 여러 펌웨어와 시스템 스택을 실행하는 복잡한 분산 시스템이 되었음을 보여줍니다.

따라서 서버는 더 이상 금속판 안에 담긴 하드웨어 구성 요소의 집합이 아니라, '아레나(arena)'라고 불리는 상호 신뢰하지 않는 추상적 구성 요소들의 논리적 조합입니다. 머신은 하나 이상의 아레나로 구성됩니다. 이러한 서버는 보안과 신뢰 도메인(아레나)의 명확한 분리를 제공하며, 각 아레나는 자체 운영 체제를 실행합니다. 따라서 컴퓨팅 워크로드를 실행하는 OS는 시스템을 관리하는 OS에 접근하거나 수정할 수 없습니다.

이러한 하드웨어 이기종성을 처리하는 가장 좋은 방법은 관리 기능을 다른 곳으로 옮겨 시스템 스택의 대부분에서 이를 숨기는 것입니다(그림 6.10). 예를 들어, 위에서 논의한 오프로드 아레나는 가상화를 별도 카드로 이동시켜 서버별 코드를 줄이거나 제거합니다. 유사하게 서버 관리 및 모니터링(예: 팬 속도 제어)은 별도 카드(Baseboard Management Controller, 또는 BMC)로 이동했습니다. 이러한 각 아레나는 하드웨어 루트 오브 트러스트(RoT)로 보호되는 자체 펌웨어 스택을 포함합니다. 7장에서는 소프트웨어 정의 인프라가 소프트웨어 래퍼와 관리를 통해 하드웨어 이기종성을 어떻게 더욱 추상화하는지 논의할 것입니다.

그림 6.11은 대표적인 다중 노드 머신을 스케치하고 플릿(fleet) 제어 평면이 노드와 어떻게 상호 작용하는지 보여줍니다. 굵은 점선은 아레나(하드웨어 신뢰 경계)를 나타냅니다. 머신의 모델은 아레나 내의 어떤 펌웨어가 RoT를 통해 증명되어야 하는지 정확하게 정의합니다. 이 그림은 또한 텔레메트리, 증명(attestation), 스케줄링 등에 걸쳐 제어 평면이 관리하는 복잡한 상호 작용을 보여줍니다. 단순화를 위해 아크의 일부만 표시했습니다.

제어 평면 설계는 이전 접근 방식(예: 소프트웨어 정의 네트워킹)과 유사합니다. 우리는 각 아레나에 국한된 소규모 경계 집행 기능 세트와 플릿 전체를 관리하는 더 광범위한 제어 평면 기능을 명확히 구분하는 것을 목표로 합니다. 또한 중앙 집중식 제어 평면을 지원하기 위해 머신, 랙, 네트워크 모델에 크게 의존합니다. 예를 들어, 우리는 엔티티-관계 그래프를 통해 그림의 각 요소를 여러 추상화 수준에서 표현할 수 있는 모델링 언어를 사용합니다. 이 그래프는 도달 가능성(어떤 컨트롤러 서비스가 어떤 제어 노드에 연결할 수 있는지), 장애 도메인, 증명 도메인(어떤 RoT가 어떤 변경 가능한 펌웨어 저장 요소를 증명할 수 있는지)도 정의합니다.

그림의 하반부는 작업 스케줄러가 관리하는 증명(attestation) 사용 사례를 보여줍니다. 워크로드는 증명된 상태의 머신에서만 실행되어야 하므로, 우리는 전사적 작업 스케줄러를 중앙 증명 권한으로 사용하기로 했습니다. 스케줄러가 리소스 풀에 새 머신을 포함하라는 요청을 받으면, 머신에 서명된 증명 정책을 요청합니다. 정책에는 머신의 신뢰 루트 목록과 해당 RoT에서 예상되는 증명이 포함되어 있습니다. 그런 다음 스케줄러는 정책의 서명을 확인하고 정책의 취소 ID를 취소 목록과 대조합니다. 정책이 유효하지 않으면 스케줄러는 머신을 거부합니다.

우리는 관리 프로세스가 머신 모델에 있는 컴퓨팅 요소와 RoT를 기반으로 머신의 펌웨어를 업그레이드(또는 다운그레이드)할 때마다 새 정책을 생성(하고 이전 정책을 취소)합니다. 스케줄러가 가용성을 저해할 수 있는 다른 서비스에 의존하지 않도록 하기 위해, 인증 기관이 암호화하여 서명한 머신 정책을 머신 자체에 저장합니다. 정책의 서명이 유효하면 스케줄러는 개별 아레나 컨트롤러에 RPC를 보내 정책에 나열된 각 장치에 대한 RoT의 증명서(attestation statement)를 수집합니다. 모든 증명이 정책 의도와 일치하면 스케줄러는 머신을 클러스터에 추가하고 작업을 예약할 수 있게 합니다.

추가적인 세부 사항은 OpenCompute[15], [16]의 기술 논문 및 관련 기고문을 참조하십시오. 하드웨어 설계를 아레나 정의에 맞게 제한하고 머신별 지식을 코드에서 모델로 이동함으로써, 하드웨어 신뢰에 대한 추론을 단순화하고 WSC 규모에서 관리를 확장하여 서버 복잡성을 증가시키면서도 새로운 서버 구성 도입에 따른 소프트웨어 엔지니어링 비용(및 지연)을 크게 줄일 수 있었습니다.

## 6.3 가속기 및 맞춤형 실리콘 (Accelerators and custom silicon)

이 책의 이전 판에서는 특수 컴퓨팅 가속기의 도입에 대해 논의했습니다. 그 이후로 머신러닝, 비디오 처리, 네트워킹, 데이터 이동, 보안 등을 위한 수많은 새로운 가속기와 함께 채택이 기하급수적으로 확대되었습니다.

### 6.3.1 전문화와 무어의 법칙의 종말 (Specialization and the end of Moore’s Law)
역사적으로 우리가 첫 WSC를 설계했을 때, 특수 목적 컴퓨팅 배포를 피했습니다. 더 높은 컴퓨팅 효율성을 약속했지만, 그 이점은 혜택을 볼 수 있는 제한된 수의 워크로드에만 전문화되는 비용을 치러야 했으며, 이는 규모와 대량이라는 WSC의 기본 원칙에 반하는 것이었습니다. 또한 특수 하드웨어의 더 긴 리드 타임과 더 높은 개발 비용은 일반 CPU의 급격한 개선으로 인해 구식이 될 위험이 있었습니다. CPU 속도가 매년 두 배로 증가하고 ASIC 배포에 2년이 걸린다면, 개발 비용을 상각하려면 4배 이상의 속도 향상을 제공해야 합니다. 그러나 전통적인 기술 스케일링(처음에는 전력 효율을 위한 데나드 스케일링, 이후 비용 효율을 위한 무어의 법칙 스케일링)이 둔화되면서, 우리는 더 이상 단위 비용당 일반 목적 CPU 성능의 높은 복리 성장률을 보지 못하고 있습니다.

동시에 전문화를 필요로 할 만큼 충분히 큰 새로운 종류의 처리 요구가 등장할 수 있습니다. TPU를 구축하게 된 핵심 동기는 모든 안드로이드 사용자가 하루에 단 몇 분의 음성 인식 기능을 필요로 한다면 당시(2013년) Google 전체 컴퓨팅 용량을 두 배로 늘려야 했을 것이라는 관찰이었습니다. GPU나 TPU와 같은 특수 목적 가속기를 통해 대규모 워크로드 클래스에 하드웨어를 최적화하는 것이 이러한 수요-공급 격차를 해결할 수 있는 유일한 옵션이었습니다.

### 6.3.2 ML 가속기 및 TPU 시스템
심층 신경망(DNN)과 AI 워크로드(3장에서 설명)는 계산 집약적입니다. 이들은 빠른 선형 대수 연산, 대규모 분산 컴퓨팅, 높은 대역폭 메모리에 대한 필요성으로 특징지어집니다. AI 워크로드는 별도의 계산 요구 사항을 가진 여러 단계를 포함합니다. 훈련(Training)은 수렴 시간 요구 사항을 충족하면서 많은 매개변수와 대량의 훈련 데이터를 가진 대형 모델을 수용하기 위해 수백 또는 수천 개의 칩을 필요로 합니다. 추론(Inference) 또는 ML 서빙은 사용자 대면 서비스로 엄격한 지연 시간 기한이 있지만 더 작은 포드에서 실행될 수 있습니다. 미세 조정(Fine-tuning)과 증류(Distillation)는 중간 지점을 차지하지만 고유한 고려 사항(예: 중간 규모 시스템에서 작은 데이터셋으로 큰 모델 실행)이 있습니다. 거대 언어 모델(LLM)의 부상은 이러한 요구를 증폭시켰습니다(3장의 그림 3.8 참조).

이러한 수요는 AI를 위한 특화된 도메인별 가속기 및 시스템 개발을 주도했습니다. 원래 그래픽 렌더링에 주로 사용되던 GPU(Graphics Processing Units)는 현대 ML 가속의 초석으로 부상했습니다. Google은 AI를 위해 특별히 제작된 TPU(Tensor Processing Units)라는 훨씬 더 전문화된 가속기의 여러 세대를 설계하고 배포했습니다. GPU와 TPU 외에도 업계는 다양한 가속기를 탐색하고 도입했습니다. Microsoft는 Catapult 및 Brainwave 프로그램[17]을 통해 FPGA를 활용했고, 나중에 전용 ASIC인 Maia[18]를 도입했습니다. Meta는 MTIA(Meta Training and Inference Accelerator)[19]를 개발했으며, Amazon은 Inferentia 및 Trainium 시스템[20], [21]을 도입했습니다.

#### 6.3.2.1 TPU
Google의 Tensor Processing Units (TPUs)는 AI를 위해 특별히 제작된 도메인별 가속기 및 시스템입니다. 그 진화(그림 6.12)는 AI 환경의 급격한 변화를 반영하며, 각 세대는 새로운 도전과 기회를 해결하기 위한 아키텍처 혁신을 도입했습니다. 2015년에 출시된 TPUv1은 추론 가속화에 중점을 두었습니다. TPUv2는 훈련 지원을 추가하여 여러 칩을 연결하는 고대역폭 토러스 네트워크를 특징으로 했습니다. TPUv3는 액체 냉각을 통해 성능을 더욱 확장하여 더 강력한 칩과 더 큰 시스템을 가능하게 했습니다. TPUv4는 광 회선 스위치(OCS)를 통한 재구성 가능한 광 인터커넥트를 도입하여 유연한 시스템 구성과 향상된 회복탄력성을 허용했습니다. 또한 추천 모델의 불규칙한 메모리 접근 패턴에 최적화된 특수 데이터 흐름 프로세서인 SparseCore를 추가했습니다. TPUv5 및 TPUv6 (Trillium) 시스템은 칩당 성능, 메모리 및 최대 포드 크기를 더욱 확장했습니다.

그림 6.13은 TPU 칩의 주요 블록을 보여줍니다. 행렬 곱셈 엔진(MXU)과 벡터 처리 유닛(VPU)으로 구성된 TensorCore, SparseCore, 고대역폭 메모리(HBM), 그리고 인터칩 인터커넥트(ICI) 라우터입니다.

행렬 곱셈 유닛(MXU)은 TPU의 심장입니다. 이는 파이프라인 방식으로 행렬 곱셈을 효율적으로 실행하는 상호 연결된 처리 요소들의 네트워크인 시스톨릭 배열(systolic-array) 아키텍처를 사용합니다. 각 처리 요소는 간단한 연산(곱셈 및 누적)을 수행하고 결과를 이웃에게 정기적인 데이터 흐름으로 전달합니다. 이 설계는 사이클과 에너지를 모두 소비하는 명시적인 로드 및 저장 명령의 필요성을 제거합니다(메모리 연산은 부동 소수점 연산보다 약 10배의 에너지를 소비합니다). 대신 값들은 시스톨릭 배열을 통해 파이프라인 방식으로 흐르며 제어 및 배선 오버헤드를 줄입니다. 256x256 시스톨릭 배열에서 각 피연산자는 256번 재사용됩니다. 이는 딥 러닝의 중심이 되는 대형 행렬 연산에 대해 높은 처리량과 에너지 효율성을 가져옵니다.

MXU 옆에는 2차원(128x8 와이드) 벡터 엔진인 벡터 처리 유닛(VPU)이 있습니다. 이 유닛은 DNN 워크로드에서 일반적인 벡터 연산(예: softmax, layernorm)을 수행하며, MXU로 데이터를 스트리밍하거나 받는 역할도 담당합니다. VPU는 벡터 메모리(vmem, 대형 온칩 소프트웨어 제어 스크래치패드)와 벡터 레지스터 파일을 수용합니다. 이 구성 요소들은 메인 고대역폭 메모리(HBM)에서 피연산자를 프리패치하고 준비하며, MXU의 출력과 부분 합을 유지하는 데 사용됩니다.

SparseCore는 특수 데이터 흐름 프로세서입니다. TensorCore를 보완하며 희소 텐서(sparse tensors)에서 발생하는 대역폭 집약적이고 불규칙한 메모리 연산을 처리합니다. SparseCore는 가변 크기의 동적 데이터 스트림을 서비스하는 특수 엔진을 특징으로 하여, HBM 대역폭에서 작동하면서 미세한 스캐터(scatter) 및 개더(gather)를 효과적으로 처리할 수 있게 합니다.

고대역폭 메모리(HBM)는 기존 DRAM보다 한 자릿수 더 높은 대역폭을 제공하여 DNN 가속기에 필수적입니다. 마지막으로, 인터칩 인터커넥트(ICI) 라우터는 다중 TPU 칩 간의 통신을 가능하게 하며, 저지연, 고대역폭 및 에너지 효율적인 통신을 위한 맞춤형 프로토콜을 구현합니다.

TPU 슈퍼컴퓨터는 이러한 TPU 칩들의 네트워크로 구성되어 강력하고 확장 가능한 병렬 컴퓨팅 시스템을 형성합니다. 초기 세대는 2D 토러스 네트워크를 채택했고, 이후 대역폭과 확장성을 높이기 위해 3D 토러스로 발전했습니다. TPUv4는 장애 발생 시 고가용성을 유지하고 점진적 배포를 가능하게 하면서도 더 큰 규모를 지원하기 위해 광 회선 스위치(OCS)를 통한 재구성 가능한 광 인터커넥트를 도입했습니다(그림 6.14). TPUv4 시스템은 4x4x4 큐브 구성의 전기 링크를 사용하여 64개 칩을 연결하며, 이것이 최소 구성 블록을 형성합니다. 이러한 큐브들은 OCS를 통해 연결되어 최대 4,096개 칩(TPUv5에서는 8,192개)의 논리적 포드(임의의 3D 토러스 모양)를 형성합니다.

OCS는 구성 요소 장애 시 가용성을 향상시킵니다. 4x4x4 큐브 내의 어떤 요소가 실패하면 소프트웨어는 즉시 OCS를 재구성하여 데이터 센터 내의 대체 4x4x4 큐브를 찾아 동일한 논리적 토폴로지를 유지하고 최종 사용자 워크로드를 위한 원활한 시스템 성능을 보장합니다. OCS는 또한 최소 배포 단위가 4x4x4 큐브이므로 모듈식 및 점진적 배포를 용이하게 합니다. 추가로, OCS는 거대 언어 모델(LLM)의 성능을 최적화하기 위해 토폴로지를 조정할 수 있게 합니다. 사용자는 애플리케이션에 필요한 특정 유형의 병렬성(데이터, 모델, 전문가 또는 파이프라인)에 맞춰 토폴로지를 조정할 수 있습니다. 예를 들어 512 칩 시스템은 8x8x8, 4x8x16 및 기타 배열로 구성되어 애플리케이션 요구에 맞출 수 있습니다.

연속적인 TPU 세대는 또한 피크 FLOPS, 대역폭 및 네트워킹 기능을 증가시켜 대규모에서의 성능과 에너지 효율성을 모두 향상시켰습니다. TPU는 수년 동안 신중하면서도 독단적인 공동 설계 베팅을 해왔습니다. 행렬 곱셈이 딥 러닝 워크로드의 중심이 되고 크기가 커질 것이라는 베팅 하에 대형 시스톨릭 배열(128x128, 256x256)을 통합했습니다. DNN 워크로드의 규모 증가를 예상하여 TPU는 대규모 포드를 지원하기 위한 독특한 네트워킹 기능에 투자했습니다(그림 6.15).

TPU는 bf16 부동 소수점 형식("Brain floating point", 이를 만든 Google Brain 연구 팀의 이름을 따옴)을 도입하여 저정밀도 수치 연산을 개척했습니다. 딥 러닝 모델은 종종 32비트 부동 소수점의 높은 정밀도를 필요로 하지 않습니다. Bf16은 가수(mantissa) 비트(정밀도 결정)를 줄이면서 FP32와 동일한 지수(exponent) 비트를 유지합니다. 반면, 기존 IEEE fp16 형식은 가수와 지수를 모두 줄여 전형적인 DNN 가중치(보통 매우 작음)를 표현하는 데 부적합했습니다. 모델 훈련의 진보는 정밀도에 대한 필요성을 더욱 줄여, 오늘날 가속기는 bf8 및 bf4와 같은 더 짧은 형식도 지원합니다.

TPU는 강력한 소프트웨어 스택과 프로그래밍 가능성을 강조했으며, XLA 컴파일러는 LLM을 위한 융합(fusion) 및 자동 샤딩(auto-sharding)과 같은 핵심 최적화를 수행하여 성능과 효율성에 결정적인 역할을 합니다. 마지막으로, TPU는 성능 및 와트당 성능(성능/TCO의 대략적인 대리 지표[22])을 개선하기 위해 지속적으로 최적화됩니다. 이러한 발전과 설계 선택은 AI의 급격한 진화와 보조를 맞추었으며 최신 추천 시스템 및 거대 언어 모델[23]을 가능하게 했습니다(그림 6.15).

#### 6.3.2.2 GPU
그래픽 처리 장치(GPU)는 병렬 처리를 위해 설계된 특수 프로세서입니다. 각 GPU 칩은 수많은 SIMT(Single Instruction Multiple Threads) 스타일의 코어를 수용하여 높은 계산 처리량을 달성하기 위해 대규모 병렬성을 활용합니다. 원래(1990년대 중반) GPU는 그래픽 처리 가속에 중점을 두었으므로 이름이 그렇게 붙었습니다. 10년 후(2006년), GPU는 전통적인 그래픽 중심 가속기와 고성능 컴퓨팅 시장을 겨냥한 "범용" GPU(GP-GPU)로 나뉘었습니다. 레이 트레이서나 픽셀 셰이더 대신, 이 GP-GPU들은 전체 32비트 및 64비트 부동 소수점 벡터 연산을 가속화했습니다.

몇 년 후, ML 연구자들은 GP-GPU가 신경망 훈련을 가속화하는 데 사용될 수 있음을 발견했고, 2012년 GPU 기반 AlexNet으로 이미지 인식 분야를 크게 발전시켰습니다. 오늘날 범용 GPU는 DNN의 핵심 연산을 가속화하는 특수 행렬 곱셈 엔진으로 증강되었으며, 다시 그냥 GPU라고 불립니다.

딥 러닝을 위한 GPU 배포는 계산 처리량과 통신 효율성을 모두 극대화하기 위해 다중 노드 구성에 의존합니다. 일반적인 시스템은 고속 PCIe 링크를 통해 가속기에 연결된 CPU 호스트를 포함합니다(그림 6.16). GPU는 초당 수조 번의 연산을 처리할 수 있으므로 시스템 균형을 맞추기 위해 표준 서버가 제공하는 100Gbps 정도보다 훨씬 많은 초당 수조 비트를 필요로 합니다. 결과적으로 GPU 배포는 최대 세 개의 별도 네트워크를 포함합니다. 첫 번째 네트워크(상단)는 CPU를 위한 일반 서버 네트워크이며, 일반적으로 세 네트워크 중 대역폭이 가장 낮습니다. 두 번째 네트워크는 PCIe를 통해 GPU에 연결된 NIC(보통 이더넷)를 포함하여 스토리지나 다른 GPU 랙에 추가 대역폭을 제공합니다. (TPU와 달리 GPU는 여러 랙에 걸친 특수 네트워크가 없습니다.)

각 트레이에는 여러 GPU 칩이 있으며, 세 번째 네트워크인 고대역폭 특수 GPU-GPU 인터커넥트(Nvidia의 NVLink 등)로 상호 연결됩니다. 이 세 번째 네트워크는 빠른 GPU 간 데이터 교환을 가능하게 하고 통신 지연을 최소화하여 TPU의 ICI 네트워크와 유사한 역할을 수행합니다.

또한 일부 GPU(예: NVIDIA의 Grace-Hopper 아키텍처)는 호스트 CPU와 GPU 사이에 고대역폭 경로를 제공하여 이 용도로 사용되던 편리하지만 느린 PCIe 링크를 대체합니다.

개별 노드를 넘어, 대규모 훈련은 종종 여러 GPU 장착 시스템 노드를 상호 연결하는 것을 포함합니다. InfiniBand 및 고속 이더넷과 같은 고성능 네트워크는 동기식 훈련 중 분산 시스템 전반에 걸친 빠른 파라미터 동기화를 촉진하는 데 중요한 역할을 합니다. NVIDIA NCCL(Collective Communication Library)과 같은 특수 소프트웨어 라이브러리는 all-reduce 및 all-gather와 같은 집합 통신 연산에 더욱 최적화되어 통신 오버헤드를 최소화하고 이러한 고대역폭 네트워크의 활용도를 극대화합니다. 그림 6.17은 훈련에 사용되는 네트워크 연결된 GPU 포드를 보여줍니다.

연속적인 GPU 세대는 FLOPS, 메모리 용량 및 대역폭, 네트워킹 기능을 지속적으로 증가시켰습니다. 또한 저정밀도 수치 연산(bf16에서 fp8, fp4로)에 대한 투자는 피크 FLOPS를 개선하고 더 큰 모델을 가능하게 합니다.

### 6.3.3 비디오 가속기 및 VCU 시스템
Google에서 머신러닝용 TPU 가속기를 도입한 성공적인 경험을 바탕으로, 우리는 또 다른 기초적인 워크로드인 비디오 처리에 집중했습니다. 비디오는 인터넷 트래픽의 지배적인 원천입니다[24]. 비디오 처리는 클라우드의 몇 가지 중요한 워크로드(비디오 스트리밍, 화상 회의, 온라인 게임 등)의 기반이 됩니다. 라이브 스트리밍의 인기 증가와 더 높은 비디오 품질/해상도(예: 4K/8K 비디오)의 결합은 비디오 처리에 필요한 컴퓨팅 자원의 기하급수적인 증가에 기여했습니다. 이에 비해 비디오 서빙(즉, 실제 스트리밍)은 처리된 비디오 블록을 NIC로 복사하기만 하면 되므로 계산 비용이 저렴합니다.
비디오 처리는 비디오 워크로드의 다양성 때문에 독특한 과제를 제기합니다.

*   **다양한 포맷과 해상도**: 모든 입력 비디오는 다양한 최종 사용자 기기 및 네트워크 조건을 지원하기 위해 여러 출력 포맷과 해상도로 트랜스코딩됩니다.
*   **복잡한 알고리즘**: 비디오 트랜스코딩은 디코딩, 스케일링, 인코딩을 위한 계산 집약적인 알고리즘을 포함하며, 압축 비용, 처리량, 지각 품질 간의 트레이드오프가 가능합니다.
*   **다양한 워크로드**: 다양한 비디오 제품(예: YouTube, Photos, 온라인 게임)은 지연 시간, 저장 공간, 재생 품질에 대한 요구 사항이 다릅니다.

비디오 처리의 다양한 구성 요소 중 비디오 트랜스코딩은 가장 중요한 구성 요소를 나타냅니다. 소비자 기기용으로 사용 가능한 솔루션에 비해 클라우드 사용 사례는 더 높은 처리량, 품질, 효율성, 유연성 및 신뢰성을 요구하지만 더 높은 전력 엔벨로프에서 실행될 수 있습니다. 이러한 고유한 요구 사항은 클라우드 기반 비디오 트랜스코딩 하드웨어에 대한 근본적으로 다른 접근 방식으로 이어집니다.

Video Coding Unit (VCU)는 웨어하우스 규모 분산 시스템에서 유연성과 효율성을 제공하도록 설계된 하드웨어 가속기입니다[25]. 소프트웨어-하드웨어 공동 설계가 개별 하드웨어 블록에서 보드, 노드, 지리적으로 분산된 클러스터에 이르기까지, 그리고 하드웨어, 펌웨어, 분산 시스템 소프트웨어 전반에 걸쳐 시스템 아키텍처(그림 6.18 참조)에 영향을 미쳤습니다.

VCU SoC는 처리량과 밀도에 최적화되어 있으며, 다중 인코더 및 디코더 코어와 지원 메모리 서브시스템으로 구성됩니다(그림 6.19 참조). 펌웨어 관리 큐는 높은 인코더 활용도를 달성하기 위해 동시 작업을 허용합니다. VCU는 프레임을 VP9 비트스트림으로 인코딩하는 것과 같은 프레임 수준 작업을 지원합니다. 각 애플리케이션은 VCU 메모리에 지속적인 스트림 상태와 함께 명령 대기열을 가집니다.

코덱별 프로그래밍은 사용자 공간에서 발생합니다. 펌웨어는 명령의 내용을 모르며 단지 다음 사용 가능한 코어로 보낼 뿐입니다. 이 설계는 애플리케이션 소프트웨어가 일반적인 데이터 센터 주가(주당 여러 번)로 업데이트될 수 있게 하며, VCU 수명 동안 인코딩 효율성 개선을 가능하게 했습니다.

두 개의 VCU ASIC이 PCIe 보드에 통합됩니다(그림 6.20). 시스템당 10개의 보드가 장착되며, 이러한 시스템 랙들은 데이터 센터 내 클러스터 전체에 배포됩니다. 시스템당 높은 VCU 밀도는 TCO 달러당 성능을 향상시키며, VCU 하드웨어 오류가 전체 시스템에 영향을 미치지 않으므로 높은 신뢰성을 유지합니다.

데이터 센터를 하나의 컴퓨터로 보는 패러다임에 따라, 우리의 클러스터 스케줄링 시스템은 트랜스코딩 작업을 VCU 시스템으로, 비트랜스코딩 처리를 비-VCU 머신으로 지시합니다. 논리적 작업자 풀(worker pools)은 VCU 또는 CPU 트랜스코딩 자원으로 구성되며, 사용 사례와 우선순위에 따라 작업이 풀에 예약됩니다. 다차원 빈 패킹 스케줄러는 인코더 코어 및 메모리와 같은 물리적 자원과 PCIe 대역폭과 같이 속성이 모호한 지표를 모두 나타내는 자원 차원을 기반으로 작업 항목을 작업자에게 매핑합니다. 스케줄러는 유휴 작업자가 다른 풀에서 사용될 수 있도록 논리적 풀에서 해제되게끔 작업자의 부하를 최대화하는 것을 목표로 합니다. 트랜스코딩 실패 시 작업은 논리적 풀 내에서 다시 예약됩니다.

VCU는 수년간 배포되어 YouTube, Google Meet, Google Photos 같은 제품에 혜택을 주었습니다. VCU는 잘 튜닝된 CPU 기반 시스템에 비해 효율성을 한 자릿수 향상시킵니다. 우리는 효율성 지표로 TCO 대비 성능(Perf/TCO)을 사용합니다(TCO는 주어진 시스템의 자본 및 운영 비용을 포함). 그림 6.21은 1세대 VCU의 처리량과 Perf/TCO를 CPU 및 GPU 처리와 비교합니다. 오프라인 2패스 인코딩의 경우, VCU 시스템은 H.264와 VP9에 대해 각각 소프트웨어 인코딩보다 8배~20배 더 높은 인코딩 처리량을 제공합니다. 하드웨어 가속의 가치는 더 높은 압축 이득을 제공하기 위해 컴퓨팅 부하가 증가한 새로운 비디오 코딩 표준에서 더욱 두드러집니다. VCU는 대규모 컴퓨팅 용량의 돌파구를 제공하며, 특히 VP9의 경우 비용 때문에 불가능했던 다중 출력 트랜스코딩(MOT)과 같은 새로운 기능을 가능하게 했습니다.

VCU의 진화는 대규모 고급 비디오 처리의 새로운 가능성을 열어주며 인코더 품질, 처리량 및 효율성을 개선합니다. 차세대 고품질 오픈 코덱인 AV1의 추가는 비디오 인코딩의 미묘한 과제를 강조합니다. AV1은 기존 CPU에서 VP9보다 5~10배 더 많은 컴퓨팅 자원을 요구하지만, VCU 설계 공간은 더 넓은 범위의 트레이드오프를 제공합니다. AV1의 복잡성은 칩 면적과 전력을 더 높은 처리량과 교환함으로써 해결할 수 있습니다.

새로운 코덱 지원 외에도 차세대 VCU는 확장을 위해 비용 효율성을 요구하는 기능을 도입합니다. CPU나 GPU에서는 너무 비용이 많이 드는 비디오 필터(프레임 또는 서브프레임 수준에서 작동하는 ML 모델 지원 포함)를 가능하게 하기 위해 하드웨어 가속 스케일링 및 전처리 알고리즘 제품군을 확장합니다. 또한 AV1과 같은 새 코덱이 클라이언트 장치의 하드웨어 디코더에 의존하여 광범위하게 채택되는 데 거의 10년이 걸리는 반면, 기존 H.264 및 VP9 코덱에 대한 지속적인 알고리즘 개선은 VCU 수명 동안 가치 있게 유지됩니다. 새로운 VCU 세대에는 인코더 사용 주기를 최대화하기 위한 인코더 코어 기능 재설계, 인코더 클럭 주파수 증가, 더 높은 인코더 처리량을 유지하기 위한 새로운 공정 기술, PCIe 및 메모리 아키텍처 채택과 같은 성능 개선도 포함됩니다.

## 6.4 네트워킹 (Networking)

WSC는 랙, 클러스터, 데이터 센터, 캠퍼스, 심지어 전 세계 여러 지역에 걸친 대규모 분산 시스템입니다. 따라서 네트워킹 서브시스템은 WSC 시스템 설계의 기본입니다.

### 6.4.1 트레이에서 행성까지 전체 네트워킹 (Overall networking from tray to planet)
데이터 패킷의 여정은 이메일의 일부, 비디오 조각, 또는 사용자 명령과 같은 단순한 정보 조각으로 시작됩니다. 이 데이터는 호스트 머신(종종 VM 내)에서 실행 중인 애플리케이션 내에서 생성되어 호스트 네트워킹 스택으로 전달됩니다. 준비된 데이터는 네트워크 인터페이스 카드(NIC 또는 스마트 NIC)로 넘겨져 프레임으로 캡슐화되고 헤더와 오류 감지를 위한 체크섬이 추가됩니다. 이 시점에서 패킷은 여정을 시작할 준비가 됩니다. 패킷은 클러스터 네트워크를 통해 동일한 클러스터 내의 목적지 머신으로 이동할 수 있습니다. 또는 캠퍼스나 광역 네트워크(WAN)를 통해 원격 클러스터의 머신으로 계속 이동할 수 있습니다. 패킷은 ISP 제공업체와 연결되는 엣지 네트워크를 통해 더 넓은 인터넷으로 갈 수도 있습니다. 이 여정에서 마주치는 네트워킹 스택은 종종 여러 다른 구성 및 세대의 여러 요소로 구성됩니다. 아래에서는 2023년경 설계를 중심으로 네트워킹 아키텍처를 설명합니다. 더 자세한 내용은 여러 Google 논문[26]~[32]에서 확인할 수 있습니다.

### 6.4.2 호스트 네트워킹 및 스마트 NIC (Host networking and smartNICs)
WSC 호스트는 높은 처리량과 낮은 지연 시간, 낮은 꼬리 지연, 트래픽 쉐이핑(페이싱 및 속도 제한), OS 바이패스, 엄격한 보안 및 라인 속도 암호화, 디버깅 가능성, QoS 및 혼잡 제어 등을 요구하며, 이 모든 것을 낮은 호스트 CPU 사용량으로 수행해야 합니다. 퍼블릭 클라우드는 가상화 및 VM 마이그레이션 지원 기능을 추가로 요구합니다. 이러한 요구 사항은 두 가지 상호 보완적인 접근 방식으로 충족될 수 있습니다: 호스트 CPU를 활용하여 낮은 지연 시간과 풍부한 기능을 제공하는 온로드(onload) 방식과 패킷 처리 및 암호화 기능과 같은 기능을 위해 NIC 카드의 컴퓨팅을 사용하는 오프로드(offload) 방식입니다. 구현 예로는 Azure의 FPGA bump-in-the-wire NIC[33], 베어 메탈 지원을 위한 Amazon의 맞춤형 NIC, Google의 호스트 측 트래픽 쉐이핑[34], Google의 Andromeda 네트워크 가상화[35]가 있습니다.

아래에서는 Google에서 사용되는 인프라 처리 장치(IPU)에 대해 설명합니다.
Google IPU 접근 방식의 핵심 원칙 중 하나는 서버 제어 평면을 동일한 CPU 코어 세트의 서버 데이터 평면에서 분리하는 것입니다. 서버의 제어 평면은 서버 모니터링, 하드웨어 리소스 가상화, 컨테이너 수명 주기, 네트워크 및 스토리지 I/O 처리, 로깅 및 고객 워크로드 실행과 무관한 기타 기능 목록을 담당합니다.

최적화 없이는 서버 제어 및 가상화 오버헤드가 상당할 수 있으며, 경우에 따라 전체 컴퓨팅 용량의 50% 이상으로 급증할 수 있습니다. 이 오버헤드는 피크를 위해 정적으로 프로비저닝되어야 하므로 비용을 감당할 수 없게 만들거나, 평균 근처에서 인위적으로 조절되어 전체 서버 성능을 제한해야 합니다. 또한 고객 워크로드와 함께 동일한 CPU에서 제어 평면을 실행하면 악의적인 게스트가 가상화 버그를 찾아 악용하여 호스트 CPU의 루트 권한을 얻을 경우 보안 위험이 발생합니다.

서버 제어 평면을 서버 데이터 평면에서 분리하면 그림 6.22와 같은 시스템 서버 아키텍처가 됩니다. 이 아키텍처에서는 IPU에 부착된 보안 제어 모듈(DC-SCM)이 연결된 CPU의 통합 RoT와 함께 전체 머신의 루트 오브 트러스트(RoT)를 제공합니다. 머신은 IPU 자체, 하나 이상의 CPU/DRAM 모듈, 그리고 NIC, SSD, GPU/TPU 및 암호화, 압축, 기타 데이터 처리를 위한 가속기를 포함한 다수의 PCIe SR-IOV 기반 I/O 장치로 구성됩니다. IPU는 유연한 물리적-가상 장치 매핑, 실시간 마이그레이션, 테넌트 격리와 같은 가상화 기능의 하드웨어 가속을 포함하여 호스트 인터페이스의 일부로 PCIe 스위치를 포함합니다. 이 IPU 기반 서버 아키텍처는 보안, 격리 또는 성능을 손상시키지 않으면서 공급업체 OTS 카탈로그의 CPU 및 I/O 장치를 빠르게 배포할 수 있게 합니다.

그림 6.23은 PCIe 애드인 카드 형태의 Google 첫 번째 Titanium IPU를 보여줍니다. 오른쪽에는 높은 수준의 블록 다이어그램이 나와 있습니다. Intel과 공동 개발하고 Intel E2100으로 상용화된 이 1세대 IPU 칩은 200Gbps 네트워크 연결을 제공하고 16개의 Neoverse N1 ARM 코어를 통합합니다. IPU SoC 설계는 서버 데이터 평면에서 물리적으로 분리된 오프로드 컴퓨팅 복합체에서 서버 제어 평면을 실행하여, 고객과 Google 모두에게 더 강력한 보안을 제공함과 동시에 고객 워크로드에 베어 메탈 성능을 제공합니다.

NIC와 데이터 가속기를 통합하면 네트워킹 및 스토리지 하드웨어 가속을 오프로드 컴퓨팅 복합체와 긴밀하게 결합합니다. 이를 통해 하드웨어 및 소프트웨어 패킷과 명령 처리를 유연하게 엮을 수 있습니다. 또한 유연한 I/O 가상화 및 리소스 분리(disaggregation)를 가능하게 하며 전체 시스템 TCO를 크게 줄입니다.

### 6.4.3 클러스터 네트워킹 (Cluster networking)
WSC 클러스터는 수천 개의 서버를 고속으로 상호 연결해야 합니다. 불행히도 우리는 임의로 큰 스위치를 그냥 사거나 만들 수 없습니다. 주어진 시점에서 사용 가능한 기술은 모든 칩이 전력 및 핀 제한이 있기 때문에 칩당 총 용량을 제한합니다. 예를 들어 2024년 현재 일반적인 상용 실리콘 스위치 칩은 최대 51.2 Tbps(128x 400 GbE 포트)의 이등분 대역폭(bisection bandwidth)을 지원하며 200 Tbps 이상을 제공할 수 있는 칩은 없습니다.

우리는 개별 스위치 칩을 계단식으로 연결하여 더 큰 스위치를 만들 수 있는데, 일반적으로 Fat tree 또는 Clos 네트워크 형태입니다[36], [30]. Clos 네트워크는 1952년 그 속성을 처음 공식화한 Charles Clos의 이름을 따서 명명되었습니다. 기본 아이디어는 각 리프(leaf) 스위치가 포트를 하향(서버) 포트와 상향(스파인) 포트로 나눈다는 것입니다. 완전히 연결된 트리에서는 포트의 절반이 각각의 방향으로 가므로, $k$-포트 스위치가 있다면 $k/2$ 포트는 서버에 연결되고 나머지는 $k/2$ 스파인 스위치 중 하나에 연결됩니다. 이 토폴로지를 사용하면 모든 리프 스위치는 다른 모든 리프 스위치로 가는 $k/2$개의 경로를 가지며, 각 경로는 서로 다른 스파인 스위치를 통과합니다. 실제 네트워크에서는 이 개념이 재귀적으로 적용됩니다. 스위치가 섀시 내부에서 결합되어 더 큰 스위치를 형성하고, 여러 스위치 섀시가 결합되어 더 큰 가상 스위치처럼 보이는 "블록"을 형성하여 그림 6.24와 같은 데이터 센터 네트워크를 구성할 수 있습니다.

$k$-포트 스위치를 사용하는 트리는 $5k^2/4$개의 스위치를 사용하여 $k^3/4$개의 서버 간에 전체 처리량을 지원할 수 있어 수만 개의 포트가 있는 네트워크를 허용합니다. 그러나 서버 수가 증가함에 따라 다른 서버로 가는 각 경로가 더 많은 포트를 포함하게 되므로 비용이 크게 증가합니다. 가장 단순한 네트워크(단일 중앙 스위치로 구성된 단일 단계)에서는 각 경로가 스위치 인(in)과 스위치 아웃(out) 두 개의 포트로 구성됩니다. 위의 3단계 네트워크는 이를 10개 포트로 5배 늘려 서버 포트당 비용을 크게 증가시킵니다. 트리의 모든 서버 포트는 8개의 추가 스위치 포트와 매칭되어야 하기 때문입니다. 따라서 클러스터 크기(및 이등분 대역폭)가 커질수록 연결된 서버당 비용도 증가합니다.

포트 비용은 상당하며, 특히 링크가 몇 미터를 넘어가서 광 인터페이스가 필요한 경우 더욱 그렇습니다. 오늘날 100m 100Gbps 링크의 광학 부품은 수백 달러(두 개의 광 포트, 광 케이블, 광 종단 및 설치 비용 포함)가 들 수 있으며, 여기에는 네트워킹 구성 요소 자체(스위치 및 NIC)는 포함되지 않습니다. 따라서 현대 데이터 센터 네트워크에서 광학 장치는 가장 큰 단일 비용 항목을 나타냅니다. 데이터 센터 내부의 광 링크는 도달 거리(즉, 최대 거리)가 다르기 때문에 다양한 유형의 광학 장치가 도달 거리, 링크 예산 및 신호 대 잡음비(SNR) 요구 사항에 따라 비용과 전력을 최적화합니다. 데이터 센터 내부의 대부분의 링크는 짧기 때문에 최적화된 버전을 통해 비용을 크게 줄일 수 있습니다. Google의 첫 번째 인터커넥트(Saturn)조차도 비용을 줄이기 위해 비표준 광학 장치를 사용했습니다[37].

머신당 네트워킹 비용을 더 줄이기 위해 WSC 설계자들은 종종 랙 상단(ToR) 스위치 위("북쪽")의 네트워크를 초과 가입(oversubscribe)합니다. 랙 내부에서 전체 대역폭을 제공하는 것은 랙에 단일 스위치로 연결할 수 있을 만큼 적은 수의 서버가 포함되어 있기 때문에 꽤 저렴합니다. 전체 NIC 속도(예: 200 Gbps)로 64개 서버를 상호 연결할 수 있는 64포트 스위치를 찾는 것은 어렵지 않습니다. 전체 Fat tree에서 각 스위치는 클러스터 패브릭을 향하는 동일한 수의 포트가 필요합니다. 상향 링크는 다시 집계(aggregation) 및 코어 계층에서 더 많은 링크를 필요로 하여 값비싼 네트워크로 이어집니다.

초과 가입된 네트워크에서는 서버와 패브릭 포트 간의 1:1 비율을 늘립니다. 예를 들어 2:1 초과 가입을 사용하면 랙 스위치에 연결된 서버의 총 대역폭의 절반에 대해서만 Fat tree를 구축하여 트리의 크기와 비용을 줄이지만, 서버당 사용 가능한 대역폭도 2배로 줄입니다. 각 서버는 여전히 200 Gbps의 트래픽 피크를 칠 수 있지만, 모든 서버가 동시에 트래픽을 보내는 경우 평균 100 Gbps만 가능합니다. 실제로는 3 이상의 초과 가입 비율이 일반적입니다. 예를 들어 200 Gbps 포트가 있는 64포트 랙 스위치는 48개 서버를 16개 업링크에 연결하여 3:1 초과 가입(서버당 66.7 Gbps)을 할 수 있습니다.

그림 6.25는 10년 이상 전에 처음 배포된 Google의 Jupiter Clos 네트워크[38] 구조를 보여줍니다. 이 네트워크의 이등분 대역폭은 초당 1.3 페타비트입니다. 이 다단계 네트워크 패브릭은 상용 실리콘으로 구축된 저라디스(low-radix) 스위치를 사용했으며, 각 스위치는 16x40 Gbps 포트를 지원했습니다. 각 40G 포트는 4x10G 또는 40G 모드로 구성할 수 있었습니다. 서버는 10Gbps 또는 40Gbps 이더넷 NIC를 사용하여 ToR 스위치에 연결되었습니다. Jupiter의 기본 구성 요소는 Centauri 스위치로, 2개의 라인 카드를 수용하는 4RU 섀시이며 각 카드에는 2개의 스위치 칩이 있습니다. 예시 ToR 구성에서 각 스위치 칩은 서버로 48x10G, 패브릭으로 16x10G로 구성되어 3:1의 초과 가입 비율을 생성합니다. 서버는 40G 모드로 구성되어 40G 버스트 대역폭을 가질 수도 있습니다.

ToR 스위치는 네트워크 패브릭의 규모를 늘리기 위해 집계 블록(Aggregation Block) 계층에 연결됩니다. 각 중간 블록(MB)에는 4개의 Centauri 섀시가 있었습니다. MB의 논리적 토폴로지는 2단계 비차단(non-blocking) 네트워크로, ToR 연결을 위해 256x10G 링크를, 스파인 블록을 통해 나머지 패브릭으로 연결하기 위해 64x40G를 사용할 수 있었습니다.

Jupiter는 외부 연결을 위해 별도의 집계 블록을 사용하며, 이는 각 집계 블록에 전체 외부 대역폭 풀을 제공합니다. 일반적으로 총 클러스터 내부 대역폭의 10%가 1~3개의 집계 블록을 사용하여 외부 연결에 할당됩니다. 이러한 집계 블록은 물리적 및 위상적으로 ToR 연결에 사용되는 것과 동일합니다. 그러나 일반적으로 ToR 연결에 사용되는 포트가 외부 패브릭 연결을 위해 재할당됩니다.

Jupiter를 통해 클러스터 내부 패브릭은 FBR(Fabric Border Routers)을 통해 클러스터 간 네트워킹 계층에 연결됩니다. 동일한 건물 내에 여러 클러스터 패브릭이 배포될 수 있으며, 동일한 캠퍼스 내에 여러 건물이 있을 수 있습니다. 작업 스케줄링 및 리소스 할당 인프라는 캠퍼스 수준 및 건물 수준의 지역성을 활용합니다.

WSC와 비교할 때 HPC(고성능 컴퓨팅) 슈퍼컴퓨터 클러스터는 종종 네트워크 대역폭 대비 연산 비율이 훨씬 낮습니다. 기상 시뮬레이션과 같은 애플리케이션이 모든 노드의 RAM에 데이터를 분산시키고, 노드는 비교적 적은 부동 소수점 연산을 수행한 후 이웃 노드를 업데이트해야 하기 때문입니다. 결과적으로 전통적인 HPC 시스템은 최첨단 링크 대역폭, 훨씬 낮은 지연 시간(특히 상호 연결에서 직접 지원되는 장벽 동기화나 스캐터/개더 연산과 같은 일반적인 기능에 대해), 그리고 일종의 글로벌 주소 공간(네트워크가 CPU 캐시 및 가상 주소와 통합된 곳)을 갖춘 독점적인 인터커넥트를 사용해 왔습니다. 점점 더 이더넷, 이더넷의 HPC 최적화 변형[39], [40], 그리고 Infiniband가 슈퍼컴퓨팅[41]을 지배하기 시작했습니다. 상용 인터커넥트의 성능이 빠르게 향상되고 있기 때문입니다.

### 6.4.4 스파인 없는 네트워킹 (Spine-less networking)
WSC는 대규모(예: 40MW 이상의 인프라)로 배포될 뿐만 아니라, 배포된 컴퓨팅, 스토리지 및 가속기 장치가 항상 진화하고 있어 기본 네트워크 인터커넥트 속도를 불과 몇 년 전 일반적이었던 40 Gbps에서 오늘의 400 Gbps로 정기적으로 갱신해야 합니다. 따라서 WSC 네트워크는 건물 규모로 이미 배포된 상당한 인프라가 호스팅하는 서비스를 중단하지 않고 동적으로 진화해야 합니다.

불행히도 데이터 센터의 Clos 토폴로지는 균일한 고속 스파인 계층을 필요로 합니다. 따라서 클러스터의 어느 부분에서든 더 빠른 포트 속도를 수용하려면 전체 스파인을 업그레이드해야 하는데, 이는 비용이 너무 많이 들어 실용적이지 않습니다. 이러한 경직성은 전체 인프라를 중단하지 않고 네트워크에 더 빠른 장치를 점진적으로 추가하는 것을 어렵게 만듭니다.

광 회선 스위치(OCS)는 이기종 속도를 지원하고 점진적 클러스터 확장을 가능하게 합니다. 광 회선 스위치는 두 세트의 미세전자기계시스템(MEMS) 거울을 사용하여 입력 및 출력 광 포트를 동적으로 연결하여 유연한 포트 대 포트 연결을 생성합니다(그림 6.26).

MEMS 거울 어레이는 미세전자기계적으로 제어되는 거울을 사용하여 제작된 작고 개별적으로 제어 가능한 거울의 집합입니다. 개별 밀리미터 크기의 거울은 두 축을 따라 기울여져 광선의 방향을 정밀하게 제어할 수 있습니다. 모든 들어오는 광섬유와 나가는 광섬유는 매우 정밀하게 정렬(콜리메이션)되어 각 광섬유의 레이저 빔이 첫 번째 거울에 닿습니다. 출력 포트 $(x, y)$에 도달하기 위해 첫 번째 거울은 빔을 출력 거울 열 $x$로 조향하고, 그 거울은 행 $y$의 광섬유로 빛을 보내도록 방향이 지정됩니다. 광 손실을 낮게 유지하려면 나가는 레이저 빔이 출력 광섬유 헤드에 정면으로 닿아야 하므로 콜리메이션이 믿을 수 없을 정도로 정밀해야 합니다. OCS는 단지 빛을 조향할 뿐이므로 사용 중인 네트워크 프로토콜, 파장 및 속도를 인식하지 못하며, 따라서 새로운 광학 장치가 도입될 때 업그레이드할 필요가 없고 여러 속도의 포트를 동시에 처리할 수 있습니다.

광 회선 스위치(OCS) 계층을 사용하면 기존 스파인 계층을 데이터 센터 네트워크에서 제거하여 다양한 집계 블록 간의 직접적인 광 연결을 가능하게 할 수 있습니다(그림 6.27). 새 집계 블록을 연결하거나 제거하려면 OCS를 다시 프로그래밍하여 거울 방향을 바꾸어 새 링크를 설정하기만 하면 됩니다. OCS가 없다면 새 집계 블록을 추가하려면 Clos 메시를 "다시 스트라이핑(re-stripe)"하기 위해 수백 또는 수천 개의 광 포트를 뽑았다가 다시 꽂아야 하는데, 이는 지루하고 오류가 발생하기 쉬운 과정입니다.

OCS 계층은 또한 "코끼리 흐름(elephant flows)"과 같은 애플리케이션별 통신 패턴에 적응하는 동적 토폴로지를 가능하게 합니다. 예를 들어 ML 훈련 실행과 같은 대규모 계산이 두 집계 블록에 걸쳐 있고 더 많은 대역폭이 필요한 경우, OCS 스파인을 재구성하여 이 두 블록에 추가 포트를 할당할 수 있습니다. OCS를 통해 네트워크 연결 재구성이 표준 관행이 되어 데이터 센터에서 처음으로 토폴로지 엔지니어링이 가능해졌습니다.

### 6.4.5 웨어하우스 스케일 데이터 센터의 광학 (Optics in warehouse-scale data centers)
데이터 센터 내의 인터커넥트는 가능한 경우 구리 링크를 사용합니다. 더 저렴하고 전력 효율적이기 때문입니다. 그러나 링크 속도가 증가함에 따라 최대 구리 케이블 길이가 감소하여 오늘날 구리 링크는 랙 내부, NIC에서 ToR로의 링크 및 밀집된 스위치 랙 내의 스위치 대 스위치 링크에서만 실행 가능합니다.

능동 광 케이블(AOC)은 양쪽 끝에 구리 커넥터가 있습니다. 커넥터 헤드의 광전 모듈은 전기 신호를 빛으로 변환합니다. 광 인터페이스가 완전히 닫혀 있으므로 광 트랜시버보다 저렴합니다. AOC는 일반적으로 도달 거리를 연장하고 구리 케이블의 부피를 줄이기 위해 구리 링크의 대안으로 사용됩니다. 이들은 행(row) 내에서 사용됩니다.

데이터 센터 내에서 광 링크는 비용을 최적화하기 위해 거리에 따라 다른 종류의 광학 장치를 사용합니다. 각 스위치 포트에는 필요한 구성을 지원하고 고장 시 교체할 수 있도록 플러그형 광 모듈(QSFP, QSFP-DD, OSFP 등 형식 사용)이 있습니다. 병렬 광섬유를 사용하는 단거리 광학 장치는 약 100m 도달 거리를 가진 다음 단계 광학 장치에 사용됩니다. 이들은 종종 도달 거리가 짧고 더 나은 결함 내성을 위해 레인 브레이크아웃/셔플링이 필요한 ToR에서 집계 블록으로의 연결에 사용됩니다. 더 긴 도달 거리와 OCS를 포함하는 경우를 위해 파장 분할 다중화(WDM)가 사용되는데, 여기서 각 광섬유는 여러 색상의 빛을 전달하여 광섬유 비용과 부피를 줄이고 OCS 스위치 포트를 더 효율적으로 사용합니다.

### 6.4.6 특수 목적 네트워크 (Special-purpose networks)
네트워크 확장성을 해결하는 또 다른 방법은 일부 트래픽을 특수 목적 네트워크로 오프로드하는 것입니다. 예를 들어 서버를 스토리지 장치에 연결하기 위한 별도 네트워크를 구축할 수 있습니다. 이러한 네트워크는 보통 스토리지 영역 네트워크(SAN)라고 불립니다. 트래픽이 더 국지적이라면(모든 서버가 모든 스토리지 장치에 연결될 필요가 없다면) 더 작은 규모의 네트워크를 구축하여 비용을 줄일 수 있습니다. 역사적으로 모든 스토리지는 이런 방식으로 네트워크화되었습니다. SAN은 이더넷보다는 파이버 채널 네트워크를 사용하여 서버를 디스크에 연결했습니다. 오늘날에는 FCoE[42], iSCSI[43]와 같은 프로토콜과 더 최근의 NVMe[44]가 "통합된" 이더넷 네트워크가 전통적인 SAN의 기능을 제공하도록 허용하므로 이더넷이 더 일반적입니다. WSC에서 스토리지 트래픽은 전통적인 스토리지 프로토콜을 완전히 배제할 수도 있습니다. 예를 들어 Google의 스토리지 계층은 Colossus RPC를 통해 액세스됩니다(자세한 내용은 4.1.2절 참조). 유사하게 AWS는 이더넷 상의 독점 프로토콜인 SRD를 사용합니다[45].

역사적으로 스토리지 트래픽이 데이터 센터의 다른 모든 트래픽(애플리케이션 RPC)을 지배했습니다. 더 최근에는 ML 트래픽(GPU-GPU 또는 TPU-TPU)이 또 다른 대규모 트래픽 소스로 부상했습니다. 이 칩들이 초당 수백 테라플롭스를 수행할 수 있으므로 칩당 수백 Gbps를 소비하거나 생산하며, 종종 다른 칩과 주고받습니다. 따라서 칩당 대역폭은 매우 높습니다. TPUv5 노드[46]는 459 TFLOPs의 16비트 연산을 위해 4.8 Tbps의 ICI 대역폭을 제공하며, H100 GPU는 2671 TFLOPs의 16비트 연산을 위해 7.2 TBps의 NVLink 대역폭을 지원합니다. 두 경우 모두 이 대역폭은 작은 블록 전송에 특화되고 이더넷 네트워크보다는 공유 메모리 인터커넥트에 더 유사한 별도 네트워크에 의해 제공됩니다.

그러나 이러한 특수 목적 가속기 인터커넥트는 크기가 제한적입니다. 2025년 기준으로 가장 큰 TPUv7 포드는 9,216개의 칩으로 구성되며, 가장 큰 NVLink 포드는 256개의 H100 GPU로 구성됩니다. 대규모 훈련 실행은 그보다 더 많은 것을 필요로 하여, 데이터 센터 네트워크가 여러 포드 간에 노드당 수백 Gbps 속도의 고대역폭 인터커넥트를 지원해야 합니다. 종종 이 포드 간 네트워크도 별도 네트워크여서 가속기 대 가속기 트래픽은 해당 네트워크로만 이동합니다. 반면, 가속기에 연결된 서버는 나머지 데이터 센터 패브릭에 연결하는 자체 NIC를 포함하며, 애플리케이션 트래픽은 해당 네트워크로만 이동합니다.

### 6.4.7 WAN
이 책에서는 주로 네트워킹 아키텍처의 데이터 센터 측면에 초점을 맞추므로 WAN 및 엣지 네트워킹은 간략하게만 다룰 것입니다. 클라우드 제공업체는 전 세계 데이터 센터를 연결하기 위해 사설 WAN을 개발했습니다. YouTube 트래픽, Google Drive, Google Ads 서빙 등의 내부 수요 때문에 Google의 네트워크는 다른 퍼블릭 클라우드 제공업체보다 훨씬 더 큽니다. 2024년 현재 Google은 31개의 해저 케이블을 소유하거나 공동 소유하고 있습니다(그림 4.5[47]).

WAN은 막대한 대역폭 요구 사항과 탄력적인 트래픽 수요를 처리하도록 설계되었으며, 성능 최적화를 위해 소프트웨어 정의 네트워킹(SDN)과 트래픽 엔지니어링을 사용합니다. 클라우드 제공업체는 또한 SDN 기반 인터넷 피어링 엣지 인프라를 개발했습니다. 이러한 인프라는 비용 효율적으로 확장하고 인터넷 피어링 규모에서 애플리케이션 인식 라우팅을 가능하게 하도록 설계되었습니다. 4.1.4절에서는 다양한 WAN 요소가 어떻게 결합되는지 설명하고, 7.3절에서는 WAN의 SDN 기능을 설명합니다.

이러한 혁신은 클라우드 제공업체가 네트워크의 성능과 신뢰성을 개선하는 데 도움을 주었으며, 또한 새로운 네트워킹 기능을 빠른 속도로 배포할 수 있게 했습니다. WAN 및 엣지 네트워킹에 대한 자세한 내용은 [48]~[53]을 참조하십시오.

## 6.5 스토리지 (Storage)

디버그 로그나 임시 파일과 같은 몇 가지 예외를 제외하고, 영구 스토리지 데이터는 모든 서버에서 접근 가능해야 합니다. 따라서 스토리지는 장치 유형이 아니라 서비스입니다. 낮은 수준에서는 디스크나 SSD 어플라이언스가 네트워크에 블록 장치를 제공하고, 높은 수준에서는 Colossus나 GCS와 같은 파일 시스템이 애플리케이션에 스토리지를 제공합니다.

WSC 워크로드에 의해 처리되는 데이터는 임시(ephemeral) 데이터와 영구(persistent) 데이터의 두 가지 범주로 나뉘는 경향이 있습니다. 임시 데이터는 보통 개별 작업에 비공개이며 작업을 다시 시작하여 재생성할 수 있습니다. 영구 데이터는 보통 분산 워크로드의 공유 상태를 형성하는 데이터입니다. 임시 데이터는 로컬 DRAM이나 로컬 스토리지에 상주하는 경향이 있고 복제되는 경우가 드물며, 일시적 성격과 단일 소유자 덕분에 관리가 단순화됩니다. 반면, 영구 데이터는 훨씬 더 내구성이 있어야 하고 많은 수의 클라이언트가 접근하므로 훨씬 더 정교한 분산 스토리지 시스템이 필요합니다. 다음으로 이러한 WSC 스토리지 시스템의 주요 기능을 논의합니다.

### 6.5.1 스토리지 미디어 (Storage media)
현대 WSC는 다양한 접근 대역폭과 지연 시간 요구 사항을 가진 방대한 양의 데이터를 저장합니다. 이는 수십 년 동안 저장될 수 있지만 거의 접근되지 않는 아카이브 데이터부터, 몇 초 만에 덮어쓰여질 수 있지만 중요한 성능 병목이 되는 스크래치 데이터까지 다양합니다. 단일 기술로는 이 모든 요구를 충족할 수 없으므로 WSC는 스토리지 기술의 신중하게 설계된 조합을 사용합니다.

하드 디스크 드라이브(HDD)는 컴퓨팅 초기 시절부터 살아남은 유일한 스토리지 기술입니다(그림 6.28). HDD는 얇은 자성 코팅이 된 회전하는 플래터 더미로 구성됩니다. 각 플래터에는 어느 위치에서든 자기 편광을 설정하거나 감지할 수 있는 헤드가 있습니다. 헤드는 암(arm) 끝의 공기역학적 캐리지에 있으며, 암은 안팎으로 움직여 플래터의 트랙(track)이라 불리는 다른 링에 도달할 수 있습니다. 특정 블록을 읽으려면 암이 해당 블록이 포함된 트랙으로 이동("탐색", seek)한 다음 해당 블록이 아래로 지나갈 때까지 기다립니다. 따라서 평균 접근 시간은 헤드를 디스크 절반만큼 이동하는 시간과 플래터의 완전한 회전 지속 시간의 절반을 더한 것입니다.

HDD는 허용 가능한 지연 시간과 함께 바이트당 낮은 TCO를 제공합니다. 그러나 회전 디스크 기술의 기계적 한계는 용량이 빠르게 증가함에도 불구하고 총 I/O 대역폭과 접근 지연 시간이 수십 년 동안 거의 일정하게 유지되었음을 의미합니다. 결과적으로 바이트당 사용 가능한 I/O는 극적으로 감소했습니다. 일반적인 최대 처리량 200MB/sec로 25TB 디스크의 모든 바이트를 스트리밍하려면 25,000/0.2 = 125,000초, 즉 35시간이 걸립니다! 이러한 추세로 인해 유명한 스토리지 개척자 짐 그레이(Jim Gray)는 2006년에 "디스크는 새로운 테이프다"[54]라고 선언했습니다.

하드 디스크 드라이브(HDD)에 대한 WSC 요구 사항은 엔터프라이즈 요구 사항과 다르며, 디스크 설계, 꼬리 지연 및 보안에서 다른 트레이드오프가 필요합니다[55]. 10년 전 하드 드라이브는 주로 엔터프라이즈 서버용으로 설계되었지만, 오늘날 대부분의 디스크는 WSC로 들어가므로 하이퍼스케일러 사용 사례에 최적화됩니다. 디스크의 주요 목표는 높은 초당 I/O(IOPS, 일반적으로 탐색에 의해 제한됨), 고용량, 낮은 꼬리 지연, 보안 요구 사항 충족 및 낮은 총 소유 비용(TCO)입니다.

향후 몇 년 내에 드라이브당 IOPS를 늘리기 위해 이중 및 다중 액추에이터(DA 및 MA) HDD가 WSC에 도입될 수 있습니다. 동시에 기와식 자기 기록(SMR) 및 열 보조 자기 기록(HAMR)과 같은 개선 사항이 디스크 용량 증가를 더욱 촉진했으므로, 바이트당 사용 가능한 IOPS가 낮아지는 전반적인 추세는 계속될 것으로 예상되며, 디스크는 주로 콜드 데이터(즉, 보존해야 하지만 거의 접근하지 않는 데이터, 예: 10년 전 백업된 사진)를 저장하는 데 사용됩니다.

솔리드 스테이트 드라이브(SSD)는 "웜(warm)" 또는 "핫(hot)" 데이터를 위해 디스크를 대체했습니다. SSD는 실리콘 플래시 메모리에 직접 데이터를 저장합니다. SSD는 HDD보다 한 자릿수 더 많은 대역폭과 몇 자릿수 더 높은 IOPS를 제공하지만, 바이트당 비용도 몇 배 더 비쌉니다. SSD는 내구성이 제한적입니다. 각 저장 셀은 제한된 횟수(수천 번 정도)만 다시 쓸 수 있습니다(10.1.5절 참조). 그 후에는 신뢰성과 성능을 잃게 되어 워크로드 관리 및 플릿 최적화를 더욱 복잡하게 만듭니다.

실리콘 트랜지스터 미세화가 한계에 다다르면서 SSD 용량 증가와 가격 인하는 훨씬 더 깊은 3차원 플래시 메모리와 메모리 셀당 저장되는 더 많은 비트에 의해 주도됩니다. 특히 더 높은 바이트 밀도는 더 높은 지연 시간과 현저히 낮은 내구성을 대가로 합니다. HDD와 SSD 간의 바이트당 TCO 격차는 계속 줄어들고 있으므로 점점 더 커지는 HDD에서 더 많은 IOPS를 오프로드해야 합니다. 따라서 SSD는 WSC 스토리지 믹스에서 계속 증가하는 비중을 차지합니다.

### 6.5.2 디스크 없는 서버 (Diskless servers)
2010년경 WSC 서버에서 로컬 하드 드라이브는 직접 연결된 스토리지(DAS)를 제공하고 부팅/로깅/스크래치 공간 역할을 했습니다. 네트워크 대역폭은 서버당 1Gbps에 거의 도달하지 못했고, 단일 디스크가 전력을 다해 실행되면 그 대역폭을 거의 포화시킬 수 있었습니다. 그러나 네트워크 속도가 빠르게 증가하는 동안 디스크 처리량은 상대적으로 정체되면서 스토리지를 서버 외부로 옮기는 것이 유리해졌습니다[56]. 스토리지 용량과 I/O 대역폭은 전문화된 독립형 스토리지 트레이를 통해 많은 서버에서 더 쉽고 효율적으로 공유될 수 있으며, 컴퓨팅 용량과 독립적으로 스토리지 용량을 공급하기가 더 쉬워져 두 차원 모두에서 유휴 자원의 낭비를 줄임으로써 WSC 전체 TCO를 크게 절감할 수 있습니다.

스토리지 장치에 대한 대부분의 전통적인 요구가 이제 이러한 트레이에 의해 처리되므로 컴퓨팅 서버는 일반 스토리지가 아닌 특수 목적을 위한 로컬 SSD 스토리지 만을 포함합니다. Google 서버는 부팅 장치 역할을 하는 작은 로컬 SSD를 포함합니다. 동일한 장치가 디버그 로그를 임시로 저장하기도 합니다. 또한 일부 애플리케이션은 처리 장치에 최대한 가깝게 하나 이상의 SSD를 두어 제공되는 극히 낮은 및/또는 일관된 지연 시간을 필요로 합니다. 일반적으로 이러한 로컬 스토리지는 캐시로만 사용됩니다(즉, 데이터가 분산 파일 시스템에도 저장됨). 분산 파일 시스템을 다시 만들지 않고 스토리지에 대한 신뢰성과 내구성을 제공하기 어렵기 때문입니다.

### 6.5.3 디스크 어플라이언스 (Disk appliances)
어플라이언스는 프로세서와 NIC가 있는 어플라이언스 트레이와 여러 HDD, 그리고 최소한의 제어 하드웨어가 있는 하나 이상의 추가 트레이로 구성됩니다. 이 설계를 통해 로컬 조건에 따라 또는 시간 경과에 따른 워크로드 변화에 따라 어플라이언스당 디스크 수를 변경할 수 있습니다. 그림 6.29는 수십 개의 하드 드라이브를 호스팅하는 Google에서 사용되는 디스크 어플라이언스의 예를 보여줍니다(이 경우 하나의 어플라이언스 트레이와 두 개의 추가 트레이 사이에 43개의 드라이브가 분할됨). 어플라이언스 트레이는 이러한 하드 드라이브에 대한 전력, 관리, 기계적 및 네트워크 지원을 제공합니다. 로컬 스토리지를 관리하고 RPC를 통해 클라이언트 요청에 응답하는 맞춤형 소프트웨어 스택을 실행합니다. HDD의 제한된 IOPS와 상대적으로 높은 지연 시간은 적당한 프로세서와 NIC로도 수십 개의 HDD 트래픽을 처리할 수 있게 합니다.

단일 드라이브의 I/O 용량이 100-200 Mbytes/sec로 제한되므로 일반적인 어플라이언스는 128GB RAM(캐싱용)이 있는 "헤드" 서버와 50-100개의 HDD를 쌍으로 구성하여 총 저장 용량 1PB+와 50Gbps 이상의 처리량을 제공합니다. HDD는 각각 8-10W를 소비하여 전체 어플라이언스 전력 소비를 1kW 범위로 만듭니다. 따라서 전체 디스크 어플라이언스 랙은 가속기 랙(100kW+)은 고사하고 서버 랙(20-40kW)보다 훨씬 적은 전력(5-10kW)을 소비합니다.

### 6.5.4 플래시 어플라이언스 (Flash appliances)
6.5.2절에서 논의한 바와 같이 분리형 스토리지는 스토리지 활용률을 개선하여 단위 비용을 줄일 수 있습니다. 이 이점은 지연 시간 및 대역폭 오버헤드와 균형을 이루어야 합니다. 디스크보다 훨씬 높은 대역폭과 낮은 지연 시간을 가진 SSD의 경우에도 이 트레이드오프는 대부분의 애플리케이션에서 수용 가능합니다. 결과적으로 WSC의 대부분의 솔리드 스테이트 스토리지는 네트워크 연결 어플라이언스에 수용됩니다. SSD 어플라이언스는 HDD 어플라이언스와 개념이 유사하지만 제약 조건이 다릅니다.

SSD는 HDD보다 한 자릿수 더 많은 I/O 대역폭을 가지므로 SSD 세트를 관리하려면 훨씬 더 많은 처리 능력, 메모리 및 네트워크 대역폭이 필요합니다. SSD 어플라이언스는 서버급 프로세서, NIC 및 대용량 DRAM을 사용합니다. 이러한 고성능 구성 요소는 SSD 어플라이언스의 TCO 오버헤드를 비슷한 수의 드라이브를 가진 HDD 어플라이언스보다 훨씬 높게 만듭니다. SSD 어플라이언스에서는 일반적으로 SSD 자체가 아니라 프로세서, 네트워크 및 메모리 대역폭이 전체 시스템 처리량의 병목 현상이 됩니다. SSD와 네트워크가 계속 빨라지고 있으므로 신중한 소프트웨어 설계 및 구현이 그 어느 때보다 중요합니다[57]. 클러스터 스케줄링은 SSD 트래픽이 네트워크 병목 현상으로 이어지지 않도록 합니다(7.3.2절 참조).

그림 6.30은 현대적인 SSD 어플라이언스를 보여줍니다. 일반적인 SSD 어플라이언스는 꽤 많은 처리 능력을 갖춘 CPU, 96~128GB RAM, 각각 4~8TB인 12~16개의 SSD, 그리고 400Gbps NIC로 구성됩니다. 일반적인 SSD는 약 20W를 소비하여 총 어플라이언스 전력 소비는 약 700W가 됩니다.

SSD 어플라이언스는 SSD의 I/O 대역폭을 초과 가입할 수 있으므로 네트워크 대역폭이 SSD의 총 대역폭보다 낮을 수 있습니다. 예를 들어 4GB/s의 읽기/쓰기 처리량을 낼 수 있는 16개의 SSD는 총 512Gbps의 처리량을 제공할 수 있지만 200Gbps NIC가 있는 어플라이언스에 연결될 수 있습니다. 이로 인해 일부 성능이 낭비될 수 있지만, 대규모 스토리지 풀의 실제 워크로드 믹스는 일반적으로 드라이브가 피크 시 제공할 수 있는 것보다 TB당 더 적은 대역폭을 필요로 합니다. 따라서 이러한 초과 가입은 SSD 스토리지의 TB당 어플라이언스 비용을 더 잘 상각하여 TCO를 낮춥니다. CXL(Compute Express Link) 및 직접 데이터 캐시 주입(direct data cache injection)과 같은 유망한 신기술은 현재 시스템 병목 현상 중 일부를 완화하고 단일 어플라이언스에서 더 많은 SSD를 지원할 수 있게 할 것입니다.

디스크에 비해 플래시의 가장 매력적인 특성은 무작위 읽기 작업 성능이 거의 세 자릿수 더 좋다는 것입니다. 최악의 경우 플래시에 대한 쓰기는 읽기보다 몇 자릿수 느릴 수 있으며 가비지 컬렉션을 신중하게 관리하지 않으면 쓰기 증폭 및 꼬리 지연을 더욱 증가시킬 수 있다는 점에 유의하십시오. NAND 플래시의 "흥미로운" 특성(읽기/쓰기 비대칭, 읽기/쓰기 간섭, 가비지 컬렉션)은 새로운 도전과 기회를 제공하지만 이 책의 범위를 벗어납니다.

### 6.5.5 스마트 스토리지 (Smart storage)
전통적으로 엔터프라이즈 스토리지 시스템은 단순한 저장 이상의 "스마트" 기능을 포함했습니다. 예를 들어, 보안, 성능 및 바이트당 유효 비용을 개선하기 위해 암호화, 압축 및 블록 중복 제거 기능을 제공했습니다. WSC에서는 이러한 기능이 스택의 더 높은 수준(즉, 파일 시스템 또는 해당 파일 시스템 위에 구축된 데이터베이스)으로 이동했습니다. 모든 데이터는 실제 스토리지 장치에 도달하기 전에 암호화되므로 장치 수준의 압축은 더 이상 작동하지 않습니다. 마찬가지로 파일 시스템은 가용성을 위해 데이터를 복제하므로 저장된 데이터를 중복 제거하여 공간을 절약하려는 스마트 디스크 어플라이언스는 역효과를 낼 것입니다.

스토리지 장치는 여전히 보호의 마지막 계층으로 유용하기 때문에 하드웨어 암호화를 제공합니다. 데이터가 불필요하게 여러 번 암호화될 수 있지만 하드웨어 가속은 암호화 비용을 충분히 낮은 수준으로 줄였으므로 다중 암호화 작업 비용은 모듈성 및 보안 이점으로 인해 상쇄됩니다. 예를 들어 이메일은 Gmail 백엔드에서 사용자별 키로 암호화되어 파일 시스템에 저장되기 전에 파일 키로 추가로 암호화되고, SSD나 디스크에 기록될 때 장치 고유 키로 다시 암호화될 수 있습니다. 이렇게 하면 사용자별 키가 파일 시스템에 노출될 필요가 없어 더 나은 구획화(compartmentalization)를 제공하고, 하위 수준에서 제공되는 기본 암호화는 OS와 같이 내장 암호화가 없는 시스템이 쓴 데이터를 보호합니다.

## 6.6 교차 문제 (Cross-cutting issues)

컴퓨터 설계자는 WSC를 구성하는 다양한 구성 요소에서 성능과 용량의 올바른 조합을 선택해야 합니다. 이 장에서는 올바른 구성 요소가 전체 WSC 시스템을 고려할 때만 명확해지는 많은 예를 논의했습니다. 균형 문제도 이 수준에서 해결되어야 하며, 이상적으로는 시스템에서 실행되는 실제 워크로드의 특성을 파악하고 WSC 수명 동안 워크로드가 어떻게 진화할지 이해해야 합니다. 현재 워크로드에 최적화하고 싶은 유혹이 있지만, 지난 5년 동안 그 구조가 의미 있게 변경되었다면 향후 5년 동안 계속 진화할 가능성이 높으므로, 최적화되었지만 유연하지 않은 아키텍처보다 더 유연한 솔루션의 실제 TCO가 더 나을 것입니다.

다음 고려 사항이 특히 중요합니다.

*   매년 수억 달러를 지출하는 대규모 서비스는 새로운 하드웨어를 활용하기 위해 애플리케이션을 재구조화할 가능성이 높습니다. 이러한 팀과의 강력한 협업은 소프트웨어-하드웨어 공동 설계를 가능하게 하면서도 프로그래밍하기 너무 복잡한 시스템을 피할 수 있게 합니다. 반면 대부분의 기업은 애플리케이션을 최적화할 시간이 없으므로 새로운 아키텍처는 기존 애플리케이션을 잘 지원해야 합니다. 이러한 애플리케이션 대부분의 경우 최적화의 (인력) 비용이 가능한 하드웨어 절감액을 초과하므로 최적화하지 않는 것이 경제적으로 합리적인 선택입니다.
*   가장 비용 효율적이고 균형 잡힌 하드웨어 구성은 여러 워크로드의 결합된 리소스 요구 사항과 일치하는 것이지, 반드시 단일 워크로드에 완벽하게 맞는 것은 아닐 수 있습니다. 예를 들어 탐색이 제한된(seek-limited) 애플리케이션은 대용량 디스크 드라이브가 필요하지 않을 수 있지만, 아카이브 스토리지가 필요한 애플리케이션과 결합되면 대형 디스크가 최적의 솔루션일 수 있습니다. 특정 클러스터가 초기에 단일 애플리케이션을 실행하더라도, 그 애플리케이션이 클러스터의 6년 수명 동안 계속 머물지는 않을 것이므로 다른 애플리케이션을 위해 재구성하는 비용을 예상해야 합니다.
*   대체 가능한(Fungible) 리소스는 더 효율적으로 사용되는 경향이 있습니다. 공유는 활용률을 높이고 사적 리소스는 활용률을 해칩니다. 기술 개선이 연간 10%에 불과한 세상에서 50% 활용률과 80% 활용률의 차이는 거의 5년의 하드웨어 개선과 맞먹습니다! 따라서 충분한 연결성이 제공된다면 소프트웨어 시스템은 원격 서버에서 제공되는 리소스를 유연하게 활용하는 것을 우선순위에 두어야 합니다.

올바른 설계 지점은 데이터 크기와 서비스 인기도에도 달려 있습니다. 예를 들어 데이터 세트는 거대하지만 요청 트래픽이 비교적 작은 서비스는 저렴한 디스크 스토리지에서 콘텐츠 대부분을 직접 제공할 수 있습니다. 데이터 세트 크기가 작거나 상당한 데이터 지역성이 있는 높은 처리량 서비스는 대신 인메모리 서빙의 이점을 누릴 수 있습니다.

마지막으로 첫 번째 요점에서 언급했듯이, 이 공간에서의 워크로드 변화는 WSC 설계자에게 도전을 제시합니다. 서버 수명(일반적으로 6년) 동안 워크로드가 변경될 가능성이 매우 높습니다. 이 문제는 데이터 센터 시설의 수명이 여러 서버 수명(20년 이상)에 걸쳐 있기 때문에 WSC 전체에 더욱 심각합니다. 이러한 경우 WSC 시스템 수명 동안 필요할 수 있는 기계류나 시설 업그레이드 종류를 구상하고 시설 설계 단계에서 이를 고려하는 것이 유용합니다. 사실 우리가 가장 자주 범한 실수는 너무 많은 결정을 설계에 고정시켜 초기 비용/성능을 "최적화"했지만(예: 교체 가능한 확장 카드에 넣는 것보다 구성 요소를 마더보드에 납땜하는 것이 저렴하므로), 결과적으로 실제 수명 TCO를 "비관화(pessimizing)"한 것일 것입니다.

## 6.7 역사적 회고 (Historical retrospective)

WSC의 지난(그리고 첫!) 25년은 서버, 스토리지, 네트워킹 전반에 걸쳐 상당한 변화를 보였습니다.

### 6.7.1 서버 및 가속기
WSC의 서버 설계는 당시 CPU의 높은 클럭 주파수 추세를 활용한 표준 기성품 부품으로 조립된 저비용 시스템으로 시작되었습니다. 이는 점차 Google을 위해 구체적으로 최적화된 완전 맞춤형 서버 설계로 발전했습니다(어떤 면에서는 특정 워크로드에 최적화된 오늘날의 맞춤형 실리콘 가속기를 위한 무대를 마련했습니다).

WSC는 요청 수준 병렬성 및 대규모 분산 시스템과 자연스럽게 일치하는 CPU 설계의 멀티코어 시대를 가장 먼저 완전히 수용(하고 가속화)했습니다. 검색에 최적화된 시스템 설계로 시작하여 서버는 다양한 워크로드를 처리하도록 진화했습니다. 더 최근에는 무어의 법칙 둔화로 인해 점점 더 이질적인 시스템 설계가 나타났는데, 이는 초기 시절의 "모두를 위한 하나의 머신" 철학에서 크게 벗어난 것입니다. 이러한 경향은 ISA에 구애받지 않는 서버(x86, ARM, RISC-V)와 다양한 시스템 균형점에 최적화된 다양한 구성에서 더욱 분명하게 나타났습니다(다음 장에서는 소프트웨어 정의 서버가 이러한 이질성을 관리하는 데 어떻게 도움이 되는지 이야기할 것입니다). 모듈식 서버는 증가된 I/O 데이터 속도와 폼 팩터 및 구성의 확산을 해결합니다. 하이퍼스케일러가 주도한 개방형 표준, 특히 서버 소프트웨어(드라이버, 인증, 테스트 등)를 위한 공통 인프라는 업계에 큰 영향을 미쳤으며, 이제 업계는 WSC 사용 사례에 최적화된 많은 제품을 공급합니다.

동시에 지난 10년은 맞춤형 실리콘 가속기로의 전환이 가속화되었습니다. Google에서는 이것이 머신러닝을 위한 가속기인 TPU로 처음 시작되었습니다. TPU는 현재 6세대에 이르렀으며 훈련과 추론 모두에 최적화되어 있습니다. 마찬가지로 GPU는 그래픽 가속기에서 과학 컴퓨팅을 지원하고 이제는 머신러닝 워크로드를 지원하도록 진화했습니다.

맞춤형 칩은 이제 보안(예: OpenTitan, Caliptra RoT 실리콘), 데이터 처리, 네트워킹을 포함한 다른 도메인을 다룹니다. VCU는 비디오 코딩 워크로드를 가속화하며 현재 3세대에 이르렀습니다(심지어 2024년 에미상을 수상했습니다!). 그러나 여러 면에서 우리는 여전히 가속기 시대의 초기 단계에 있습니다.

### 6.7.2 네트워킹
WSC 네트워킹은 지난 25년 동안 아마도 가장 급진적인 진화를 겪었을 것입니다. 최초의 네트워킹 설계는 스케일업 네트워킹을 위한 "four-post" 벤더 라우터를 기반으로 했으며, 1Gbps NIC, 높은 초과 가입을 가진 10Gbps 업링크, 1Tbps 클러스터 패브릭의 속도와 피드를 가졌습니다. 낮은 서버당 단면 대역폭(cross-section bandwidth)은 서버 부착 스토리지와 랙 지역성을 위한 소프트웨어 최적화를 필요로 했습니다. 이러한 제한은 2008년 상용 실리콘으로 구축된 최초의 Clos 네트워크로 해제되어 서버당 단면 대역폭을 한 자릿수 향상시켰습니다. 소프트웨어 정의 네트워킹은 전통적인 네트워크 라우팅 문제를 분산 시스템 문제로 전환하여 모델 기반 네트워킹 설계 및 개발을 도입했습니다. 네트워킹 속도와 피드는 지난 15년 동안 거의 두 자릿수 증가했습니다.

광학 장치는 비용 절감된 단거리 트랜시버를 기반으로 대형 하이퍼스케일 데이터 센터에서 처음으로 대규모 도입되었습니다. 강력한 리소스 격리와 결합된 Linux 컨테이너는 대규모 클러스터가 효율적인 빈 패킹으로 여러 대형 애플리케이션(검색, Gmail, 데이터 분석, 클라우드 컴퓨팅)에서 공유될 수 있게 했습니다. Google의 Global Cache는 사용자에게 더 가까운 위치에서 정적 콘텐츠와 비디오를 제공하기 위해 개발되었습니다. 다음 혁신의 물결은 캠퍼스와 WAN 수준 모두에서 엔터프라이즈급 상용 실리콘 네트워킹 기술로 비용 효율성을 더욱 높였습니다. 중앙 집중식 트래픽 엔지니어링과 종단 간 대역폭 집행을 갖춘 WAN의 소프트웨어 정의 네트워킹은 고가용성 트래픽과 저가용성 트래픽을 혼합하여 값비싼 WAN 링크를 높은 활용률로 실행할 수 있게 했습니다. 네트워킹 속도와 피드는 계속 진화하여 1Pbps 클러스터 패브릭을 달성했습니다.

지난 10년 동안 혁신은 계속되었습니다. 광 회선 스위칭(OCS)은 점진적으로 배포하고 갱신할 수 있는 모듈식 네트워킹을 허용했습니다. 스파인 없는 네트워크는 클러스터 패브릭에서 더 낮은 비용과 전력을 허용했습니다. 사용자 공간의 새로운 OS 바이패스 기술[58]은 낮은 지연 시간의 높은 IOPS 호스트 대 호스트 통신을 가능하게 했습니다. 소프트웨어 정의 네트워킹은 신뢰성과 기능 속도를 위해 관리 평면을 개선했습니다. Espresso SDN 기반 엣지 피어링은 전통적인 BGP 기반 최단 경로 알고리즘에 비해 라우팅 및 트래픽 밸런싱을 개선했습니다[50]. Andromeda 클라우드 네트워크는 클라우드 가상 네트워킹을 위한 SDN 기반 오버레이를 도입했습니다[35]. 더 최근에는 토폴로지 엔지니어링 최적화가 논리적 네트워크 토폴로지를 트래픽 패턴에 일치시켜 효율성을 개선했습니다[26].

지연 기반 혼잡 제어[59]는 거의 10배의 꼬리 지연 개선을 가져왔습니다. 큰 패킷에 대한 최적화와 장애 주변의 호스트 기반 경로 재설정은 효율성과 신뢰성을 더욱 개선했습니다[52]. 속도와 피드는 이제 100-200Gbps 호스트 NIC, 200-400Gbps 링크, 6.5Pbps 클러스터 네트워킹을 포함합니다. 우리는 이제 스마트 NIC(IPU) 시대에 접어들어 네트워킹 및 스토리지 오프로드와 가속을 수용하고 있습니다. 이러한 설계는 또한 멀티 테넌트 네트워크에서 보안 격리를 개선했습니다. 또한 네트워킹은 OCS를 통해 상호 연결된 대규모 ML 포드와 함께 머신러닝 시스템에 특화되기 시작했습니다.

### 6.7.3 스토리지
서버 컴퓨팅과 마찬가지로 WSC의 스토리지 시스템은 수평 기록(longitudinal recording)이 여전히 주류 하드 드라이브 기술이었던 시대에 타사 시스템을 활용하는 것으로 시작하여 겸손하게 출발했습니다. 곧 성능과 신뢰성을 개선하기 위한 WSC별 혁신을 갖춘 컴퓨팅 머신에 직접 부착된 저비용 맞춤형 스토리지 트레이로 진화했습니다. WSC는 또한 수평 기록에서 수직 기록(perpendicular recording)으로의 산업 전환을 수용하는 데 앞장서서 디스크 드라이브 바이트 밀도를 극적으로 확장할 수 있게 했습니다.

플래시 스토리지는 처음에는 SLC NAND, 나중에는 더 낮은 비용, 더 높은 비트 밀도의 MLC NAND를 사용한 맞춤형 FPGA 기반 PCIe SSD 장치로 시작되었습니다. 스토리지는 최초의 HDD 어플라이언스와 함께 분리되어 더 나은 활용도, 유지 관리 및 TCO를 이끌어냈습니다. 이후 빠르게 증가하는 네트워크 대역폭에 힘입어 SSD도 스토리지 어플라이언스로 이동했습니다.

WSC는 하드 드라이브 기술 발전의 다음 물결인 공기 충전에서 헬륨 충전 HDD로의 전환을 조기에 채택하여 더 낮은 전력과 더 높은 밀도로 더 얇은 플래터를 가능하게 했습니다. SSD는 TLC로, 나중에는 3D 적층 NAND로 전환되었습니다. 더 최근에는 HDD 비트 밀도 확장이 둔화됨에 따라 HSMR 및 에너지 보조 기술을 활성화하는 데 초점이 맞춰졌습니다. 데이터 양은 컴퓨팅보다 훨씬 빠르게 계속 증가하고 있으며, 이에 따라 스토리지는 계속해서 WSC 시스템 설계의 중요한 구성 요소가 될 것입니다. 새로운 클라우드 및 ML 워크로드는 새로운 도전과 요구 사항을 가져오므로 스토리지 시스템은 향후 몇 년 동안 다시 진화할 것으로 예상됩니다.

## References

1.  L. Barroso, J. Dean, and U. Hölzle, “Web search for a planet: The Google cluster architecture,” IEEE Micro, vol. 23, no. 2, pp. 22–28, 2003.
2.  Transaction Processing Performance Council, TPC Benchmark C (TPC-C), https://www.tpc.org/tpcc/.
3.  U. Hölzle, “Brawny cores still beat wimpy cores, most of the time,” 2010.
4.  D. G. Andersen, J. Franklin, M. Kaminsky, et al., “Fawn: A fast array of wimpy nodes,” 7, vol. 54, New York, NY, USA: Association for Computing Machinery, Jul. 1, 2011, pp. 101–109.
5.  L. A. Barroso, K. Gharachorloo, R. McNamara, et al., “Piranha: A scalable architecture based on singlechip multiprocessing,” 2, vol. 28, New York, NY, USA: Association for Computing Machinery, May 1, 2000, pp. 282–293.
6.  K. Lim, P. Ranganathan, J. Chang, et al., “Understanding and designing new server architectures for emerging warehouse-computing environments,” in Proceedings of the 35th Annual International Symposium on Computer Architecture, ser. ISCA ’08, USA: IEEE Computer Society, Jun. 1, 2008, pp. 315–326.
7.  HP Inc., HP launches new class of server for social, mobile, cloud and big data, News Release: https://investor.hp.com/news-events/news/news-details/2013/HP-Launches-New-Class-of-Server-for-Social-Mobile-Cloud-and-Big-Data/default.aspx, 2013.
8.  J. Hamilton, “Cooperative expendable micro-slice servers (CEMS): Low cost, low power servers for internet-scale services,” in Conference on Innovative Data Systems Research (CIDR’09)(January 2009),2009.
9.  Amazon Web Services, AWS Graviton Processor, https://aws.amazon.com/ec2/graviton/.
10. D. DeWitt and J. Gray, “Parallel database systems: The future of high performancedatabase systems,” 6,vol. 35, New York, NY, USA: Association for Computing Machinery, Jun. 1, 1992, pp. 85–98.
11. A. Vahdat and M. Lohmeyer, Enabling next-generation AI workloads: Announcing TPU v5p and AI Hypercomputer, 2023.
12. UALink Consortium, Ultra Accelerator Link Consortium.
13. D. Gibson, H. Hariharan, E. Lance, et al., “Aquila: A unified, low-latency fabric for datacenter networks,” in 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22), 2022, pp. 1249–1266.
14. Open Compute Project, Open Compute Project, https://www.opencompute.org.
15. A. Lagar-Cavilla, A. Muñoz, B. Kelly, and P. J. Indranil Banerjee, Composable security architectures, Presentation at the 2021 OCP Global Summit: https://www.opencompute.org/events/past-events/2021-ocp-global-summit, 2021.
16. A. Lagar-Cavilla, V. Sampath, B. Pillillli, and P. Jayanna, Composable security architectures ii, Presentation at the 2023 OCP Global Summit: https://www.opencompute.org/events/past-events/2023-ocp-global-summit, 2023.
17. A. Putnam, A. M. Caulfield, E. S. Chung, et al., “A reconfigurable fabric for accelerating large-scale datacentre services,” 11, vol. 59, New York, NY, USA: Association for Computing Machinery, Oct. 28, 2016, pp. 114–122.
18. Microsoft Azure, Azure Maia for the era of AI: From silicon to software to systems, Azure Blog: https://azure.microsoft.com/en-us/blog/azure-maia-for-the-era-of-ai-from-silicon-to-software-to-systems/, 2024.
19. A. Firoozshahian, J. Coburn, R. Levenstein, et al., “MTIA: First generation silicon targeting Meta’s recommendation systems,” in Proceedings of the 50th Annual International Symposium on Computer Architecture, ser. ISCA ’23, New York, NY, USA: Association for Computing Machinery, Jun. 17, 2023, pp. 1–13.
20. Amazon Web Services, AWS Trainium, https://aws.amazon.com/machine-learning/trainium/.
21. Amazon Web Services, AWS Inferentia, https://aws.amazon.com/machine-learning/inferentia/.
22. N. P. Jouppi, D. H. Yoon, M. Ashcraft, et al., “Ten lessons from three generations shaped Google’s TPUv4i,” in Proceedings of the 48th Annual International Symposium on Computer Architecture, ser. ISCA ’21, IEEE Press, Nov. 25, 2021, pp. 1–14.
23. G. Team, R. Anil, S. Borgeaud, et al., “Gemini: A family of highly capable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.
24. Sandvine, Global internet phenomena report, https://www.applogicnetworks.com/blog/the-2025-global-internet-phenomena-report, 2025.
25. P. Ranganathan, D. Stodolsky, J. Calow, et al., “Warehouse-scale video acceleration: Co-design and deployment in the wild,” in Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS ’21, New York, NY, USA: Association for Computing Machinery, Apr. 17, 2021, pp. 600–615.
26. L. Poutievski, O. Mashayekhi, J. Ong, et al., “Jupiter evolving: Transforming google’s datacenter network via optical circuit switches and software-defined networking,” in Proceedings of the ACM SIGCOMM 2022 Conference, ser. SIGCOMM ’22, New York, NY, USA: Association for Computing Machinery, Aug. 22, 2022, pp. 66–85.
27. A. D. Ferguson, S. Gribble, C.-Y. Hong, et al., “Orion: Google’s software-defined networking control plane,” in 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21), 2021, pp. 83–98.
28. A. Singh, J. Ong, A. Agarwal, et al., “Jupiter rising: A decade of Clos topologies and centralized control in Google’s datacenter network,” in Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, ser. SIGCOMM ’15, New York, NY, USA: Association for Computing Machinery, Aug. 17, 2015, pp. 183–197.
29. A. Kumar, S. Jain, U. Naik, et al., “BwE: Flexible, hierarchical bandwidth allocation for WAN distributed computing,” in Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication, ser. SIGCOMM ’15, New York, NY, USA: Association for Computing Machinery, Aug. 17, 2015, pp. 1–14.
30. A. Vahdat, M. Al-Fares, N. Farrington, et al., “Scale-out networking in the data center,” IEEE Micro, vol. 30, no. 4, pp. 29–41, 2010.
31. D. Abts and B. Felderman, “A guided tour of data-center networking,” Commun. ACM, vol. 55, no. 6, pp. 44–51, Jun. 2012.
32. D. Abts and J. Kim, High performance datacenter networks: Architectures, algorithms, and opportunities. Springer Nature, 2022.
33. D. Firestone, A. Putnam, S. Mundkur, et al., “Azure accelerated networking: SmartNICs in the public cloud,” in 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18), 2018, pp. 51–66.
34. A. Saeed, N. Dukkipati, V. Valancius, et al., “Carousel: Scalable traffic shaping at end hosts,” in Proceedings of the Conference of the ACM Special Interest Group on Data Communication, ser. SIGCOMM ’17, New York, NY, USA: Association for Computing Machinery, Aug. 7, 2017, pp. 404–417.
35. M. Dalton, D. Schultz, J. Adriaens, et al., “Andromeda: Performance, isolation, and velocity at scale in cloud network virtualization,” in 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18), 2018, pp. 373–387.
36. R. Niranjan Mysore, A. Pamboris, N. Farrington, et al., “Portland: A scalable fault-tolerant layer 2 data center network fabric,” in Proceedings of the ACM SIGCOMM 2009 Conference on Data Communication, ser. SIGCOMM ’09, New York, NY, USA: Association for Computing Machinery, Aug. 16, 2009, pp. 39–50.
37. Nyquist Capital, Google’s secret 10GbE switch, Blog Post: https://nyquistcapital.com/2007/11/16/googles-secret-10gbe-switch/, Nov. 2007.
38. A. Singh, J. Ong, A. Agarwal, et al., “Jupiter Rising: a decade of Clos topologies and centralized control in Google’s datacenter network,” 4, vol. 45, New York, NY, USA: Association for Computing Machinery, Aug. 17, 2015, pp. 183–197.
39. Ultra Ethernet Consortium, Ultra Ethernet Consortium, https://ultraethernet.org/.
40. D. De Sensi, S. Di Girolamo, K. H. McMahon, D. Roweth, and T. Hoefler, “An in-depth analysis of the Slingshot interconnect,” inProceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, ser. SC ’20, IEEE Press, Nov. 9, 2020, pp. 1–14.
41. TOP500.org, TOP500 List - June 2024, https://top500.org/lists/top500/2024/06/, Jun. 2024.
42. P. Kale, A. Tumma, H. Kshirsagar, P. Ramrakhyani, and T. Vinode, “Fibre Channel over Ethernet: A beginners perspective,” in 2011 International Conference on Recent Trends in Information Technology (ICRTIT), 2011, pp. 438–443.
43. J. Satran, K. Meth, C. Sapuntzakis, M. Chadalapaka, and E. Zeidner, “Internet small computer systems interface (iSCSI),” Tech. Rep., 2004.
44. NVM Express, NVM Express Base Specification, https://nvmexpress.org/specification/nvm-express-basespecification/.
45. L. Shalev, H. Ayoub, N. Bshara, and E. Sabbag, “A cloud-optimized transport protocol for elastic and scalable HPC,” IEEE Micro, vol. 40, no. 6, pp. 67–73, 2020.
46. Google Cloud, Introduction to Cloud TPU v5p, Cloud TPU Documentation: https://cloud.google.com/tpu/docs/v5p.
47. Submarine Cable Networks, Complete List of Google’s Subsea Cable Investments, https://www.submarinenetworks.com/en/nv/insights/complete-list-of-google-s-subsea-cable-investments.
48. U. Hölzle, B4 and Google’s network, Presentation: https://www.segment-routing.net/images/hoelzle-tue-openflow.pdf.
49. S. Jain, A. Kumar, S. Mandal, et al., “B4: Experience with a globally-deployed software defined WAN,” in Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM, ser. SIGCOMM ’13, New York, NY, USA: Association for Computing Machinery, Aug. 27, 2013, pp. 3–14.
50. K.-K. Yap, M. Motiwala, J. Rahe, et al., “Taking the edge off with Espresso: scale, reliability and programmability for global internet peering,” in Proceedings of the Conference of the ACM Special Interest Group on Data Communication, ser. SIGCOMM ’17, New York, NY, USA: Association for Computing Machinery, Aug. 7, 2017, pp. 432–445.
51. C.-Y. Hong, S. Mandal, M. Al-Fares, et al., “B4 and after: Managing hierarchy, partitioning, and asymmetry for availability and scale in Google’s software-defined WAN,” in Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, ser. SIGCOMM ’18, New York, NY, USA: Association for Computing Machinery, Aug. 7, 2018, pp. 74–87.
52. D. Wetherall, A. Kabbani, V. Jacobson, et al., “Improving network availability with Protective ReRoute,” in Proceedings of the ACM SIGCOMM 2023 Conference, ser. ACM SIGCOMM ’23, New York, NY, USA: Association for Computing Machinery, Sep. 1, 2023, pp. 684–695.
53. A. Krentsel, N. Saran, B. Koley, et al., “A decentralized SDN architecture for the WAN,” in Proceedings of the ACM SIGCOMM 2024 Conference, ser. ACM SIGCOMM ’24, New York, NY, USA: Association for Computing Machinery, Aug. 4, 2024, pp. 938–953.
54. J. Gray, Tape is dead. disk is tape. flash is disk. ram locality is king, https://research.microsoft.com/en-us/um/people/gray/talks/flash_is_good.ppt.
55. E. Brewer, L. Ying, L. Greenfield, R. Cypher, and T. T’so, Disks for data centers, https://research.google/pubs/disks-for-data-centers/, 2016.
56. G. Ananthanarayanan, A. Ghodsi, S. Shenker, and I. Stoica, “{Disk-locality} in datacenter computing considered irrelevant,” in 13th Workshop on Hot Topics in Operating Systems (HotOS XIII), 2011.
57. L. Barroso, M. Marty, D. Patterson, and P. Ranganathan, “Attack of the killer microseconds,” Commun. ACM, vol. 60, no. 4, pp. 48–54, Mar. 2017.
58. M. Marty, M. de Kruijf, J. Adriaens, et al., “Snap: A microkernel approach to host networking,” in Proceedings of the 27th ACM Symposium on Operating Systems Principles, ser. SOSP ’19, New York, NY, USA: Association for Computing Machinery, Oct. 27, 2019, pp. 399–413.
59. G. Kumar, N. Dukkipati, K. Jang, et al., “Swift: Delay is simple and effective for congestion control in the datacenter,” in Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication, ser. SIGCOMM ’20, New York, NY, USA: Association for Computing Machinery, Jul. 30, 2020, pp. 514–528.