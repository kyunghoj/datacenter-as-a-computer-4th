# 5. WSC 하드웨어: 데이터 센터 인프라

## 5.1 데이터 센터 인프라 기초

### 5.1.1 지구에서 캠퍼스로, 그리고 건물로

인터넷과 클라우드 서비스는 전 세계 여러 데이터 센터 건물에 워크로드가 분산된, 지구 규모의 컴퓨터(planet-scale computer) 위에서 실행됩니다. 이러한 데이터 센터는 컴퓨팅, 스토리지, 네트워킹 인프라를 수용하도록 설계되었습니다. 건물의 주된 기능은 그곳에 있는 장비와 인력에게 필요한 유틸리티, 즉 전력, 냉각, 보호(shelter), 보안을 제공하는 것입니다. 고전 물리학의 정의에 따르면 데이터 센터에서 생산되는 '일(work)'은 거의 없습니다. 일부 방출되는 광자(photon)를 제외하면 소비되는 모든 에너지는 열로 변환됩니다. 입력 에너지를 전달하고 그 결과로 발생하는 폐열(waste heat)을 제거하는 것이 데이터 센터 설계의 핵심이며, 비(非) 컴퓨팅 비용의 대부분을 차지합니다. 데이터 센터 건설 비용은 대략 전달되는 전력량에 비례하며, 크기, 위치, 설계에 따라 상당히 다르지만 일반적으로 와트(Watt)당 5~20달러 범위입니다(8장 참조).

### 5.1.2 데이터 센터의 기본 구조

데이터 센터의 크기는 매우 다양하며, 일반적으로 IT 장비에 지속적으로 공급할 수 있는 총 전력인 **임계 전력(critical power)**으로 설명됩니다. 전통적인 기업용(enterprise) 서버는 5,000평방피트(450제곱미터)보다 작고 임계 전력이 1MW 미만인 데이터 센터에 수용되었습니다 [1], [2]. 여러 회사의 서버를 호스팅하기 위해 지어진 상용 데이터 센터(주로 코로케이션 데이터 센터 또는 "Colos"라고 함)는 더 크며 수십 메가와트의 임계 부하를 지원할 수 있습니다. 대형 클라우드 제공업체의 데이터 센터도 비슷하지만, 코로케이션 센터보다 더 큰 경우가 많습니다. 때로는 여러 데이터 센터가 단일 캠퍼스에 그룹화되기도 합니다. 대부분의 데이터 센터는 단층이지만, 일부는 다층 구조이기도 합니다(**그림 5.1**). WSC(Warehouse-Scale Computer) 데이터 센터 캠퍼스의 임계 전력은 종종 100MW를 초과합니다.

**그림 5.1 다양한 Google 데이터 센터**
*(더블린(아일랜드), 메이스 카운티(오클라호마), 임스하벤(네덜란드), 르누아르(노스캐롤라이나), 생기슬랭(벨기에), 하미나(핀란드)의 전경)*

데이터 센터 건물은 여러 구성 요소로 이루어져 있습니다. 기계 야드(mechanical yard) 또는 중앙 유틸리티 건물은 냉각탑(cooling tower) 및 칠러(chiller, 냉동기)와 같은 냉각 시스템을 호스팅합니다. 전기 야드(electrical yard)는 발전기 및 배전 센터와 같은 전기 장비를 수용합니다. 데이터 센터 내부의 주 서버 홀(main server hall)은 핫 아일(hot aisle, 열복도)과 콜드 아일(cold aisle, 냉복도)로 구성된 컴퓨팅, 스토리지, 네트워킹 장비를 호스팅합니다(일부 데이터 센터에는 열기 차폐 구조가 있음). 서버 층에는 운영 엔지니어를 위한 수리 구역도 있을 수 있습니다. 대부분의 데이터 센터에는 클러스터 간, 캠퍼스 수준, 장거리 연결을 포함한 네트워킹을 위해 지정된 별도 구역과 별도의 시설 관리 네트워크가 있습니다. 데이터 센터 가용성에서 네트워킹이 차지하는 중요성을 고려할 때, 네트워킹 구역은 신뢰성을 높이기 위해 추가적인 물리적 보안 및 고가용성 기능을 갖추고 있는 경우가 많습니다.

데이터 센터 건물 건설은 내화 및 불연성 구조, 안전 등에 관한 확립된 규정[3]을 따르며, 설계에는 일반적으로 원형 잠금장치(circle lock), 금속 탐지기, 경비 인력, 광범위한 카메라 네트워크를 포함한 정교한 출입 보안이 통합됩니다. YouTube의 비디오 투어[4]는 실제 시설 보안 설계를 잘 보여줍니다.

**그림 5.2**는 아이오와주 카운실 블러프스에 있는 Google 데이터 센터 캠퍼스의 조감도를 보여줍니다. **그림 5.3**은 단일 건물을 확대하여 일반적인 구성 요소 중 일부를 더 자세히 보여줍니다.

**그림 5.2 Google 데이터 센터 캠퍼스 조감도**
*(건물 1, 건물 2, 건물 3, 물 탱크, 중앙 유틸리티 건물, 변전소 1, 변전소 2 등이 표시됨)*

**그림 5.3 Google 데이터 센터 건물**
*(기계 야드-칠러, 냉각탑 등, 주 서버 홀-머신 열, 네트워크, 운영 구역 등, 전기 야드-변압기, 발전기 등이 표시됨)*

**그림 5.4**는 일반적인 데이터 센터 아키텍처의 구성 요소를 보여줍니다. IT 장비(6장에서 논의됨) 외에 데이터 센터의 두 가지 주요 시스템은 전력 공급(빨간색 표시)과 냉각(녹색 표시)을 제공합니다. 이에 대해서는 5.2절에서 자세히 논의하겠습니다.

**그림 5.4 일반적인 데이터 센터의 주요 구성 요소**
*(유틸리티 전원, 백업 발전/배전, 변압기, 전력 버스, 네트워킹 룸, 팬 코일 공조 장치, 칠러, 냉각탑, 물 저장소, 랙, 서버 등이 표시됨)*

### 5.1.3 데이터 센터 분류: 티어(Tiers)

데이터 센터 설계는 종종 4단계 티어 시스템을 사용하여 분류됩니다[5]. 데이터 센터를 전문으로 하는 전문 서비스 조직인 업타임 인스티튜트(Uptime Institute)는 데이터 센터의 배전, 무정전 전원 장치(UPS), 냉각 공급 및 이중화를 기반으로 하는 4단계 분류를 옹호합니다[6].

*   **Tier I** 데이터 센터는 이중화 구성 요소 없이 배전, 무정전 전원 장치(UPS), 냉각 분배를 위한 단일 경로만 있습니다.
*   **Tier II** 데이터 센터는 이 설계에 이중화 구성 요소(N + 1)를 추가하여 가용성을 향상시킵니다.
*   **Tier III** 데이터 센터는 유틸리티를 위한 하나의 활성 경로와 하나의 대체 분배 경로를 갖습니다. 각 경로에는 이중화 구성 요소가 있으며 동시에 유지 보수가 가능합니다(concurrently maintainable). 이들은 함께 계획된 유지 보수 시 다운타임이 발생하지 않도록 이중화를 제공합니다.
*   **Tier IV** 데이터 센터는 동시에 활성화된 두 개의 전력 및 냉각 분배 경로와 각 경로의 이중화 구성 요소를 갖추고 있으며, 단일 장비 고장이 부하에 영향을 미치지 않도록 해야 합니다.

업타임 인스티튜트의 사양은 높은 수준에서의 데이터 센터 성능에 중점을 둡니다. 이 사양은 요구 사항을 충족하기 위한 특정 구성 요소 목록을 규정하기보다는 토폴로지(위상)를 암시합니다. 주목할 만한 예외는 백업 디젤 연료 및 물 저장량, ASHRAE 온도 설계 포인트[7]입니다. 업타임 표준 내에는 주어진 티어 등급을 달성할 수 있는 많은 아키텍처가 있습니다. 대조적으로, TIA-942 표준은 더 규범적이며 건물 구조, 천장 높이, 전압 레벨, 랙 유형, 패치 코드 라벨링과 같은 다양한 구현 세부 사항을 지정합니다.

공식적으로 티어 등급을 획득하는 것은 노동 집약적이며 인증 기관 중 한 곳의 전체 검토가 필요합니다. 이러한 이유로 대부분의 데이터 센터는 공식적인 등급을 받지 않습니다. 대부분의 상용 데이터 센터는 건설 비용과 신뢰성 사이의 균형을 선택하여 Tier III와 IV 사이 어딘가에 해당합니다. 일반적으로 가장 낮은 개별 하위 시스템 등급(냉각, 전력 등)이 데이터 센터의 전체 티어 등급을 결정합니다.

실제 데이터 센터의 신뢰성은 설계뿐만 아니라 데이터 센터를 운영하는 조직의 품질에 크게 영향을 받습니다. 업계에서 사용되는 이론적 가용성 추정치는 Tier II 데이터 센터의 경우 99.7%에서 Tier III 및 IV의 경우 각각 99.98% 및 99.995%까지 다양합니다[6]. 그러나 실제 신뢰성은 이러한 계산에 포함되지 않은 요인에 의해 좌우되는 경우가 많습니다. 예를 들어, 업타임 인스티튜트는 데이터 센터 중단의 70% 이상이 인력 배치, 유지 보수 및 교육에 대한 경영진의 결정을 포함한 인적 오류의 결과라고 보고합니다[6]. 또한 소프트웨어의 지속적인 통합 및 배포를 사용하는 환경에서는 소프트웨어로 인한 중단이 건물 중단보다 더 많이 발생합니다. 애플리케이션별 고려 사항도 있습니다. 예를 들어, 3장에서 하이퍼스케일 워크로드가 신뢰성 및 가동 시간(uptime)에 대한 트레이드오프 측면에서 신흥 클라우드 및 머신 러닝 워크로드와 어떻게 다른지 논의했던 것을 상기해 보십시오.

5.4.2절에서는 WSC 장비를 수용할 수 있는 인프라의 준비 수준에 따라 "엔벨로프(envelopes)"라는 맥락에서 데이터 센터 건물을 생각할 수 있는 또 다른 관점을 논의합니다.

### 5.1.4 클라우드 및 AI가 데이터 센터 설계에 미치는 영향

전통적으로 WSC 데이터 센터 설계의 핵심 철학은 규모의 경제를 수용하고 하이퍼스케일 워크로드 및 시스템에 맞춤화된 설계를 목표로 했습니다. 그러나 3장에서 논의했듯이 오늘날의 WSC는 클라우드 및 AI/ML 워크로드를 통해 훨씬 더 많은 다양성을 보여줍니다.

특히 ML 워크로드는 전력 밀도를 크게 증가시켜 데이터 센터 설계에 흥미로운 새로운 과제를 제기했습니다. 예를 들어, 아래의 **그림 5.5**는 두 가지 전력 밀도 분포 곡선을 보여줍니다. 왼쪽 곡선은 기존의 컴퓨팅 및 스토리지 시스템의 혼합을 보여주는 반면, 오른쪽 곡선은 머신 러닝 랙 도입의 영향을 보여줍니다. 곡선의 오른쪽 이동은 가속기 시스템의 전력 밀도 증가를 나타냅니다. 기존 서버의 랙당 10-30kW에 비해 가속기 시스템은 랙당 50-200kW 범위일 수 있으며, 미래 세대에는 더 높아지는 추세입니다. 저밀도 기존 서버와 신흥 가속기를 위한 고전력 밀도의 양봉형(bimodality) 분포는 이 더 넓은 범위를 지원하기 위한 새로운 설계 트레이드오프를 유발합니다.

ML 워크로드는 또한 클라우드 및 하이퍼스케일 워크로드에 비해 가용성 및 가동 시간 요구 사항이 다릅니다. 3장에서 논의한 바와 같이, 이러한 워크로드는 대규모 동기식(bulk-synchronous)이며 처리량 지향적이고, 체크포인트-재시작 메커니즘과 산발적인 인프라 다운타임을 더 잘 견딜 수 있는 고객 SLO를 가지고 있습니다. 결과적으로 Tier-3 이상의 데이터 센터 요구 사항이 필요한 클라우드의 기업용 워크로드와 비교할 때, 일부 ML 워크로드(특히 훈련 워크로드)는 잠재적으로 Tier-1 수준의 데이터 센터를 사용할 수 있습니다. 앞서 논의했듯이 이러한 티어 간의 비용 범위는 상당히 크므로 데이터 센터 설계에서 새로운 트레이드오프가 발생합니다.

**그림 5.5 가속기 포함 및 미포함 시 전력 밀도 분포**
*(파란색: 기존 컴퓨팅 장비, 빨간색: 기존 및 ML 컴퓨팅 장비)*

클라우드 데이터 센터 배포는 다른 고려 사항을 가져옵니다. 고객 요구 사항의 광범위한 다양성에 맞춰 더 많은 지리적 다양성을 도입하고, 지역(regional) 및 구역(zonal) 격리에 대해 더 엄격한 요구 사항을 부과합니다. 또한 하이퍼스케일 및 ML 워크로드가 선호하는 대규모 배포와 비교할 때, 클라우드 배포는 데이터 센터 용량이 더 작은 단위로 점진적으로 배포되는 새로운 위치에서의 성장을 위해 "사용한 만큼 지불(pay as you go)" 모델을 선호하며, 따라서 규모에 따른 순수 비용 효율성보다는 배포 속도에 중점을 둡니다.

아래에서는 데이터 센터의 핵심 요소인 전력 하위 시스템과 냉각 하위 시스템을 심층적으로 살펴보면서 이러한 고려 사항이 WSC 데이터 센터의 설계 트레이드오프를 어떻게 변화시키는지 논의할 것입니다.

---

## 5.2 데이터 센터 전력 시스템

### 5.2.1 전체 전력 공급 아키텍처

전력은 먼저 유틸리티 변전소(utility substation)에 들어와 고전압(일반적으로 110kV 이상)을 중전압(일반적으로 50kV 미만)으로 변환합니다. 중전압은 1차 스위치기어(switchgear)와 중-저전압 변압기(일반적으로 1000V 미만)를 포함하는 1차 배전 센터(유닛 변전소라고도 함)로의 사이트 수준 배전에 사용됩니다. 여기에서 전력은 저전압 라인을 통해 건물로 들어와 무정전 전원 장치(UPS) 시스템으로 이동합니다. UPS 스위치기어는 유틸리티 전원이 실패할 때 작동하는 디젤 발전기 세트로부터 동일한 전압의 두 번째 공급도 받습니다. UPS의 출력은 데이터 센터 플로어의 전원 분배 장치(PDU)로 라우팅됩니다. PDU는 변환 및 배전 아키텍처의 마지막 계층이며 개별 회로를 오버헤드 버스바(busbar) 또는 개별 랙으로 라우팅합니다.

대형 데이터 센터를 위한 기존의 AC 전력 배전 방식이 **그림 5.6**에 나와 있습니다. 이 토폴로지는 유틸리티 공급 손실 시 이중화를 제공하는 한 쌍의 중전압 버스에서 전체 데이터 센터 플로어로 전력이 퍼져 나가기 때문에 "방사형(radial)"이라고 합니다. 저전압(400-480V) AC 전력은 유틸리티 전원용 강압 변압기 또는 백업 발전기에 의해 공급되는 많은 PDU에 의해 데이터 센터 플로어에 공급됩니다.

각 PDU에는 "G"로 표시된 자체 백업 발전기가 있습니다. 저전압 장비의 고장으로 인한 전력 손실은 격리된 예비(redundant) PDU의 존재로 인해 크게 완화됩니다. 이 예비 PDU는 어떤 PDU가 고장 나거나 일시적으로 서비스에서 제외되어야 할 때 그 자리를 대신할 수 있습니다. 자동 절체 스위치(ATS)는 주 전원이 실패할 때 자동으로 주 PDU에서 백업 전원으로 전력을 전환합니다. 이 $N + 1$ 설계는 ATS 스위치로 구현하기 간단합니다. $N + M$ 설계는 다음 예에서 볼 수 있듯이 더 복잡합니다.

**그림 5.6 방사형(Radial) 전력 아키텍처**
*(유틸리티 소스 A/B -> 중전압 배전 A/B -> 변압기/발전기 -> 저전압 PDU -> ATS -> 데이터 센터 플로어)*

데이터 센터 전력 배전을 위한 대안적 아키텍처는 Google의 중전압 전력 평면(**그림 5.7**)으로, 전체 데이터 센터만큼 큰 도메인에 걸쳐 전력을 공유할 수 있습니다 [8]. 건물 수준에서의 고가용성은 이중화된 유틸리티 AC 입력에 의해 제공됩니다. 건물 수준 변압기는 전압을 11-15kV의 중전압으로 낮추어 건물의 전기실 전체로 추가 분배합니다. 백업 전력을 위해 $N + M$개의 중전압 발전기가 버스에 병렬로 연결되며, 차단기와 스위치로 구성된 자동화 시스템이 유틸리티와 발전기 소스 사이를 선택합니다. 유틸리티 및 발전기 소스 모두에서 많은 유닛 변전소로 가는 중복 경로가 존재합니다. 각 유닛 변전소는 데이터 센터 플로어의 랙 열(row)에 분배하기 위해 전압을 저전압 AC로 낮춥니다.

중전압 전력 평면 아키텍처는 기존의 방사형 아키텍처에 비해 몇 가지 이점을 제공합니다. 첫째, 다양한 워크로드의 큰 풀(pool)은 전력 오버서브스크립션(oversubscription)[9](9장에서 자세히 논의됨)의 기회를 증가시킬 수 있습니다. Google의 전력 평면 아키텍처를 사용한 이러한 전력 오버서브스크립션은 데이터 센터의 임계 전력 용량을 초과하여 배포할 수 있는 IT 장비의 양을 크게 증가시킵니다. 둘째, 병렬 발전기 팜(farm)은 최소한의 중복 장비로 발전기 고장에 대한 복원력을 제공하지만, 적절하게 테스트하고 유지 관리하지 않을 경우 새로운 고장 모드를 도입할 수 있는 더 복잡한 제어 시스템을 비용으로 지불해야 합니다.

마지막으로, 전력은 전체 데이터 센터 플로어에 걸쳐 더 대체 가능(fungible)합니다. 중전압 및 저전압 배전 구성 요소의 적절한 크기 조정을 통해, 전력이 낭비(stranding)되지 않고 높은 동적 범위의 배포 전력 밀도를 지원할 수 있습니다. 랙 전력은 랙 내의 IT 장비 유형에 따라 상당히 다르기 때문에 이러한 유연성은 중요한 이점입니다. 예를 들어, 스토리지 집약적인 랙은 컴퓨팅 집약적인 랙보다 훨씬 적은 전력을 소비합니다. 기존의 방사형 전력 아키텍처에서는 데이터 센터 플로어 한 영역의 낮은 전력 밀도가 인프라의 영구적인 미사용으로 이어질 수 있습니다. 중전압 전력 평면은 플로어 전체의 전력 공유를 가능하게 합니다. 한 영역의 고전력 랙이 다른 영역의 저전력 랙을 보상하여 건물의 전력 용량을 완전히 활용할 수 있도록 합니다.

**그림 5.7 중전압 전력 평면 아키텍처 개념**
*(유틸리티 소스 A/B -> 소스 선택 -> 중전압 배전 -> 유닛 변전소 -> 저전압 배전/데이터 센터 플로어, 중앙에 발전기 팜)*

업스트림 전원이 제공하는 가용성이 애플리케이션(예: ML 훈련)에 충분하다면, 행(row) 전력을 비이중화(non-redundantly) 방식으로 사용하여 사용 가능한 전력량을 효과적으로 두 배로 늘릴 수 있습니다. 따라서 백업에 사용되는 인프라의 양은 2N 구성으로 사용될 경우 최대 50%가 될 수 있으며, 전력 최적화 구성으로 사용될 경우 0%까지 낮아질 수 있습니다.

또 다른 인기 있는 구성은 코로케이션 환경에서 자주 사용되는 분산형 이중화(distributed redundant) 방식입니다. 이는 저전압 또는 중전압 인프라와 함께 사용될 수 있습니다.

유닛 변전소와 열(row) 수준 버스웨이는 **그림 5.8**과 같이 인터리브(interleaved) 방식으로 배치됩니다. 이들은 $(N + 1)$ 접근 방식으로 $(2N)$ 시스템의 기능을 제공합니다. 표시된 예는 4개의 인프라 유닛을 사용하여 3개의 용량 유닛을 생성하며, 일반적으로 "4/3 DR"로 약칭됩니다. 각 (이중) 랙 열은 4개의 전원 공급 장치 모두에 의해 공급되며, 각 랙에는 두 개의 중복 전원 인렛이 있습니다. 전원 공급 장치 중 하나가 고장 나면(유닛 변압기 또는 그 아래의 버스바 차단기에서), 랙의 25%가 대체 전원으로 전환됩니다. 따라서 전원 공급 장치가 정상적으로 부하의 75% 이하로 실행되는 한 부하 중단은 발생하지 않습니다. 이 방식은 듀얼 공급(dual fed) 랙 또는 듀얼 공급 서버 비용을 지불하고 $(2N)$ 시설을 요구하지 않으면서 $(2N)$ 이중화를 제공합니다.

**그림 5.8 분산형 이중화(DR) 아키텍처 개념 (HAC = 열복도 차폐)**

### 5.2.2 AC 및 DC 배전 아키텍처 비교

유틸리티 그리드의 고전압 DC(HVDC)는 호환되지 않는 전력망 연결, 연쇄 고장에 대한 저항력 제공, 대규모 전력 흐름의 능동적 제어, 장거리 전력 전송 효율성 측면에서 장점을 제공합니다. 데이터 센터에서 DC 배전의 사례는 효율성, 구성 요소 수 감소로 인한 신뢰성 증가, 공간 감소, 기본 DC 출력을 가진 분산형 발전기와의 통합 용이성을 중심으로 합니다. 위에서 언급한 이중 변환 UPS와 비교할 때 DC 시스템은 UPS의 마지막 인버터 단계를 제거합니다. 전압이 서버 전원 공급 장치(PSU)의 DC 1차 단계와 일치하도록 선택되면 PDU 변환, PSU 정류, PSU 역률 보정의 세 가지 추가 단계가 제거됩니다.

**그림 5.9**와 **그림 5.10**은 데이터 센터 업계에서 일반적으로 사용되는 AC 및 DC 배전 아키텍처를 비교합니다. "파워 트레인"의 각 단계에 대해 최신 상용 가용 효율성([10] 기준)이 표시되어 있습니다. 최신 구성 요소를 사용하는 전체 파워트레인 효율성은 AC 배전에 비해 DC 배전이 몇 퍼센트 더 높게 유지됩니다. 이 차이는 구형 구성 요소를 사용하는 데이터 센터에서 더 두드러졌습니다[11]. 표시된 AC 아키텍처는 북미에서 흔히 볼 수 있는 전압 체계에 해당합니다. 전 세계 대부분의 다른 지역에서는 AC PDU의 추가 전압 변환을 피할 수 있어 PDU 효율성이 약간 더 높습니다.

데이터 센터용 상용 DC 장비는 사용할 수 있지만 비용은 여전히 동급 AC 장비보다 높습니다. 마찬가지로 대규모 데이터 센터를 건설하는 데는 수백 명, 때로는 수천 명의 숙련된 근로자가 참여합니다. 이들 중 일부만이 전기 기술자일 것이지만, DC 기술자의 제한된 가용성은 건설, 서비스 및 운영 비용 증가로 이어질 수 있습니다. 그러나 DC 배전은 태양광, 연료 전지, 풍력 터빈과 같은 분산형 발전기를 통합할 때 더 매력적입니다. 이러한 전원은 일반적으로 기본 DC를 생산하며 DC 배전 아키텍처에 쉽게 통합됩니다.

구리 인터커넥트로 연결된 고전력 가속기와 같은 새로운 고밀도 컴퓨팅 솔루션은 전류량과 열 문제를 증가시켜 버스바에 더 많은 압력을 가합니다.

**그림 5.9 일반적인 AC 배전 아키텍처**
*(효율성: AC/DC 97% -> PDU 97% -> PSU 94% -> VR 96% = 전체 85%)*

**그림 5.10 DC 배전 아키텍처**
*(효율성: AC/DC 99% -> PDU 98% -> PSU 96% -> VR 96% = 전체 89%)*

대부분의 새로운 랙 설계가 이미 DC 전력을 소비하고 있기 때문에, 미래의 산업 표준은 DC 버스바 아키텍처와 더 높은 전압을 채택할 수 있습니다.

### 5.2.3 무정전 전원 장치(UPS)

UPS는 일반적으로 세 가지 기능을 하나의 시스템에 결합합니다.

*   첫째, 활성 전원 입력(유틸리티 전원 또는 발전기 전원)을 선택하는 절체 스위치(transfer switch)가 포함되어 있습니다. 정전 후 절체 스위치는 발전기가 시동되어 전력을 공급할 준비가 된 시점을 감지합니다. 일반적으로 발전기가 시동되어 전체 정격 부하를 감당하는 데 10~15초가 걸립니다.
*   둘째, 유틸리티 고장과 발전기 전력 가용성 사이의 시간을 메우기 위해 어떤 형태의 에너지 저장 장치(전기적, 화학적 또는 기계적)를 포함합니다.
*   셋째, 들어오는 전원 공급을 조절하여 전압 스파이크나 처짐(sags), 또는 AC 공급의 고조파 왜곡을 제거합니다. 이 조절은 "이중 변환"을 통해 수행될 수 있습니다.

전통적인 UPS는 AC-DC-AC 이중 변환을 사용합니다. 입력 AC는 DC로 정류되어 배터리 스트링에 연결된 UPS 내부 버스에 공급됩니다. DC 버스의 출력은 다시 AC로 인버팅되어 데이터 센터 PDU에 공급됩니다. 유틸리티 전원이 실패하면 입력 AC는 손실되지만 내부 DC는 (배터리에서) 남아 있어 데이터 센터로의 AC 출력이 중단 없이 계속됩니다. 결국 발전기가 시작되어 입력 AC 전원을 다시 공급합니다.

전통적인 이중 변환 아키텍처는 견고하지만 비효율적이어서 흐르는 전력의 몇 퍼센트를 열로 낭비했습니다. 라인 인터랙티브(line-interactive), 델타 변환(delta-conversion) 또는 멀티 모드 시스템과 같은 최신 설계는 광범위한 부하 경우에 대해 96-98% 범위의 효율로 작동합니다. 이러한 최신 설계는 속도와 빠른 전압 처짐 및 팽창(swells)에 대한 보호 측면에서 다양하므로 완전히 상호 교환할 수는 없습니다.

UPS 시스템은 상당한 공간을 차지하기 때문에 일반적으로 데이터 센터 플로어와 분리된 방에 보관됩니다. 일반적인 UPS 용량은 장비의 전력 요구 사항에 따라 수백 킬로와트에서 2메가와트 이상까지 다양합니다. 더 큰 용량은 여러 개의 작은 장치를 결합하여 달성됩니다.

Google은 중앙 UPS 룸 대신 각 서버 또는 서버 랙에 UPS 배터리를 배치하는 분산형 UPS 설계를 개척했습니다. 원래의 12V "플로팅(floating)" 배터리 아키텍처[12]는 서버의 AC/DC 전원 공급 장치의 출력(DC) 쪽에 배터리를 배치하여 아주 적은 충전 전류와 간단한 스위칭 회로만 필요로 했습니다. AC 전원이 실패하면 12V DC 배터리가 서버 마더보드에 전원을 공급하는 12V 라인의 전압을 유지하여 매우 간단하고 자연스러운 방식으로 보호 기능을 제공합니다. (통신 장비는 오랫동안 신뢰성을 위해 DC 전원 공급 장치를 사용해 왔으며 오늘날에도 사용하고 있습니다.) 나중에 4개의 서버 랙마다 배터리 랙을 분배하는 오픈 컴퓨트 UPS(OpenCompute UPS)[13]도 비슷한 전략을 채택했습니다. 이러한 설계는 업스트림 UPS를 제거하고 UPS 손실을 사실상 제거합니다(480V AC 입력이 있는 인라인 UPS의 ~1%–3%에 비해 ~0.1%).

오늘날의 증가된 전력 밀도로 인해 랙은 48V DC 전원 버스에서 작동합니다[14]. 랙 수준 정류기는 들어오는 AC 전력을 DC로 변환하고 48V 배터리 모듈은 동일한 버스에 직접 연결됩니다. 정류기 및 배터리 모듈은 각 랙 내용물의 전력 요구 사항에 맞게 확장할 수 있으며(스토리지 랙은 서버 랙보다 더 적은 모듈이 필요함), 공통 48V 버스에서의 자연스러운 병렬 연결은 원하는 경우 N+M 이중화를 허용합니다.

2015년경까지 데이터 센터 UPS는 12V 자동차 배터리와 유사한 납축전지를 사용했습니다. 이 배터리는 비교적 저렴하고 신뢰할 수 있지만 시간이 지남에 따라 성능이 저하되어 5년마다 교체해야 합니다. 최근에는 리튬 인산철(LFP)을 포함한 리튬 이온 배터리(Li-ion)가 더 높은 방전율과 더 긴 배터리 수명을 제공하기 때문에 더 보편화되었습니다. 그러나 손상된 리튬 배터리는 열 폭주 반응을 일으켜 화재가 발생할 수 있으므로 신중한 화재 예방 조치가 필요합니다. 예를 들어, 2020년 10월 한국 기업 카카오를 호스팅하는 데이터 센터가 리튬 배터리 화재로 파괴되었습니다. 그러나 엄격한 테스트 및 표준화를 포함하여 안전 문제를 신중하게 해결함으로써 WSC의 무중단 운영을 위해 리튬 이온 배터리의 이점을 크게 누릴 수 있습니다. 최근 Google은 전 세계 데이터 센터에 1억 개 이상의 리튬 이온 배터리 셀을 배포하는 중요한 이정표에 도달했습니다[15].

### 5.2.4 전원 분배 장치(PDU)

예시 데이터 센터에서 UPS 출력은 데이터 센터 플로어의 PDU로 라우팅됩니다. PDU는 주택의 차단기 패널과 비슷하지만 최종 전압 조정을 위한 변압기를 통합할 수도 있습니다. PDU는 더 큰 입력 공급을 받아 플로어의 랙에 전력을 분배하는 더 작은 회로로 나눕니다. 각 회로는 자체 차단기로 보호됩니다. 기존 PDU는 75–225kW의 부하를 처리하며 개별 회로는 최대 약 6kW(110–230V에서 20 또는 30A)를 처리합니다.

대형 데이터 센터에서 볼 수 있는 PDU는 백업 발전기 크기에 해당하는 2–3MW를 처리하며, 일반적으로 각 서버 열을 따라 오버헤드로 실행되는 소수의 버스바(busbar)를 공급합니다. 버스바에 부착된 탭오프(Tap-off) 장치는 개별 회로를 아래 랙으로 떨어뜨리고 회로 차단기를 포함하여 부분적으로 PDU의 역할을 수행합니다. 버스바는 케이블링 작업을 크게 줄여주기 때문에 인기가 있습니다. 각 랙은 이제 서버 열 끝에 있는 PDU에서 긴 케이블을 연결하는 대신 오버헤드 버스바에서 짧은 케이블만 필요로 합니다.

PDU는 종종 두 개의 독립적인("A" 및 "B") 전원을 수락하고 약간의 지연으로 둘 사이를 전환할 수 있어 추가적인 이중화를 제공합니다. 한 소스의 손실은 서버에 대한 전력을 중단하지 않습니다. 이 시나리오에서는 데이터 센터의 UPS 장치가 일반적으로 A 및 B 측면에 복제되므로 UPS 고장으로도 서버 전력이 중단되지 않습니다.

북미에서 PDU 입력은 일반적으로 480V 3상 전원입니다. 이를 위해서는 PDU가 서버에 원하는 110V 출력을 제공하기 위해 최종 변환 단계를 수행해야 하므로 또 다른 비효율성의 원인이 됩니다. EU에서는 PDU 입력이 일반적으로 400V 3상 전원입니다. 단일 위상과 중성선(neutral) 조합에서 전력을 가져오면 추가 변압기 단계 없이 원하는 230V를 제공할 수 있습니다. 북미에서 동일한 트릭을 사용하려면 컴퓨터 장비가 277V(PDU에 대한 480V 입력에서 파생됨)를 수용해야 하는데, 불행히도 이는 표준 전원 공급 장치의 상한 범위를 초과합니다.

실제 데이터 센터에는 여기에 설명된 단순화된 설계의 많은 변형이 포함되어 있습니다. 여기에는 발전기 또는 UPS 장치의 "병렬 연결(paralleling)"이 포함되는데, 이는 여러 장치가 공유 버스를 공급하여 고장 난 장치의 부하를 다른 장치가 픽업할 수 있는 배열로, RAID 시스템에서 디스크 고장을 처리하는 것과 유사합니다. 일반적인 병렬 구성에는 $N + 1$(한 번에 하나의 고장 또는 유지 보수 작업 허용), $N + 2$(유지 보수를 위해 한 장치가 오프라인인 경우에도 하나의 고장 허용, 또는 "동시 유지 보수 가능성"), $2N$(완전 이중화 쌍)이 포함됩니다.

### 5.2.5 전력 마이크로그리드 및 기타 미래 기회

거의 모든 건물과 마찬가지로 데이터 센터는 하나 이상의 유틸리티가 공급하는 전력을 소비하며 전력망에 연결되어 있습니다. 100년 전에는 산업 시설이 자체 민간 발전소를 갖는 것이 더 흔했지만 공유 전력망의 경제적 및 신뢰성 이점으로 인해 거의 모두 대체되었습니다. 최근에는 풍부한 분산 에너지 자원은 있지만 전력망이 약하고 신뢰할 수 없는 지역의 맥락에서 마이크로그리드 개념이 다시 부상했습니다. 일부 상황에서는 마이크로그리드가 데이터 센터를 지원할 수도 있습니다.

마이크로그리드(**그림 5.11**)는 신뢰성과 복원력, 비용 절감 및 지속 가능성 향상을 제공할 수 있습니다. 데이터 센터는 사이트가 그리드에서 분리되어 있는 동안 발전기가 백업 전력을 제공하는 그리드 장애 시 이미 마이크로그리드로 작동합니다. 또한 데이터 센터는 전력 관리(전력 모니터링 및 제한(capping) 포함)와 같은 주요 마이크로그리드 기능을 사용하므로 마이크로그리드에 통합하기가 더 쉽습니다. 예를 들어, 데이터 센터는 "미터기 뒤(behind the meter)"에 무탄소 에너지(CFE) 소스를 통합하여 에너지 비용을 줄이고 현장 백업 전력의 추가 소스를 확보하여 신뢰성을 높일 수 있습니다. 데이터 센터 마이크로그리드의 또 다른 용도는 배터리 저장 장치를 사용하여 피크 시간대의 그리드 소비를 피하는 것입니다.

마이크로그리드 기반 최적화는 백업 전력원을 포함하여 24/7 무탄소 에너지 모델로 전환하려는 광범위한 목표와 일치합니다. 데이터 센터 부하를 디스패치 가능한(dispatchable) 부하로 취급하면 유틸리티와 공급 및 수요를 협력적으로 관리할 수 있습니다. 예를 들어, 워크로드를 시간이나 위치별로 이동하면 워크로드의 탄소 발자국을 개선할 수 있습니다[16]. 여기에는 단순한 백업이 아닌 용량 증대 및 전력 변동 제어를 위해 대기 전력 자산을 사용하거나[17], 연료 전지와 같은 새로운 무탄소 백업 전원을 도입하는 것이 포함될 수 있습니다.

앞서 우리는 ML 워크로드가 WSC 데이터 센터 설계에 새로운 요구 사항을 도입하는 방법에 대해 논의했습니다. **그림 5.12**는 대규모 ML 훈련 워크로드의 전력 변동을 보여주는 구체적인 예를 보여줍니다. 대규모 작업이 시작될 때 수천 개의 TPU 또는 GPU가 동시에 유휴 상태에서 바쁜 상태로 이동하여 대부분 유휴 상태에서 대부분 전체 전력으로 전력 소비가 급격히 변화합니다(그림의 #1). 훈련 실행 중에 작업자 태스크는 각 훈련 단계 끝에서 전역적으로 동기화된 데이터 교환 중에 잠시 유휴 상태가 될 수 있습니다(#2). 중단(예: 랙 손실)이 발생하면 새로운 구성(#3)에 맞게 태스크와 데이터가 재분배되는 동안 훈련이 더 긴 기간(#4) 동안 중단될 수 있습니다.

이러한 시작-중지 주기는 적절하게 관리되지 않을 경우 그리드 중단을 유발할 수 있는 큰 전력 변동으로 이어질 수 있습니다. 현재 이러한 변동은 소프트웨어 관리 전력 소비(9장에서 자세히 논의됨)를 통해 관리됩니다. 마이크로그리드와 에너지 저장 장치는 이러한 변동으로부터 그리드를 차폐하기 위한 버퍼를 제공하는 데 도움이 될 수 있지만, 변동이 마이크로그리드에 비해 크지만 그리드에 비해 작기 때문에 문제를 악화시킬 수도 있습니다. 25MW와 50MW 사이에서 진동하는 부하는 50MW 마이크로그리드에는 심각한 문제를 일으키지만 10GW 지역 그리드에는 거의 문제가 되지 않습니다.

**그림 5.11 마이크로그리드 개요**
*(분산 에너지 자원(풍력, 태양광, 스토리지, CHP), 메인 그리드, 마이크로그리드 제어 시스템, 디젤 발전기, 데이터 센터 연결)*

**그림 5.12 ML 훈련 실행의 변동하는 전력 소비**
*(1: 전체 부하 변동, 2: 부분 변동, 3: 짧은 딥 지속 시간, 4: 재스케줄 지연)*

데이터 센터 전기 시스템의 다른 미래 기회로는 전력 공급 증가를 위한 고전압(600V) 시스템, 연료 전지 및 수소 기반 솔루션과 같은 현장 발전 기술, 더 진보된 센서 및 소프트웨어 정의 보호 채택이 포함됩니다. 또한 시간 동기화된 장애 기록 시스템은 향상된 관측 가능성(observability)과 제어 가능성을 약속합니다.

---

## 5.3 데이터 센터 냉각 시스템

데이터 센터는 포함된 서버에 전원을 공급하고 발생한 열을 제거합니다. 이전 섹션에서 전력을 전달하는 방법을 살펴보았으므로 이 섹션에서는 냉각 시스템의 설계를 살펴봅니다.

전력과 냉각 모두 추가적인 오버헤드를 유발합니다. 역사적으로 이러한 오버헤드는 상당했으며 거의 100%에 달했습니다. 즉, 이러한 오버헤드로 인해 데이터 센터는 서버 단독에 필요한 에너지의 두 배를 소비했습니다. 그러나 모범 사례를 적용하면 이 오버헤드는 10-20%로 줄어듭니다. 전력에 대해 논의할 때 고효율 배전 및 UPS와 같은 모범 사례 기술을 식별했습니다. 마찬가지로 냉각 하위 시스템에는 프리 쿨링(free cooling, 서버의 목표 흡입 온도를 높임으로써 더욱 강화됨) 및 잘 관리된 공기 흐름을 포함한 많은 최적화가 포함됩니다. 아래에서 이에 대해 더 자세히 논의하겠습니다.

### 5.3.1 전체 냉각/기계 아키텍처

데이터 센터 냉각 시스템은 장비에서 발생하는 열을 제거합니다. 열을 제거하기 위해 냉각 시스템은 열교환을 통해 따뜻해지는 차가운 매체를 순환시킨 다음 루프의 다른 곳에서 다시 냉각시키는 일련의 계층적 루프를 사용해야 합니다. 개방 루프(open loop)는 나가는 따뜻한 매체를 외부의 차가운 공급으로 교체하거나 보충하여 루프를 통과하는 각 사이클에서 (일부) 새로운 재료를 사용합니다. 대조적으로 폐쇄 루프(closed loop)는 별도의 매체를 재순환시켜 열교환기나 환경으로 열을 지속적으로 전달합니다. 모든 루프는 결국 외부 환경으로 열을 전달해야 하지만 내부 루프는 열을 외부 루프로 전달하여 간접적으로 그렇게 합니다.

폐쇄 루프 시스템은 다양한 형태로 제공되지만 가장 일반적인 것은 데이터 센터 플로어의 공기 회로입니다. 그 기능은 서버에서 열을 격리 및 제거하여 열교환기로 운반하는 것입니다. **그림 5.13**과 같이 찬 공기가 서버로 흘러가 열을 받고 결국 열교환기에 도달하여 서버를 통과하는 다음 주기를 위해 다시 식혀집니다.

많은 데이터 센터는 슬래브 바닥 위의 2~4피트 높이의 지지대 위에 놓인 강철 그리드에 설치된 콘크리트 타일인 이중 마루(raised floor)를 사용합니다. 바닥 아래 공간에는 종종 랙으로 가는 전원 케이블이 포함되지만 주된 목적은 서버 랙에 찬 공기를 분배하는 것입니다. 바닥 아래 플레넘(plenum), 랙, 그리고 다시 CRAC(1960년대 용어인 컴퓨터 룸 에어컨)으로 이어지는 공기 흐름은 1차 공기 회로를 정의합니다. 편리하고 유연하지만 이중 마루는 추가 비용을 발생시키므로 대부분의 하이퍼스케일 설계는 이를 피합니다.

가장 단순한 폐쇄 루프 시스템에는 두 개의 루프가 포함됩니다. 첫 번째 루프는 **그림 5.13**에 표시된 공기 회로이고, 두 번째 루프(CRAC 내부의 액체 공급)는 CRAC에서 외부 열교환기 또는 라디에이터(일반적으로 건물 지붕에 배치됨)로 직접 연결되어 열을 환경으로 방출합니다.

**그림 5.13 핫-콜드 아일 설정이 있는 이중 마루 데이터 센터**
*(액체 공급, CRAC 유닛, 바닥 타일, 랙, 천장 등으로 구성된 공기 흐름도)*

대규모 데이터 센터에서 일반적으로 사용되는 3루프 시스템이 **그림 5.14**에 나와 있습니다. 첫 번째 데이터 센터 플로어 루프는 데이터 센터 플로어의 IT 장비에 의해 가열되고 팬 코일에 의해 교대로 냉각되는 공기를 순환시키는 것과 관련됩니다. 이 첫 번째 공기 루프는 매우 짧게 유지됩니다. 일반적으로 랙은 시원한 물 코일이 있는 팬이 식혀서 랙의 전면으로 되돌려 보내는 인클로저("핫 아일")로 뒤쪽으로 열을 배출합니다. 공정수(process water) 루프에서 팬 코일의 따뜻한 물은 냉각 플랜트로 돌아와 냉각된 후 다시 팬 코일로 펌핑됩니다. 마지막으로 응축수(condenser water) 루프는 칠러 장치에 의한 기계적 냉동과 냉각탑에서의 증발을 결합하여 공정수에서 받은 열을 제거합니다. 응축수 루프는 칠러의 응축기 쪽에서 열을 제거하기 때문에 그렇게 불립니다. 열교환기는 루프 간의 열 전달의 대부분을 수행하면서 공정수가 응축수와 섞이는 것을 방지합니다. 날씨 조건이 양호하면 냉각탑의 응축수 증발 냉각만으로 전체 데이터 센터 열 부하를 제거할 수 있습니다. 그러면 칠러 증발기 및 칠러 응축기 열 전달 단계가 불필요해집니다.

각 토폴로지는 복잡성, 효율성 및 비용 측면에서 트레이드오프를 제시합니다. 예를 들어 직접 공기 냉각은 매우 효율적일 수 있지만 모든 기후에서 작동하지는 않으며, 공기 중 미립자 필터링이 필요하고, 지원되는 전력 밀도를 제한하며, 복잡한 제어 문제를 도입할 수 있습니다. 2루프 시스템은 구현하기 쉽고 건설 비용이 비교적 저렴하며 외부 오염으로부터 격리되지만 일반적으로 운영 효율성이 낮습니다. 3루프 시스템은 건설 비용이 가장 많이 들고 제어가 적당히 복잡하지만 오염 보호 기능을 제공하며 이코노마이저(economizer)를 사용할 때 효율성이 좋습니다.

**그림 5.14 3루프 데이터 센터 냉각 시스템**
*(IT 랙 -> 데이터 센터 플로어(공기) -> 팬 코일 열교환기 -> 공정수 루프 -> 냉각 플랜트 열교환기(칠러/응축기) -> 응축수 루프 -> 냉각탑)*

또한 냉각 없이는 데이터 센터가 몇 분 만에 과열될 수 있으므로 발전기(및 때로는 UPS 장치)는 대부분의 기계적 냉각 장비에 백업 전력을 제공해야 합니다. 그 이유를 알아보기 위해 25MW 부하를 지원하는 100 * 50 * 5미터($25,000 m^3$) 크기의 서버 홀이 있는 대형 데이터 센터를 고려해 보겠습니다. 공기의 비열은 1kJ/kg이고 무게는 $1.2kg/m^3$입니다. 25MW 부하는 1초에 25MJ을 주입하며, 이는 서버 홀 공기 $1m^3$당 1kJ이므로 초당 $0.8^\circ C$씩 가열됩니다. 냉각 플랜트가 고장 나면 공기 온도는 이론적으로 2분 안에 $100^\circ C$에 도달합니다!

실제로는 다른 재료가 공기 중의 열 일부를 흡수하기 때문에 상황이 덜 심각합니다. 예를 들어 수랭식 건물에서 냉각 파이프에는 지정된 출구 온도를 넘어 따뜻해져 많은 열을 흡수할 수 있는 상당한 양의 물이 포함되어 있어 추가적인 여유 시간을 제공합니다. 마찬가지로 랙과 건물 자체도 따뜻해지기 시작하여 주변 온도 상승을 지연시킵니다. 그럼에도 불구하고 대부분의 장비는 $40^\circ C$ 이상의 흡입 온도를 견딜 수 없으므로 냉각 실패에 반응할 시간은 거의 없습니다.

따라서 냉각이 실패하면 데이터 센터 운영자는 장비의 영구적인 손상을 방지하기 위해 냉각 실패 후 5~15분 이내에 IT 장비를 종료해야 합니다. 이러한 이벤트를 긴급 전원 차단(EPO)이라고 하며, 냉각이 빠르게 복원되더라도 모든 장비의 통제되지 않은 종료로 인해 일반적으로 저장된 모든 데이터를 검증하고 손실된 스토리지 서버를 복구하기 위한 긴 클러스터 재시작이 발생하므로 몇 시간의 다운타임을 의미합니다.

일반적인 데이터 센터에서 칠러와 펌프는 발전기가 지원하는 임계 부하의 40% 이상을 추가할 수 있어 전체 건설 비용을 크게 증가시킵니다. CRAC, 칠러, 냉각탑은 데이터 센터 냉각 시스템에서 가장 중요한 빌딩 블록 중 하나입니다. 각각에 대해 자세히 살펴보겠습니다.

### 5.3.2 항온항습기(CRAC)

1990년대까지 데이터 센터는 일반적으로 컴퓨터 룸 에어컨(CRAC)을 사용했습니다. CRAC는 열교환기, 송풍기 및 제어 장치를 포함하는 소비자용 에어컨과 유사합니다. CRAC 장치는 일반적으로 데이터 센터 플로어 주변에 위치하며 방이나 이중 마루 아래 공간으로 찬 공기를 불어넣습니다. 주로 사용하는 냉각 방식에 따라 직접 팽창(DX), 유체 용액 또는 물로 구분됩니다.

DX 장치는 CRAC 내부의 냉각(증발기) 코일과 데이터 센터 외부의 열 방출(응축기) 코일이 있는 분리형 에어컨입니다. 유체 용액 CRAC는 이 기본 아키텍처를 공유하지만 상변화 냉매 대신 물과 글리콜 혼합물을 코일을 통해 순환시킵니다. 마지막으로 수랭식 CRAC는 냉수 루프에 연결됩니다.

고전적인 설계에서 CRAC 장치는 바닥 아래 공간으로 찬 공기를 불어넣어 이중 마루 플레넘을 가압하고, 이 공기는 서버 랙 앞의 천공 타일(perforated tiles)을 통해 빠져나갑니다. 공기는 서버를 통과하여 "핫 아일"로 배출됩니다. 랙은 일반적으로 뜨거운 공기와 찬 공기가 섞여 발생하는 비효율성을 줄이기 위해 콜드 아일과 핫 아일이 번갈아 나타나는 긴 열(row)로 배치됩니다. 실제로 많은 최신 데이터 센터는 벽으로 콜드 아일 또는 핫 아일을 물리적으로 격리합니다[18]. **그림 5.13**과 같이 서버에서 생성된 뜨거운 공기는 CRAC의 흡입구로 재순환되어 냉각된 후 다시 이중 마루 플레넘으로 배출됩니다.

### 5.3.3 칠러(Chillers)

**그림 5.15**에 표시된 수랭식 칠러는 대형 수랭식 에어컨과 같습니다. 칠러는 압축기, 팽창 밸브 및 배관으로 구성된 상단 장착형 냉동 시스템을 통해 결합된 두 개의 크고 분리된 구획의 물속에 증발기 및 응축기 코일을 담급니다. 차가운 구획에서는 데이터 센터의 따뜻한 물이 공정 냉수 공급(PCWS) 루프로 돌아가기 전에 증발기 코일에 의해 냉각됩니다. 뜨거운 구획에서는 응축수 루프의 시원한 물이 응축기 코일에 의해 데워지고 열을 냉각탑으로 운반하여 증발 냉각을 통해 환경으로 방출합니다. 가변 주파수 드라이브(VFD)는 칠러 모터의 속도를 제어합니다. 칠러는 압축기를 사용하기 때문에 작업을 수행하는 데 상당한 양의 에너지가 소비되므로 냉각탑만 사용하는 것보다 효율성이 떨어집니다. 그러나 냉각탑이 모든 냉각 부하를 처리하기에는 너무 따뜻한 날에도 작동하기 때문에 칠러는 냉각탑 기반 설계를 보완하고 필요할 때만 실행하는 데 사용됩니다.

**그림 5.15 수랭식 원심 칠러**

### 5.3.4 프리 쿨링(Free cooling)

프리 쿨링은 차가운 외부 공기를 사용하여 냉수를 생산하거나 서버를 직접 냉각하는 것을 말합니다. 비용이 0이라는 의미에서 완전히 무료는 아니지만 칠러에 비해 에너지 비용이 매우 낮습니다.

가장 간단한 토폴로지는 신선한 공기 냉각(또는 공기 이코노마이저)입니다. 본질적으로 창문을 여는 것입니다. 이러한 시스템이 **그림 5.16**에 나와 있습니다. 이것은 기온이 높지 않은 지역에서 인기 있는 단일 개방 루프 시스템입니다. 대형 벽면 팬이 외부 공기를 콜드 아일로 직접 보내고 따뜻한 공기는 건물의 상단으로 나갑니다. 선택적으로 공기 이코노마이저는 흡입 경로에 물 스프레이를 포함하여 외부 건구 온도는 높지만 공기가 건조할 때 콜드 아일 온도를 낮춥니다. 냉각탑과 마찬가지로 물을 증발시키면 공기 온도가 습구 온도에 가까워집니다.

위에서 언급했듯이 공기 이코노마이저 데이터 센터는 외부 환경에 개방되어 있으며 냉각을 위해 낮은 건구 온도를 사용합니다. (건구 온도는 일반 온도계로 측정한 공기 온도입니다). 외부 온도가 한계 내에 있을 때(이 분야의 극단적인 실험에 대해서는 [19] 참조) 대형 팬이 외부 공기를 실내 또는 이중 마루 플레넘으로 직접 밀어 넣습니다. 공기가 서버를 통과하면 건물 밖으로 배출됩니다. 공기 이코노마이저 시스템은 매우 효율적일 수 있지만 오염을 제어하기 위한 효과적인 필터링이 필요하며, 보조 냉각이 필요할 수 있고(외부 조건이 좋지 않을 때), 제어하기 어려울 수 있습니다. 구체적으로 오작동이 발생하면 공기는 열을 비교적 적게 저장할 수 있기 때문에 온도가 매우 빠르게 상승합니다. 대조적으로 물 기반 시스템은 물 저장 탱크를 사용하여 상당한 열 버퍼를 제공할 수 있습니다.

수랭식 이코노마이저 데이터 센터는 습구 온도를 활용합니다. 습구 온도는 증발을 통해 도달할 수 있는 가장 낮은 수온입니다. 공기가 건조할수록 건구 온도와 습구 온도의 차이가 커집니다. 예를 들어 $25^\circ$, 습도 50%인 공기의 습구 온도는 $18^\circ$이지만 습도 90%에서는 $23.7^\circ$입니다. 따라서 수랭식 이코노마이저 데이터 센터는 일 년 중 더 많은 시간 동안 칠러 없이 운영할 수 있습니다. 일부 공기 이코노마이저 데이터 센터는 증발 냉각을 활용하기 위해 (데이터 센터에 들어가기 전에) 공기 흐름에 물을 분사하는 하이브리드 시스템을 사용합니다.

일반적인 수랭식 이코노마이저 데이터 센터는 평행 열교환기를 사용하여 습구 온도가 양호할 때 칠러를 끌 수 있습니다. 냉각탑의 용량(습구 온도가 감소함에 따라 증가함)에 따라 제어 시스템은 칠러와 냉각탑 사이의 물 흐름 균형을 맞춥니다.

또 다른 접근 방식은 냉각탑 대신 라디에이터를 사용하여 응축수 유체 또는 공정수를 팬 냉각 라디에이터를 통해 펌핑하는 것입니다. 글리콜/물 기반 CRAC와 유사하게 이러한 시스템은 동파를 방지하기 위해 글리콜 기반 루프를 사용합니다. 라디에이터는 추운 기후(예: 시카고의 겨울)에서는 잘 작동하지만 달성 가능한 차가운 온도가 외부 건구 온도에 의해 제한되고 대류가 증발보다 덜 효율적이기 때문에 온건하거나 따뜻한 온도에서는 잘 작동하지 않습니다.

**그림 5.16 공기 이코노마이저 데이터 센터의 공기 흐름 개략도**
*(신선한 공기 흡입 -> 냉각 공급 -> 핫 아일 수집 -> 배출)*

### 5.3.5 냉각탑(Cooling towers)

냉각탑(**그림 5.17**)은 물의 일부를 대기 중으로 증발시켜 물 흐름을 냉각시킵니다. 액체를 기체로 바꾸는 데 필요한 에너지를 잠열(latent heat)이라고 합니다. 증발하는 물은 이 잠열을 제거하여 유리한 날씨 조건에서 남은 물을 상당히 냉각시킬 수 있습니다. 냉각된 물의 최종 온도는 외부 공기의 습구 온도에 가깝습니다. 달성 가능한 수온과 습구 온도의 차이를 냉각탑의 "어프로치(approach)"라고 하며 일반적으로 1-$2^\circ C$입니다. (실제 습구 온도에 도달하려면 물이 무한히 천천히 흘러야 합니다.)

탑을 흐르는 물은 칠러나 PCWS 루프에 연결된 다른 열교환기에서 직접 옵니다. **그림 5.18**은 작동 원리를 보여줍니다.

1.  데이터 센터의 뜨거운 물이 냉각탑 상단에서 탑 내부의 "충전재(fill)" 재료 위로 흐릅니다. 충전재는 증발 성능을 향상시키기 위해 추가 표면적을 생성합니다.
2.  물이 탑 아래로 흐르면서 일부가 증발하여 남은 물에서 에너지를 빼앗아 온도를 낮춥니다.
3.  상단의 팬은 탑을 통해 공기를 흡입하여 증발을 돕습니다. 건조한 공기가 측면으로 들어와 습한 공기가 상단으로 나갑니다.
4.  차가운 물은 탑 바닥에 모여 데이터 센터로 반환됩니다.

냉각탑은 습도가 낮은 온대 기후에서 가장 잘 작동합니다. 아이러니하게도 탑과 파이프의 결빙을 방지하기 위한 추가 메커니즘이 필요하기 때문에 매우 추운 기후에서는 잘 작동하지 않습니다.

**그림 5.17 데이터 센터 냉각탑**

**그림 5.18 냉각탑 작동 원리**
*(1. 뜨거운 물 유입, 2. 물과 공기의 접촉/증발, 3. 공기 유입, 4. 냉각된 물 배출)*

### 5.3.6 공기 흐름 고려 사항

기존 데이터 센터는 위에서 논의한 이중 마루 설정을 사용합니다. 특정 랙이나 열(row)에 전달되는 냉각 양을 변경하기 위해 천공 타일을 솔리드 타일로 교체하거나 그 반대로 합니다. 냉각이 잘 작동하려면 타일을 통해 들어오는 찬 공기 흐름이 랙의 서버를 통과하는 수평 공기 흐름과 일치해야 합니다. 예를 들어 랙에 각각 100CFM(분당 입방 피트)의 공기 흐름이 있는 서버가 10개 있는 경우 천공 타일에서 나오는 순 흐름은 1,000CFM이어야 합니다(서버로 가는 공기 경로가 엄격하게 제어되지 않는 경우 더 높아야 함). 낮으면 일부 서버는 찬 공기를 받는 반면 다른 서버는 랙 위나 다른 누출 경로에서 재순환된 따뜻한 공기를 섭취합니다.

**그림 5.19**는 데이터 센터의 공기 흐름을 과잉 할당(oversubscribing)하는 랙에 대한 전산 유체 역학(CFD) 분석 결과를 보여줍니다. 이 예에서 랙 상단을 가로지르는 재순환으로 인해 상부 서버가 따뜻한 공기를 섭취합니다. 하단 서버도 랙 아래의 재순환 경로에 영향을 받습니다. 케이블 관리 하드웨어의 막힘으로 인해 랙 중간쯤에 적당한 온난 구역이 발생합니다. (이중 마루가 없지만 오버헤드 냉각이 있는 설계에서는 상단과 하단이 반전된 것을 제외하고 효과는 동일합니다.)

**그림 5.19 공기 흐름이 부족하게 공급된 랙의 CFD 모델**

이러한 상황에 대한 시설 관리자의 첫 번째 반응은 CRAC 출력의 온도를 낮추는 것일 수 있습니다. 효과는 있지만 에너지 비용이 크게 증가하므로, 대신 근본적인 문제를 해결하고 가능한 한 찬 공기와 따뜻한 공기를 물리적으로 분리하는 동시에 CRAC로 돌아가는 경로를 최적화하는 것이 좋습니다. 이 설정에서는 전체 방이 찬 공기로 채워지므로(따뜻한 배기는 별도의 플레넘이나 덕트 시스템 내부에 유지되므로) 랙의 모든 서버가 동일한 온도에서 공기를 섭취합니다[18].

핫 아일 차폐(containment)가 있더라도 공기 흐름은 데이터 센터 전력 밀도를 제한합니다. 서버 전반의 고정된 온도 차이에 대해 랙의 공기 흐름 요구 사항은 전력 소비에 따라 증가하며 랙 전면에 공급되는 공기 흐름은 전력에 따라 선형적으로 증가해야 합니다. 이는 차례로 이중 마루 플레넘이나 핫 아일에 필요한 정압(static pressure)의 양을 증가시킵니다. 낮은 전력 밀도에서는 이를 달성하기 쉽지만 어느 시점이 되면 물리 법칙으로 인해 압력과 공기 흐름을 더 높이는 것이 경제적으로 비현실적이 됩니다. 일반적으로 이러한 제한으로 인해 비용을 크게 늘리지 않고 150–200 W/sq ft 이상의 전력 밀도를 초과하기 어렵습니다.

### 5.3.7 랙 내부, 열 내부(In-row) 및 액체 냉각

랙 내부(In-rack) 냉각은 기존의 이중 마루 한계를 넘어 전력 밀도와 냉각 효율성을 높일 수 있습니다. 일반적으로 랙 내부 쿨러는 랙 뒷면에 공기 대 물 열교환기를 추가하여 서버를 빠져나가는 뜨거운 공기가 즉시 물로 냉각된 코 위로 흐르도록 하여 본질적으로 서버 배기 가스와 CRAC 입력 사이의 경로를 단락시킵니다. 랙 내부 냉각은 열의 일부 또는 전부를 제거하여 효과적으로 CRAC를 대체할 수 있습니다. 분명히 냉각수를 각 랙으로 가져와야 하므로 배관 비용이 크게 증가합니다. 일부 운영자는 코일 누출이나 사고로 인해 장비에 물이 쏟아질 수 있으므로 데이터 센터 바닥에 물이 있는 것을 걱정할 수도 있습니다.

열 내부(In-row) 냉각은 냉각 코일이 랙 안이 아니라 랙 옆에 있다는 점을 제외하면 랙 내부 냉각처럼 작동합니다. 포집 플레넘은 뜨거운 공기를 코일로 보내고 콜드 아일로의 누출을 방지합니다. **그림 5.20**은 열 내부 냉각 제품과 랙 사이에 배치되는 방법을 보여줍니다.

**그림 5.21**은 Google 열 내부 냉각 시스템(가장 안쪽 냉각 루프)의 주요 기능과 공기 흐름을 보여줍니다. IT 장비의 뜨거운 배기는 수직 플레넘 공간 또는 핫 아일(1)에서 상승하여 드롭 천장(2) 위의 별도 플레넘 공간으로 들어갑니다. 그곳에서 열교환기(3)에 의해 냉각되고 열은 공정수에 의해 제거됩니다. 마지막으로 팬이 찬 공기를 콜드 아일(4)로 불어 넣어 다시 IT 장비로 들어갑니다.

뜨거운 공기 차폐가 효율적이고 저렴하기 때문에 유사한 변형이 널리 보급되었습니다. 높은 수직 핫 아일 플레넘은 IT 랙 후면의 배기 공기를 오버헤드 팬 코일로 보냅니다. 팬 코일은 외부 냉각 플랜트에서 냉각된 공정수를 받습니다. 이 물은 핀(fin)에 부착된 여러 튜브 통로를 통해 흐르며 들어오는 뜨거운 공기에서 열을 흡수합니다. 팬 코일 장치의 송풍기는 냉각된 공기를 콜드 아일로 강제로 내려 보내 서버 및 네트워킹 장비의 흡입구로 들어갑니다. 냉각 플랜트 및 공정수 루프와 함께 이 공기 루프는 IT 장비가 소비하는 에너지에 10% 미만을 추가하는 고효율 엔드투엔드 냉각 시스템을 구성합니다.

**그림 5.20 열 내부(In-row) 에어컨**

**그림 5.21 콜드 아일 및 관련 열기 플레넘의 단면**
*(1. 핫 아일 상승, 2. 천장 플레넘, 3. 열교환기/공정수, 4. 콜드 아일 하강)*

마지막으로 콜드 플레이트(cold plate), 즉 국소 액체 냉각 히트싱크를 사용하여 서버 구성 요소를 직접 냉각할 수도 있습니다. 일반적으로 모든 구성 요소를 콜드 플레이트로 냉각하는 것은 비실용적입니다. 대신 전력 소비가 가장 높은 구성 요소(예: 프로세서 칩)를 액체 냉각 대상으로 지정하고 다른 구성 요소는 공냉식으로 냉각합니다. 히트싱크를 순환하는 액체는 트레이나 랙 근처에 배치되거나 데이터 센터 건물(예: 냉각탑)의 일부일 수 있는 액체 대 공기 또는 액체 대 액체 열교환기로 열을 운반합니다.

더 높은 비용과 기계적 설계 복잡성에도 불구하고, 콜드 플레이트는 칩당 TDP가 일반 히트싱크로 냉각하기에 실용적인 수준(일반적으로 칩당 200–250W)을 초과하는 초고밀도 워크로드를 냉각하는 데 필수적이 되고 있습니다. Google의 3세대 텐서 처리 장치(TPU)가 그 예입니다. **그림 5.22**와 같이 동일한 마더보드에 있는 4개의 TPU가 단일 물 루프에서 직렬로 냉각됩니다[20]. 찬물은 랙(왼쪽)에서 들어와 튜브를 통해 빠져나가기 전에 4개의 콜드 플레이트를 모두 통과합니다. 각 칩은 들어오는 물을 거의 같은 양만큼 가열하므로 나가는 수온은 4개의 TPU가 소비하는 총 전력을 물 흐름 속도와 물의 열 질량으로 나눈 값에 의해 결정됩니다. 원칙적으로 각 칩은 자체 냉수 공급을 가질 수 있지만 TPU v3의 경우 그럴 필요가 없었습니다.

이 수랭식을 가능하게 하려면 각 TPU 랙에 찬물을 공급해야 합니다. 원칙적으로 물은 이미 바닥에서 사용할 수 있지만(위에서 논의한 대로 랙 뒤나 위에 있는 냉각 인프라를 통해 흐르고 있음), 트레이(on-tray) 냉각에는 훨씬 더 많은 소비 지점(트레이)으로 물을 미세하게 분배해야 합니다. **그림 5.23**은 공정수 루프에 연결되어 각 랙에 대한 흐름으로 나누는 냉각 분배 장치(CDU)를 보여줍니다. 물은 상단에 보이는 검은 호스를 통해 각 랙으로 유입(및 유출)됩니다. 각 트레이는 물을 흘리지 않고 개별 트레이를 서비스할 수 있는 흘림 방지(no-drip) 퀵 커넥트 커넥터를 통해 해당 공급 장치에 연결됩니다.

**그림 5.22 구리 콜드 플레이트는 TPUv3에 액체 냉각을 제공합니다**

**그림 5.23 냉각 분배 장치(CDU)와 TPU 랙 열**

컴퓨팅 전력(및 열) 밀도가 계속 증가함에 따라 시스템 설계자는 액체 침수 냉각(immersion cooling)과 같은 WSC를 위한 다른 새로운 냉각 솔루션도 탐구하고 있습니다. 이름에서 알 수 있듯이 이러한 설계는 전체 서버를 비전도성 유체에 담가 열을 더 효과적으로 방출하고 전력을 덜 사용합니다. 액체 냉각의 우수한 열교환 기능은 부피가 큰 히트싱크, 팬, 공기 흐름 채널의 필요성을 제거하여 잠재적으로 새롭고 더 밀도 높은 시스템 설계(새로운 3D 서버 설계 포함[21])를 가능하게 합니다.

단상(1P) 및 2상(2P)의 두 가지 주요 침수 냉각 솔루션이 탐구되었습니다. 1P 침수 냉각[22]은 펌프를 사용하여 라디에이터 냉각 루프를 통해 미네랄 오일과 같은 유전체 유체를 순환시킵니다. 반면 2P 침수 냉각은 끓는점이 낮은(섭씨 50-60도) 액체를 사용하여 상변화를 통해 열을 방출하여 서버 구성 요소를 냉각합니다[23].

침수 냉각은 훨씬 낮은 PUE를 가능하게 할 수 있지만 아직 널리 채택되지는 않았습니다. 서버 보드는 냉각 유체 및 신호 무결성에 미치는 영향과 호환되도록 재설계되어야 할 수 있으며 광 커넥터도 재설계가 필요할 수 있습니다. 이러한 문제를 해결할 수는 있지만, 설계를 주류(일용품) 공급망 밖으로 이동시켜 추가 비용을 발생시킵니다. 또한 침수된 랙은 유지 관리가 훨씬 더 어려우며, 특히 전체 전력 밀도가 높아야 하는 경우 높은 탱크가 필요합니다.

---

## 5.4 기타 데이터 센터 고려 사항

### 5.4.1 속도와 효율성을 위한 모듈형 데이터 센터 설계

WSC 데이터 센터는 크고 복잡하며 설계 및 배포는 시간 집약적이고 노동 집약적이며 자본 집약적입니다. 모듈형 데이터 센터 설계는 이러한 지표를 개선할 수 있습니다.

모듈형 설계를 통해 개별 빌딩 블록으로 특정 제품(또는 특정 고객 요구 사항을 목표로 하는 변형)을 만들 수 있습니다. **그림 5.24**와 같이 모듈형 데이터 센터는 계층적 구성 접근 방식을 따르며, 개별 구성 요소에서 시작하여 모듈, 주요 데이터 센터 하위 구성 요소, 궁극적으로 전체 데이터 센터를 구축하도록 결합됩니다. 모듈형 데이터 센터는 두 가지 핵심 요소로 특징지어집니다.

*   구성 요소 및 하위 시스템 간의 유연한 인터페이스를 통해 여러 구성으로 전체 시스템을 구축할 수 있습니다. 이러한 인터페이스는 소프트웨어 API와 유사하며 신중하게 설계되고 버전 제어되어야 합니다.
*   "부품 키트(Kit-of-parts) 건설"은 사전 설계되고 표준화된 구성 요소 세트를 사용하여 다양한 방식으로 조립하여 다른 구성 요소를 만드는 것입니다. 부품 키트 건설은 조립의 유연성과 제조 효율성을 달성하려고 할 뿐만 아니라 분해 가능성(demountability), 분해 및 재사용을 요구하는 조립식(prefabrication)의 하위 집합입니다.

모듈형 데이터 센터는 새로운 또는 위치별 요구 사항이나 공급망 제약을 충족하기 위해 지연 바인딩(late binding) 구성 및 유연한 재구성을 가능하게 합니다. 예를 들어 핫 아일 냉각 구조는 일반 변형과 지진 등급 변형으로 제공될 수 있습니다. 새로운 데이터 센터는 표준화된 부품 세트에서 빠르게 구성할 수 있으며 하위 시스템(네트워크 룸, 전력, 냉각, 공간)이 독립적으로 발전할 수 있습니다. 마지막으로 중요한 것은 사전 배포된 모듈형 건설 및 배송(예: 장비 스키딩 및 사전 조립)을 포함하여 제조가 더 쉬워진다는 것입니다.

**그림 5.24 모듈형 데이터 센터의 계층적 구성**
*(데이터 센터 -> 제품(전기/냉각 등) -> 모듈 -> 구성 요소(발전기/변압기 등))*

### 5.4.2 경제 모델: 5가지 엔벨로프(Envelopes)

리드 타임과 비용을 줄이기 위해 데이터 센터 용량은 서로 다른 완료 단계에서 보유될 수 있습니다. 예를 들어 적절한 토지를 찾고 데이터 센터에 필요한 모든 구역 설정 승인 및 유틸리티 연결을 얻는 데 몇 달 또는 몇 년이 걸릴 수 있습니다. 마찬가지로 부지 배수와 건물의 안정적인 기초를 확보하기 위해 땅을 평평하게 하거나 경사를 만드는 "부지 정지(grading)" 작업에 몇 달이 걸릴 수 있습니다. 용량을 정지된 토지로 보유하는 것은 부분적으로 완성된 시설로 보유하는 것보다 훨씬 저렴하지만 서버를 호스팅하기 위한 실제 용량을 구축하는 데 더 긴 리드 타임이 필요합니다. 따라서 수요가 불확실한 경우(항상 그렇습니다!), 최상의 공급 전략은 건설 프로세스의 여러 단계에서 추가 버퍼를 유지하고, 초기 단계일수록 유지 비용이 훨씬 낮으므로 더 큰 버퍼를 유지하는 것입니다.

**그림 5.25**의 맨 윗줄은 데이터 센터 건설 단계를 보여줍니다. 권리 확보된 토지(Entitled land)는 현재 우리가 소유한 토지 필지("타이틀 보유")입니다. 다음 단계는 유틸리티, 주로 상하수도, 전력, 어쩌면 도로에 연결하는 것입니다. 이상적으로 토지는 이미 필요한 용량의 유틸리티에 가깝습니다. 이 단계는 유틸리티의 응답성과 허가를 받고 작업을 실행하는 능력에 따라 몇 달에서 몇 년이 걸릴 수 있습니다. 패드 및 쉘(Pad and shell)은 부지 정지(건물에 적합한 패드 생성) 및 내부 장비(fitout) 없는 건물 쉘 건설을 의미합니다. 그 다음은 일반적으로 전기 변전소로, 종종 리드 타임이 길고(12-18개월이 일반적임) 상당한 투자입니다. 경험적으로 변전소 비용은 $0.10/W이며 유틸리티가 유휴 상태로 두기 위해 건설하지 않으므로 최소 전력 소비 요구 사항이 따르는 경우가 많습니다.

그 후 실제 데이터 센터 인프라 구축이 포함되는 다음 단계부터는 투자($/W)가 급격히 증가합니다. 먼저 바닥 외부의 시설 부분인 냉각 및 전기 플랜트와 그 제어 장치입니다. 다음은 데이터 센터 홀 내부 장비(fitout), 즉 버스바, 케이블 트레이, 냉각 장치 등으로 데이터 센터 내부를 채우는 것입니다. 마지막으로 공간이 장비를 받을 준비가 되면 랙에 필요한 인프라를 제공하기 위한 클러스터 핏아웃(cluster fit-out)이 수행됩니다: 랙 전원용 탭 박스, Clos 네트워크용 네트워크 랙 및 광섬유 번들 등.

다른 주체가 이러한 건설 단계를 수행할 수 있으므로 다양한 경제 모델이 가능합니다. **그림 5.25**에 설명된 5가지 엔벨로프 프레임워크는 벤더와 WSC 제공업체 간의 책임 분할의 일반적인 구분을 보여줍니다. 각 엔벨로프는 최종 WSC 운영자가 구축한 것과 제공업체로부터 구매(또는 임대)한 것 사이의 경계를 이동시킵니다.

엔벨로프 1은 하이퍼스케일러가 내부적으로 엔드투엔드로 설계하고 구축한 WSC 데이터 센터를 나타냅니다. 시장에서 구매하는 유일한 구성 요소는 토지와 유틸리티 연결입니다. 엔벨로프 2는 사내에서 구축하는 대신 제3자가 구축한 건물 쉘을 추가로 구매합니다. 스펙트럼의 반대편 끝에 있는 엔벨로프 5는 실제 장비 배포를 제외한 모든 것을 제3자가 수행하는 코로케이션 선택을 나타냅니다. 엔벨로프 4는 매우 유사하지만 플로어 핏아웃 및 운영의 일부를 WSC 운영자에게 이동시킵니다. WSC 운영자가 전체 시설을 임대할 때 더 일반적입니다.

WSC 운영자는 현지 상황에 따라 이러한 엔벨로프를 모두 사용하거나 일부만 사용할 수 있습니다. 예를 들어 소규모 네트워크 POP 데이터 센터의 경우 엔벨로프 5가 일반적으로 가장 비용 효율적인 반면, 대규모 데이터 센터 캠퍼스의 경우 엔벨로프 1이 일반적으로 가장 저렴합니다.

**그림 5.25 데이터 센터 설계의 5가지 엔벨로프**
*(토지 -> 유틸리티 연결 -> 패드/쉘 -> 변전소 -> 냉각/전기 플랜트 -> 데이터 센터 홀 -> 핏아웃)*

### 5.4.3 데이터 센터의 물리적 자동화

창고형 컴퓨터는 외관과 규모뿐만 아니라 실제 창고와 유사합니다. WSC에서도 상당한 물품 이동이 발생합니다. 매일 트럭이 도착하여 새 장비를 내리거나 초과 장비를 다른 위치로 옮기거나 노후 장비를 제거합니다. DIMM과 디스크는 플로어의 트레이로 또는 트레이에서 배달되며, 서버와 랙은 열(row)이나 층 사이를 이동합니다.

실제 창고에서는 물리적 자동화가 일반적입니다. 물품은 창고 바닥을 돌아다니는 로봇, 높은 선반을 서비스하는 자동 리프트 또는 측면으로 이동시키는 컨베이어 시스템을 통해 이동됩니다. WSC에서는 자재 이동이 덜 집중적이고 저렴한 소비재를 선반으로 옮기는 것에 비해 값비싼 부품에 대한 정밀한 작업이 필요한 경우가 많기 때문에 물리적 자동화가 덜 널리 퍼져 있습니다.

그러나 데이터 센터 건물과 캠퍼스가 커짐에 따라 더 안전하고 효율적인 운영을 가능하게 하는 것이 점점 더 중요해지고 있습니다. 자동화의 주요 이점 중 하나는 자재가 자동화 시스템을 통과할 때 스캔하고 추적할 수 있는 내장 기능입니다. 이는 하드 드라이브 및 SSD와 같은 사용자 데이터가 포함된 저장 매체의 추적성을 개선하는 데 특히 유용합니다. 자동화와 인터페이스하도록 서버와 랙을 설계하면 서버 설치 및 수리를 개선할 수 있는 새로운 기회도 열립니다.

**그림 5.26**은 Google 데이터 센터에서 완전히 적재된 랙을 픽업, 이동 및 배치하는 데 사용되는 랙 무버(Rack Mover) 로봇을 보여줍니다. 이 랙 이동 방법은 5배 빠르며 부상 위험을 줄입니다. 또한 랙은 낮은 수준의 충격과 진동에 노출되므로 새로운 위치에서 랙을 더 빠르게 가동할 수 있습니다.

**그림 5.26 Google 랙 이동용 랙 무버**

데이터 센터 운영 팀의 주요 목표 중 하나는 저장 매체의 안전한 관리입니다. 기술자가 사용자 데이터가 포함된 미디어를 처리할 때 미디어의 소유권을 가져와 처리를 위해 전용 보안 구역으로 안전하게 전달해야 합니다. 각 단계에서 장치를 스캔하여 모든 이동을 추적하고 감사할 수 있습니다. 저장 매체 장치의 수가 증가함에 따라 자동화는 이 작업에 큰 도움이 되었습니다.

**그림 5.27**은 파쇄기로의 미디어 액세스 경로를 제어하는 검증 후드(verification hood)가 있는 디스크 파쇄기를 보여줍니다. 후드는 미디어 부품 번호를 캡처하고 미디어가 파기될 것인지 확인하며 파쇄기에 의해 미디어가 파기되면 감사 로그를 업데이트합니다. 로그는 향후 감사를 위해 사용할 수 있는 파기 증명서(COD) 배후의 소스 데이터를 캡처합니다. 삭제 및 분해 후 미디어 재사용은 순환 경제를 지원할 수 있습니다[24].

다른 물리적 자동화도 데이터 센터 기술자의 물리적 노고를 줄이는 데 도움이 될 수 있습니다. 예를 들어 서버 트레이 리프트를 사용하면 한 기술자가 가장 무거운 서버도 삽입하거나 제거할 수 있습니다. 마찬가지로 자율 주행 차량(AGV)은 그러한 트레이를 기술자에게 오가며 운반하여 100미터 이상 떨어져 있을 수 있는 수집 지점까지 걸어가는 시간을 절약할 수 있습니다. 데이터 센터 사용 사례는 다른 산업과 유사한 요구 사항을 가지고 있으므로 신흥 자동화 공급업체 생태계가 이러한 사용 사례를 점점 더 잘 지원하고 있습니다.

**그림 5.27 미디어 검증 장치가 있는 디스크 파쇄기**

---

## 5.5 역사적 회고

Google에서 WSC의 첫 25년 동안 WSC 데이터 센터는 상당한 진화를 겪었습니다. Google은 처음에 타사 코로케이션(Colo)의 저렴하고 조잡한 랙 솔루션으로 시작했습니다. **그림 5.28**은 그러한 초기 배포 중 하나를 보여줍니다. 당시 코로케이션은 전력이 아닌 공간 기준으로 요금을 청구했기 때문에 랙은 평방피트당 훨씬 더 많은 서버를 채워 넣었고, 때로는 시설의 냉각 지원 용량보다 높은 전력 밀도에 도달하여 사진에 보이는 외부 팬이 필요했습니다!

곧 Google은 자체 데이터 센터를 구축하기 시작하여 대형 창고를 인수하고 컴퓨팅 인프라로 채웠습니다. 이것이 창고형 컴퓨팅이라는 용어의 기원이었습니다. WSC 데이터 센터에 대한 많은 주요 혁신이 이 시기에 설계되고 배포되었습니다: 열기 포집, 온수(warm air) 냉각, 로컬 UPS. 첫날부터 이러한 데이터 센터 설계는 전력 및 냉각과 관련된 오버헤드를 최소화하는 것을 강조했습니다. 이는 나중에 PUE(전력 사용 효율성) 지표로 제도화될 것이었습니다(9장에서 논의됨). 이것은 또한 Google이 최초의 컨테이너 기반 데이터 센터(2003년 Google 특허 출원으로 거슬러 올라가는 아이디어[25])를 실험했던 단계였습니다.

**그림 5.28 Google 서버와 "외부 냉각 보조 장치"가 있는 초기 코로케이션**

컨테이너 기반 데이터 센터(**그림 5.30**)는 서버 랙을 컨테이너(일반적으로 20 또는 40피트 길이) 안에 배치하고 열교환 및 전력 분배를 컨테이너에 통합함으로써 열 내부 냉각을 한 단계 더 발전시켰습니다. 열 내부 냉각과 마찬가지로 컨테이너에는 냉수 공급이 필요했고 코일을 사용하여 모든 열을 제거했습니다. 밀접 결합된(close-coupled) 공기 처리는 일반적으로 일반 이중 마루 데이터 센터보다 더 높은 전력 밀도를 허용했습니다. 따라서 컨테이너 기반 데이터 센터는 일반적인 데이터 센터 룸의 모든 기능(랙, CRAC, PDU, 케이블링, 조명)을 하나의 작은 패키지로 제공했습니다. **그림 5.29**는 Google의 컨테이너 설계의 등각 단면도를 보여줍니다. 일반 데이터 센터 룸과 마찬가지로 컨테이너가 완전히 작동하려면 칠러, 발전기, UPS 장치와 같은 외부 인프라가 동반되어야 합니다.

그러나 다음 세대의 Google 데이터 센터는 컨테이너에서 벗어나 대신 더 넓은 창고 수준에서 동일한 원칙을 통합했습니다. 데이터 센터 설계의 이 다음 단계는 확장성, 적정 규모(right sizing) 및 가동 시간에 중점을 두었습니다. 랙 수준 UPS가 도입되었으며 더 큰 전력 및 냉각 공유 도메인도 도입되었습니다. 다음 혁신의 물결은 데이터 센터로의 중전압 전력 전달을 도입한 야심 찬 아키텍처[26]와 데이터 센터 활용도 및 비용 효율성을 높이기 위한 공격적인 전력 오버서브스크립션을 위한 소프트웨어 공동 설계(codesign)가 이끌었습니다. 이 기간 동안 배터리 기술도 납축전지에서 리튬 이온으로 전환되었습니다.

최근 클라우드 컴퓨팅 워크로드의 성장으로 인해 이기종 IT 요구 사항과 상용(off-the-shelf) 구성 요소에 대한 관심이 높아졌습니다. 현장 건설 시간을 줄이기 위해 조립식 솔루션(예: 모듈형 전기 건물)을 사용하여 건물이 더 모듈식으로 설계되었습니다.

**그림 5.29 Google의 2005년 컨테이너 설계**

**그림 5.30 Google 캠퍼스에 설치 중인 프로토타입 컨테이너**

머신 러닝 배포는 모든 랙 지점에 대규모로 액체 냉각을 배포하게 했습니다. 물리적 자동화는 자재 취급의 효율성, 안전 및 보안을 개선했습니다.

오늘날 WSC 데이터 센터 설계는 아직 끝나지 않았습니다. 비용과 에너지 효율성, 지속 가능성, 설계 시간, 모듈성 및 점진적 배포 가능성 간의 복잡한 트레이드오프 세트를 최적화하는 것은 계속해서 혜택을 주는 선물입니다.

---

## References

1. US EPA, Report to Congress on server and data center energy efficiency, https://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Report_Exec_Summary_Final.pdf, 2007.
2. J. Koomey et al., “Growth in data center electricity use 2005 to 2010,” A report by Analytical Press, completed at the request of The New York Times, vol. 9, no. 2011, p. 161, 2011.
3. International Code Council (ICC), Chapter 6: Types of Construction - 2015 International Building Code(IBC), https://codes.iccsafe.org/public/document/IBC2015/chapter-6-types-of-construction, 2015.
4. dup.
5. W. P. Turner IV, J. PE, P. Seader, and K. Brill, “Tier classification defines site infrastructure performance,” Uptime Institute, vol. 17, 2006.
6. Uptime Institute, Tier Standard: Operational Sustainability, https://uptimeinstitute.com/resources/asset/tier-standard-operational-sustainability.
7. Uptime Institute, Tier Standard: Topology, https://uptimeinstitute.com/publications/asset/tier-standard-topology.
8. V. Sakalkar, V. Kontorinis, D. Landhuis, et al., “Data center power oversubscription with a medium voltage power plane and priority-aware capping,” in Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems, 2020, pp. 497–511.
9. P. Ranganathan, P. Leech, D. Irwin, and J. Chase, “Ensemble-level power management for dense blade servers,” in 33rd International Symposium on Computer Architecture (ISCA’06), 2006, pp. 66–77.
10. A. J. George and G. Ferrand, Cost study of AC vs. DC data center power topologies based on system efficiency, https://www.eltek.com/globalassets/media/downloads/white-papers_case-studies/cost-study-on-ac-vs-dc-data-center-based-on-system-efficiency-an-eltek-white-paper.pdf, 2017.
11. F. L. Rawson, J. Pfleuger, and T. Cader, “Comparing the energy consumption of servers using workload injection,” Intel Corporation, Tech. Rep., 2008.
12. W. Whitted, M. Sykora, K. Krieger, et al., “Data center uninterruptible power distribution architecture,” US Patent 7,560,831, Jul. 2009.
13. P. Sarti, Battery cabinet hardware v1. 0, https://telecombloger.ru/wp-content/uploads/2012/10/Open_Compute_Project_Open_Rack_v1.0.pdf, 2012.
14. S. McCauley and S. Jiang, Google 48V update: Flatbed and STC, https://www.opencompute.org/files/External-2018-OCP-Summit-Google-48V-Update-Flatbed-and-STC-20180321.pdf, 2018.
15. Google Cloud, TPUs improved carbon efficiency of AI workloads by 3x, Google Cloud Blog: https://cloud.google.com/blog/topics/systems/100-million-li-ion-cells-in-google-data-centers, 2025.
16. A. Radovanovic, Our data centers now work harder when the sun shines and wind blows, The Keyword, https://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows/, 2020.
17. V. Kontorinis, L. E. Zhang, B. Aksanli, et al., “Managing distributed UPS energy for effective power capping in data centers,” in 2012 39th Annual International Symposium on Computer Architecture (ISCA), 2012, pp. 488–499.
18. M. K. Patterson and D. Fenwick, The state of data center cooling, https://www.ceclimited.com/sites/all/themes/creative/state-of-date-center-cooling.pdf, 2008.
19. D. Atwood and J. G. Miner, Reducing data center cost with an air economizer, https://www.intel.com/content/dam/doc/technology-brief/data-center-efficiency-xeon-reducing-data-center-cost-with-air-economizer-brief.pdf, 2008.
20. N. P. Jouppi, D. H. Yoon, G. Kurian, et al., “A domain-specific supercomputer for training deep neural networks,” Commun. ACM, vol. 63, no. 7, pp. 67–78, Jun. 2020.
21. M. Kodnongbua, Z. Englhardt, R. Bianchini, et al., “Dense server design for immersion cooling,” ACM Trans. Graph., vol. 43, no. 6, Nov. 2024, issn: 0730-0301.
22. Y. Zhong, A large scale deployment experience using immersion cooling in data centers, Open Compute Project Summit, https://www.opencompute.org/events/past-events/2019-ocp-global-summit, 2019.
23. B. Ramakrishnan, H. Alissa, I. Manousakis, et al., “Cpu overclocking: A performance assessment of air, cold plates, and two-phase immersion cooling,” IEEE Transactions on Components, Packaging and Manufacturing Technology, vol. 11, no. 10, pp. 1703–1715, 2021.
24. iNEMI, iNEMI 2019 Value Recovery from End-of-Life Electronics Project Phase 2 Report, https://thor.inemi.org/webdownload/2019/iNEMI-Value_Recovery2_Report.pdf, 2019.
25. W. H. Whitted and G. Aigner, Modular data center, US Patent 7,278,273, 2007.
26. V. Sakalkar, V. Kontorinis, D. Landhuis, et al., “Data center power oversubscription with a medium voltage power plane and priority-aware capping,” in Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS ’20, New York, NY, USA: Association for Computing Machinery, Mar. 13, 2020, pp. 497–511.