다음은 PDF 파일의 내용을 한국어로 번역하여 마크다운(Markdown) 형식으로 정리한 것입니다. 원문의 구조와 내용을 충실히 반영하였습니다.

---

# 10. 신뢰할 수 있는 컴퓨팅: 신뢰성, 가용성, 보안, 프라이버시

## 10.1 대규모 환경에서의 결함 감내 (Fault tolerance at scale)

### 10.1.1 대규모 환경에서의 결함 감내 소개

대규모 엔터프라이즈 및 소비자 클라우드 사용은 서비스가 언제나 사용 가능할 것이라는 신뢰를 필요로 합니다. 이러한 기대는 건물 크기만한 컴퓨터(building-sized computers)에 대한 높은 신뢰성 요구사항으로 이어집니다. 근본적으로, 적절한 수준의 신뢰성은 장애 비용(수리 비용 포함)과 이를 예방하는 비용 사이의 트레이드오프(tradeoff)입니다. 전통적인 엔터프라이즈급 서버의 경우 장애 비용이 높다고 여겨지므로, 설계자들은 중복 전원 공급 장치, 팬, 오류 정정 코드(ECC), RAID 디스크 등을 추가하여 더 신뢰할 수 있는 하드웨어를 제공하기 위해 많은 노력을 기울입니다. 많은 레거시 엔터프라이즈 애플리케이션은 잦은 하드웨어 결함을 견디도록 설계되지 않았으며, 사후에 결함 감내(fault-tolerant) 기능을 추가하기도 어렵습니다. 이러한 상황에서는 하드웨어를 매우 신뢰성 있게 만드는 것이 합리적인 설계 결정입니다.

그러나 웨어하우스 규모 컴퓨터(WSC)에서는 주로 그 규모 때문에 하드웨어 신뢰성만으로는 충분한 가용성을 제공할 수 없습니다. 어떤 클러스터가 30년(10,000일)이라는 뛰어난 평균 고장 간격(MTBF)을 가진 초고신뢰성 서버 노드를 가지고 있다고 가정해 봅시다. 이렇게 신뢰성 높은 서버라 할지라도 10,000대의 서버로 구성된 클러스터는 평균적으로 하루에 한 대의 서버 장애를 겪게 됩니다. 따라서 전체 클러스터의 가용성에 의존하는 애플리케이션은 MTBF가 하루를 넘지 못하게 됩니다.

게다가 크고 복잡한 인터넷 서비스는 종종 버그가 없지 않은 여러 소프트웨어 모듈이나 계층으로 구성되며, 이는 하드웨어 구성 요소보다 더 높은 비율로 실패할 수 있습니다. 결과적으로 WSC 애플리케이션은 애플리케이션 자체의 코드나, 실패한 VM을 예비 노드에서 재시작하는 가상 머신 프로비저닝 시스템과 같은 미들웨어가 제공하는 기능을 통해 실패한 서비스를 우회하여 작동해야 합니다. 이러한 환경을 위한 소프트웨어 작성의 함의 중 일부는 MSN과 윈도우 라이브(Windows Live)의 대규모 서비스를 설계하고 운영한 경험을 바탕으로 해밀턴(Hamilton)[1]이 논의한 바 있습니다. 신뢰할 수 있는 시스템에 대한 추가적인 심도 있는 논의를 위해 SRE 서적[2]과 소프트웨어 신뢰성 서적[3]도 추천합니다.

일부 퍼블릭 클라우드 워크로드는 신뢰성 방정식에 또 다른 변수를 추가합니다. 엔터프라이즈 클라우드 워크로드는 종종 온프레미스(on-premise) 환경에서 클라우드로 거의 수정 없이 이동("리프트 앤 시프트")됩니다. 결과적으로 결함 감내 기능이 거의 없거나 전혀 내장되어 있지 않으며, 따라서 신뢰할 수 있는 VM에 의존합니다. 로드 밸런싱된 서버에서 실행되는 현대적인 대규모 서비스는 단일 머신 장애를 쉽게 견딜 수 있지만, 단일 클라우드 VM에서 실행되는 레거시 엔터프라이즈 애플리케이션은 그렇지 못합니다.

서버 중단에 대한 이러한 민감성에도 불구하고 높은 신뢰성을 제공하기 위해 퍼블릭 클라우드는 두 가지 주요 방어책을 사용합니다. 첫째, 하드웨어 장애가 발생했을 때 VM을 새로운 서버로 빠르게 다시 스케줄링할 수 있습니다. 이는 평균 복구 시간(MTTR)을 낮게 유지합니다. 둘째, 이러한 애플리케이션의 대부분은 상태비저장(stateless)이며 데이터를 데이터베이스에 저장합니다. 일반적으로 자체 관리형 데이터베이스는 GCP의 Cloud SQL과 같은 관리형 클라우드 데이터베이스로 마이그레이션되며, 이는 일반적으로 온프레미스 설치보다 더 나은 가동 시간과 성능을 제공합니다.

AI 학습 워크로드는 다른 과제를 안겨줍니다. 모델 학습은 각 노드가 모델 데이터의 일부를 포함하고 매 학습 단계(step)마다 다른 노드와 데이터를 교환하는 고도로 동기화된 계산입니다. (여러 면에서 이는 기상 예보와 같은 슈퍼컴퓨팅 애플리케이션과 유사합니다.) 결과적으로 단일 노드가 다운되면 전체 계산이 중단됩니다. 따라서 각각 5년(1800일)의 MTBF를 가진 9,000개의 TPU에서 실행되는 학습 작업은 약 4시간마다 중단을 겪게 될 것입니다! 예를 들어, 54일간의 Llama3 학습 실행 중 16K-GPU 클러스터는 약 320회의 하드웨어 장애를 겪었으며, 이로 인해 학습 실행 처리량이 약 10% 감소했습니다[4]. 6장에서 논의된 바와 같이 광 회선 스위칭(optical circuit switching)은 교체 TPU를 빠르게 교체하고 최신 체크포인트에서 재시작하는 데 도움을 줄 수 있지만, 대규모 동기식 계산의 신뢰성을 관리하는 것은 여전히 과제로 남아 있습니다.

계속하기 전에 가용성(availability), 비가용성(unavailability), 그리고 장애(failure)의 차이를 이해하는 것이 중요합니다. (가용성과 신뢰성의 차이는 10.1.3.1절에서 논의할 것입니다.) 시스템의 가용성은 시스템을 사용할 수 있는 시간의 비율입니다. 반대로 비가용성은 시스템을 어떤 이유로든 사용할 수 없는 시간의 비율입니다. 장애는 비가용성의 한 원인이지만, 하드웨어 또는 소프트웨어 업그레이드를 위한 계획된 유지 보수와 같은 다른 원인보다 훨씬 덜 일반적인 경우가 많습니다. 따라서 장애가 전혀 없는 시스템이라도 가용성이 100% 미만일 수 있으며, 장애율이 높은 시스템이라도 다른 비가용성 원인이 지배적이라면 장애가 적은 시스템보다 더 나은 가용성을 가질 수 있습니다.

### 10.1.2 하드웨어 대 소프트웨어의 결함 감내

결함 감내(fault-tolerant) 소프트웨어는 결함 없는 운영을 가정하는 소프트웨어보다 더 복잡합니다. 가능한 한 WSC는 애플리케이션 수준 소프트웨어에서 장애 처리의 복잡성을 대부분 숨길 수 있는 결함 감내 소프트웨어 인프라 계층을 구현하려고 노력해야 합니다.

일단 서비스에 과도한 중단 없이 하드웨어 결함을 감내할 수 있게 되면, 컴퓨터 아키텍트는 전체 시스템 비용 효율성을 극대화하는 하드웨어 신뢰성 수준을 선택할 수 있는 여지를 갖게 됩니다. 또한 이 모델은 일반적인 운영 절차를 단순화할 수 있습니다. 예를 들어, 클러스터의 저수준 시스템 소프트웨어를 업그레이드하기 위해 서버에서 실행 중인 워크로드를 "비울(drained)" 수 있습니다. 많은 워크로드는 수명이 짧아 빠르게 비울 수 있으며, 남아 있는 장기 실행 워크로드는 다른 호스트로 마이그레이션할 수 있습니다. 서버가 유휴 상태가 되면 침습적인 업그레이드(예: 재부팅이 필요한 경우)를 수행할 수 있습니다. 하드웨어 업그레이드도 유사한 절차를 따를 수 있습니다. 기본적으로 서버 장애를 처리하기 위해 구축된 동일한 결함 감내 소프트웨어 인프라 메커니즘이 광범위한 운영 절차를 지원하는 데 필요한 모든 메커니즘을 갖출 수 있습니다. 적절한 시간 창(time window)을 선택하고 재시작 작업의 속도를 제한함으로써 운영자는 계획된 서비스 수준 중단 횟수를 관리할 수 있습니다.

전통적인 서버 환경과 달리 하이퍼스케일 애플리케이션은 더 이상 개별 서버를 무슨 수를 써서라도 계속 가동할 필요가 없습니다. 이 단순한 요구사항의 변화는 기계 및 데이터 센터 설계에서 운영에 이르기까지 배포의 거의 모든 측면에 영향을 미치며, 그렇지 않았다면 불가능했을 최적화 기회를 가능하게 합니다. 예를 들어 이것이 복구 모델에 어떤 영향을 미치는지 살펴봅시다. 우주 입자 충돌로 인한 수정 불가능한 오류와 같이 피할 수 없는 일시적 하드웨어 결함이 있는 상황에서도 높은 신뢰성을 유지해야 하는 시스템은, 결함 감지 시 이전의 올바른 상태에서 실행을 재시작할 수 있도록 체크포인트 복구를 위한 하드웨어 지원이 필요할 수 있습니다. 이러한 결함 발생 시 다운되는 것이 허용되는 시스템은 체크포인팅에 드는 추가 비용이나 에너지 오버헤드를 부담하지 않기로 선택할 수 있습니다.

이러한 요구사항의 변화는 규모의 결과로 내장된 중복성을 가진 하이퍼스케일 애플리케이션에 매우 잘 작동합니다. 대규모 요청 볼륨을 처리하기 위해 이러한 애플리케이션은 이미 수평적으로 확장해야 하며, 이는 동일한 서버의 많은 복제본을 생성하여 결과적으로 결함 감내를 제공합니다. 종종 단일 서버 애플리케이션인 엔터프라이즈 애플리케이션의 경우 이것이 작동하지 않으므로 다른 솔루션이 필요합니다. 일반적인 접근 방식에는 명시적으로 조정된 유지 보수(예: 주말 유지 보수 기간 동안 커널 업그레이드 수행), 암시적으로 조정된 유지 보수(예: VM이 종료되거나 다시 시작될 때까지 대기), VM 마이그레이션(이전 서버가 유지 보수를 받을 수 있도록 고객 VM을 새 서버로 투명하게 이동) 제공이 포함됩니다.

또 다른 일반적인 접근 방식은 코드가 실행되는 동안 업그레이드하는 "무중단(hitless)" 업그레이드입니다. 예를 들어 Linux 커널은 라이브 패치[5]를 제공하며, 많은 네트워크 스위치는 데이터 평면 코드와 데이터를 제어 평면 코드와 엄격하게 분리하여 무중단 업그레이드를 제공합니다. 업그레이드 중에는 제어 평면이 다시 시작되는 동안 데이터 평면(일반적으로 하드웨어로 구현됨)이 패킷을 계속 전달합니다. 이러한 시스템은 유지 관리 및 테스트가 복잡할 수 있지만 피할 수 없는 경우가 많습니다.

클라우드 서비스는 엔터프라이즈 사용 사례의 경우에도 그들 자체가 하이퍼스케일 서비스이므로 신뢰성에 대한 책임의 변화로부터 이점을 얻을 수 있습니다. 스토리지, 네트워킹 또는 모니터링과 같은 기본 클라우드 서비스는 "서비스로서(as a service)" 제공되므로 하드웨어 장애가 이러한 서비스 내에서 처리되며 고객에게는 보이지 않습니다. 마찬가지로 분석이나 데이터베이스 백업과 같은 상위 수준 서비스도 서버리스로 실행되므로 고객은 기계 장애에 대해 걱정할 필요가 없습니다. 심지어 가상 머신 자체도 기본 하드웨어보다 더 신뢰할 수 있습니다. 예를 들어 서버가 수정 불가능한 DRAM 오류를 보이기 시작했지만 이러한 오류가 특정 VM에 아직 영향을 미치지 않은 경우, 클라우드 제공업체는 VM을 새 서버로 투명하게 마이그레이션한 다음 결함이 있는 서버를 수리할 수 있습니다.

소프트웨어 계층에서 장애를 견딜 수 있는 시스템은 하드웨어 계층이 결함을 감지하고 적시에 소프트웨어에 보고해야 합니다. 하드웨어가 모든 결함을 투명하게 수정할 필요는 없습니다. 그러나 합리적인 비용이나 복잡성으로 하드웨어 오류 수정(또는 더 나은 오류 감지)이 가능한 경우 이를 지원하는 것이 좋습니다. 예를 들어 DRAM은 합리적인 추가 비용으로 강력한 오류 수정 기능을 제공하며, 하드웨어 DRAM 오류 감지를 소프트웨어로 대체하기는 어려울 것입니다.

하드웨어 오류가 감지되어야 한다는 요구사항을 완화하려면 모든 소프트웨어 구성 요소가 자신의 올바른 실행을 확인해야 하는 부담을 져야 합니다. 일반적으로 추가된 소프트웨어 복잡성은 하드웨어 오류 감지 비용을 크게 초과합니다.

예를 들어 초기 Google 서버에는 DRAM 패리티 검사가 없었습니다. 웹 검색 색인을 생성하는 것은 본질적으로 장기간에 걸쳐 여러 기계를 사용하여 매우 큰 셔플/병합 정렬 작업을 수행하는 것으로 구성됩니다. 2000년에 Google 웹 색인에 대한 월간 업데이트 중 하나가 출시 전 검사에서 실패했는데, 테스트된 쿼리의 일부가 겉보기에는 무작위 문서들을 반환하는 것이 발견되었기 때문입니다. 조사 끝에 새 색인 파일에서 데이터 구조의 일관된 위치에 비트가 0으로 고정된 패턴이 발견되었습니다. 이는 결함이 있는 DRAM 칩을 통해 많은 데이터를 스트리밍한 나쁜 부작용이었습니다. 이 문제의 재발 가능성을 최소화하기 위해 색인 데이터 구조에 일관성 검사가 추가되었으며 다행히도 이러한 성격의 추가 문제는 보고되지 않았습니다. Google의 차세대 기계에는 메모리 패리티 감지 기능이 포함되었습니다. ECC 메모리 가격이 경쟁력 있는 수준으로 떨어지자 이후 모든 세대에는 ECC DRAM이 사용되었습니다.

그러나 많은 WSC 규모 시스템에는 여전히 소프트웨어 오류 검사가 포함되어 있으며 이는 필수적입니다. 하드웨어 오류 감지가 충분히 강력하지 않을 수 있기 때문입니다. 예를 들어 이더넷 프레임에는 32비트 CRC 필드가 있고 TCP는 그 위에 16비트 체크섬을 추가하지만, 초당 페타비트로 실행되는 네트워크에서는 일부 손상된 패킷이 이러한 하드웨어 검사로 감지되지 않을 것입니다. 2010년 AWS S3 스토리지 시스템의 글로벌 중단을 일으킨 것은 감지되지 않은 비트 오류였습니다[6]. CPU의 침묵 데이터 손상(Silent data corruption)(아래 10.1.10절 참조)은 WSC 설계자들에게 또 다른 골칫거리를 제공하며, 인텔 창립자 앤디 그로브(Andy Grove)의 좌우명인 "편집광만이 살아남는다(only the paranoid survive)"에 새로운 의미를 부여합니다.

### 10.1.3 결함 분류 (Categorizing faults)

효율적인 결함 감내 소프트웨어 계층은 결함 원인, 통계적 특성 및 해당 복구 동작에 대한 일련의 예상을 기반으로 해야 합니다. 이러한 예상 없이 개발된 소프트웨어는 두 가지 위험에 시달릴 수 있습니다. 근본적인 결함이 과소평가될 경우 중단되기 쉽거나, 결함이 실제보다 훨씬 더 빈번하다고 가정될 경우 과도한 오버프로비저닝을 요구하게 됩니다.

WSC 시스템에서 결함에 대한 정확한 정량적 평가를 제공하는 것은 배포 환경마다 장비와 소프트웨어 인프라가 다양하기 때문에 어렵습니다. 대신 공개적으로 이용 가능한 소스와 우리 자신의 경험을 바탕으로 높은 수준의 트렌드를 요약해 보겠습니다.

#### 10.1.3.1 신뢰성(Reliability) 대 가용성(Availability)

신뢰성과 가용성은 결함 감내 시스템의 기본이 되는 뚜렷하지만 서로 얽혀 있는 시스템 특성입니다. 둘 다 전체 시스템의 견고성에 기여하지만, 시스템 운영 프로필의 다른 측면을 정량화하므로 관련이 있지만 별개의 척도입니다. 신뢰성은 특정 기간 동안 고장 없는 운영 확률을 정량화하는 반면, 가용성은 임의의 시점에 시스템이 작동할 확률을 정량화합니다.

신뢰성은 정의된 운영 조건에서 주어진 시간 간격 내에 시스템이 지정된 기능을 실패 없이 수행할 확률로 공식 정의됩니다. 정량적으로 신뢰성은 종종 평균 고장 간격(MTBF)과 고장률(Failure Rate)을 사용하여 특성화됩니다. 연속적인 고장 사이의 평균 경과 시간으로 계산되는 MTBF는 고장 전 시스템의 운영 수명에 대한 통계적 기대를 제공합니다. 반대로 고장률은 단위 시간당 예상되는 고장 횟수를 나타냅니다. 높은 신뢰성을 나타내는 시스템은 높은 MTBF와 그에 상응하는 낮은 고장률을 보여줄 것입니다.

반면 가용성은 정의된 기간 동안 주어진 시점에 시스템이 의도한 기능을 수행하기 위해 작동하고 접근 가능할 확률을 측정합니다. 이 지표는 시스템이 기능적 상태에 있고 사용할 준비가 된 시간의 비율을 정량화합니다. 가용성은 주로 시스템의 다운타임(downtime)에 영향을 받으며, 이는 중단의 빈도와 지속 시간을 모두 포함합니다. 가용성과 관련된 지표에는 일반적으로 "nines"로 표현되는 가용성 백분율(예: "five nines" = 99.999%)과 평균 복구 시간(MTTR)이 포함됩니다. MTTR은 고장 후 시스템을 운영 상태로 복원하는 데 필요한 평균 시간을 나타냅니다. 따라서 고가용성 시스템은 고장 예방(신뢰성에 기여)과 빠른 복구 메커니즘(MTTR 최소화)을 결합하여 다운타임을 최소화합니다.

시스템은 비교적 낮은 신뢰성을 가지고도 높은 가용성을 보여줄 수 있습니다. 이 시나리오는 시스템이 잦은 고장을 겪지만 신속한 복구 메커니즘을 보유하여 다운타임을 최소화할 때 발생합니다.

시스템 설계에서 신뢰성 또는 가용성을 강조하는 선택은 애플리케이션의 중요도에 따라 달라집니다. 항공우주 또는 의료 장비 제어 시스템과 같이 고장이 치명적인 결과를 초래하는 시스템에서는 높은 신뢰성이 가장 중요합니다. 그러나 클라우드 인프라나 통신 네트워크와 같이 일시적 또는 부분적 서비스 중단은 허용되지만 장기간의 중단은 바람직하지 않은 시나리오에는 고가용성이 적합합니다.

신뢰성과 가용성은 동일하지 않기 때문에 99.99%의 가용성은 매우 다른 경험을 나타낼 수 있습니다. 이는 1년에 한 번 약 1시간 지속되는 단일 중단이 있는 시스템, 또는 매주 1분간의 중단이 있는 시스템을 나타낼 수도 있고, 전체 중단은 없지만 항상 요청의 0.01% 처리에 실패하는 시스템을 나타낼 수도 있습니다.

엔터프라이즈 고객의 신뢰성 및 가용성 요구사항은 다릅니다. 소비자 서비스의 경우 모든 사용자가 비슷하므로 단일 평균 지표로 충분한 경우가 많습니다. 엔터프라이즈 서비스의 경우 일부 고객은 다른 고객보다 천 배 더 중요할 수 있습니다(매출, 사용량, 브랜드 인지도 등으로 측정). "고객의 99.99%가 아무런 문제를 겪지 않고 있다"는 상태는 좋을 수도 있고, 문제가 가장 중요한 고객에게 집중되어 있다면 끔찍할 수도 있습니다. 또한 대부분의 엔터프라이즈 고객은 클라우드 제공업체에 대한 고신뢰성 경로를 구축하므로 엔드 투 엔드 신뢰성은 실제 서비스 신뢰성에 의해 지배되는 반면, 소비자의 경우 여전히 ISP 신뢰성에 의해 지배될 수 있습니다.

인터넷 서비스의 경우 인터넷 자체의 가용성 특성이 제한적이기 때문에 완벽한 가용성은 현실적인 목표가 아닙니다. 2001년에 Chandra 등[7]은 라우팅 문제를 포함한 다양한 연결 문제로 인해 인터넷 엔드포인트가 시간의 1%에서 2% 사이로 서로 도달하지 못할 수 있다고 보고했습니다. 이는 "two nines(99%)" 미만의 가용성으로 해석됩니다. 즉, 2001년 시대의 인터넷 서비스가 완벽하게 신뢰할 수 있었다 하더라도 사용자는 평균적으로 99.0% 이하의 가용성을 가진 것으로 인식했을 것입니다. 그러나 시간이 지남에 따라 인터넷 신뢰성은 향상되었습니다. 2014년 Google 측정 결과(Chrome 사용자 관점에서 볼 때) Google 서버에 대한 엔드 투 엔드 가용성은 99.6%-99.9%였지만 ISP별로 편차가 컸습니다. 전 세계 일부 지역은 덜 견고한 인프라와 의도적인 차단으로 인해 여전히 훨씬 낮은 가용성을 경험하고 있습니다[8]. 2024년의 엔드 투 엔드 가용성은 선진국 ISP 전체에서 99.95% 이상, 개발도상국에서는 99.8-99.9%로 향상되었습니다. 이 수치들은 크지만 드문 중단을 포함하지 않으므로 실제 가용성을 과대평가할 수 있습니다. 예를 들어 2024년 5월 12일 이중 케이블 절단으로 인해 케냐, 탄자니아, 르완다, 우간다를 포함한 여러 국가에서 장기 중단이 발생했습니다.

신뢰성 및 가용성 지표 외에도 두 가지 다른 지표가 중단 복구에 중점을 둡니다. 목표 복구 시간(Recovery Time Objective, RTO)은 시스템이 오프라인 상태일 수 있는 최대 허용 시간으로, 서비스가 얼마나 빨리 복구되어야 하는지를 나타냅니다. 이는 "얼마나 오랫동안 다운될 수 있는가?"에 답합니다. 반면 목표 복구 시점(Recovery Point Objective, RPO)은 최대 허용 데이터 손실량을 의미하며, 마지막 양호한 백업과 중단 발생 상태 사이의 시간으로 측정됩니다. RPO는 백업 빈도를 나타냅니다. 본질적으로 RTO는 다운타임에 대한 허용 오차에 관한 것이고, RPO는 데이터 손실에 대한 허용 오차에 관한 것입니다.

#### 10.1.3.2 심각도 (Severity)

하드웨어 또는 소프트웨어 결함은 인터넷 서비스에 다양한 정도로 영향을 미쳐 서로 다른 서비스 수준의 실패 모드를 초래할 수 있습니다. 가장 심각한 모드는 높은 신뢰성 수준을 요구할 수 있는 반면, 가장 덜 해로운 모드는 덜 비싼 솔루션으로 달성할 수 있는 더 완화된 요구사항을 가질 수 있습니다. 우리는 서비스 수준 장애를 심각도 감소 순으로 다음과 같은 범주로 광범위하게 분류합니다:

*   **손상됨(Corrupted):** 커밋된 데이터가 손실되거나 손상되어 재생성이 불가능함.
*   **도달 불가능(Unreachable):** 서비스가 다운되거나 사용자가 도달할 수 없음.
*   **저하됨(Degraded):** 서비스는 사용 가능하지만 저하된 모드임(예: 일부 기능을 사용할 수 없음).
*   **가려짐(Masked):** 결함이 발생하지만 결함 감내 소프트웨어 및 하드웨어 메커니즘에 의해 사용자에게 숨겨짐.

견고성의 허용 수준은 이러한 범주에 따라 다릅니다. 우리는 잘 설계된 결함 감내 인프라에 의해 대부분의 결함이 가려져(masked) 서비스 제공업체 외부에서는 사실상 보이지 않을 것으로 예상합니다. 가려진 결함이 서비스의 최대 지속 가능 처리량 용량에 영향을 줄 수는 있지만, 신중한 수준의 오버프로비저닝을 통해 서비스가 건전한 상태를 유지하도록 보장할 수 있습니다.

결함이 완전히 가려질 수 없다면, 가장 덜 심각한 징후는 서비스 품질이 약간 저하되는 것입니다. 여기서 다른 서비스들은 다른 방식으로 저하된 가용성을 도입할 수 있습니다. 이러한 저하된 서비스의 한 예는 웹 검색 시스템이 처리량을 향상시키기 위해 데이터 파티셔닝 기술을 사용하지만 데이터베이스의 일부를 제공하는 일부 시스템을 잃는 경우입니다[9]. 검색 쿼리 결과는 불완전하겠지만 많은 경우 여전히 허용 가능할 것입니다. 결함으로 인한 우아한 저하(graceful degradation)는 최신성(freshness) 감소로 나타날 수도 있습니다. 예를 들어 사용자가 이메일 계정에 액세스할 수는 있지만 새 이메일 배달이 몇 분 지연되거나 사서함의 일부 조각이 일시적으로 누락될 수 있습니다. 이러한 종류의 결함도 최소화해야 하지만 완전한 비가용성보다는 덜 심각합니다. 인터넷 서비스는 우아하게 저하된 서비스를 위한 이러한 기회를 활용하도록 의도적으로 설계되어야 합니다. 즉, 이 지원은 종종 애플리케이션에 특화되어 있으며 클러스터 인프라 소프트웨어 계층 내에 쉽게 숨겨지지 않습니다.

특히 피해가 큰 장애 클래스 중 하나는 중요 데이터, 특히 사용자 데이터, 중요 운영 로그 또는 재생성하기 어렵거나 불가능한 관련 데이터에 대한 커밋된 업데이트의 손실 또는 손상입니다. 데이터 내구성(durability)에 대한 소비자와 기업의 기대치는 높습니다. 서비스가 모든 사용자에게 완벽하게 가용되는 것보다 데이터를 잃지 않는 것이 훨씬 더 중요합니다. 동시에 일부 데이터는 덜 중요합니다. 예를 들어 웹의 복사본과 해당 인덱스 파일은 검색 엔진에 방대하고 중요한 데이터이지만, 궁극적으로 손실된 파티션을 다시 크롤링하고 인덱스 파일을 다시 계산하여 재생성할 수 있습니다. 따라서 스토리지 내구성을 위한 최적의 설계 지점은 기본 데이터의 가치에 따라 시스템마다 다를 것입니다.

#### 10.1.3.3 근본 원인 (Root causes)

WSC에서는 중단이나 기타 심각한 서비스 수준 중단과 같이 전체 시스템의 건강에 영향을 미칠 가능성 측면에서 결함을 이해하는 것이 유용합니다. Oppenheimer 등[10]의 고전적인 2003년 논문은 각각 500대 이상의 서버로 구성된 세 가지 인터넷 서비스를 연구하고 서비스 수준 장애의 가장 일반적인 원인을 식별하려고 노력했습니다. 그들은 운영자 원인 또는 구성 오류가 서비스 수준 장애의 가장 큰 원인이며, 하드웨어 관련 결함(서버 또는 네트워킹)은 전체 장애 이벤트의 10~25%에 기여한다고 결론지었습니다.

Oppenheimer의 결과는 Gray[11]의 중요한 연구와 다소 일치하는데, Gray는 인터넷 서비스를 살펴보지는 않았지만 1985년부터 1990년 사이의 고도로 결함 감내적인 Tandem 서버의 현장 데이터를 조사했습니다. Gray 또한 하드웨어 결함이 전체 중단의 작은 부분(10% 미만)을 차지한다는 것을 발견했습니다. 소프트웨어 결함(~60%)과 유지 보수 및 운영 결함(~20%)이 중단 통계를 지배했습니다.

더 최근의 연구들도 유사한 결론에 도달했습니다. 장애는 주로 소프트웨어와 관련이 있으며 하드웨어 장애는 비교적 드뭅니다. 예를 들어 2010년부터 2017년 사이의 Google 사후 분석에 따르면 중단의 상위 두 가지 트리거는 구성 및 바이너리 푸시로 모든 중단의 68%를 차지했습니다[2]. 상위 근본 원인은 소프트웨어 버그(41%), 개발 프로세스 실패(예: 테스트 누락, 20%), 복잡한 시스템 동작(시스템 간의 예상치 못한 상호 작용, 즉 "심층(deep)" 버그, 17%)이었습니다.

처음에는 이 두 가지 매우 다른 시스템에서 하드웨어 결함이 중단 이벤트에 거의 기여하지 않는다는 사실이 놀랍습니다. 이러한 수치는 이 시스템들의 하드웨어 구성 요소의 기본 신뢰성에 대한 진술이라기보다는, 결함 감내 기술이 구성 요소 장애가 상위 수준 시스템 동작에 영향을 미치는 것을 방지하는 데 얼마나 성공적이었는지를 나타냅니다. Tandem의 경우 이러한 기술은 주로 하드웨어에서 구현된 반면, 클라우드 서비스에서는 결함 감내 소프트웨어 인프라의 품질 덕분이라고 할 수 있습니다. 소프트웨어 기반이든 하드웨어 기반이든 결함 감내 기술은 결함이 통계적으로 크게 독립적일 때 특히 잘 작동하며, 이는 하드웨어 결함의 경우(항상은 아니지만) 자주 그렇습니다. 소프트웨어, 운영자 및 유지 보수로 인한 결함은 한 번에 여러 시스템에 영향을 미칠 가능성이 더 높으므로 극복하기 훨씬 더 어려운 상관된 장애 시나리오를 생성하기 때문에 중단에 큰 영향을 미칩니다.

하드웨어 장비 장애 이외의 요인이 서비스 수준 중단을 지배하는 이유는 알려진 하드웨어 장애 패턴을 견디도록 서비스를 설계하는 것이 일반적인 소프트웨어 버그나 운영자 실수에 대해 탄력적으로 만드는 것보다 쉽기 때문입니다. 39,000개 이상의 스토리지 시스템 데이터를 기반으로 한 Jiang 등[12]의 연구는 디스크 장애가 사실 스토리지 시스템 장애의 지배적인 원인이 아니라고 결론지었습니다. 이 결과는 분산 스토리지 시스템 가용성에 대한 Ford 등[13]의 분석과 일치합니다. Google의 Colossus 분산 파일 시스템 데이터를 사용하여 수행된 이 연구에서 계획된 스토리지 노드 재부팅 이벤트는 노드 수준 비가용성의 지배적인 원인입니다. 이 연구는 또한 상관된 장애(짧은 시간 창 내에 여러 스토리지 노드에서 발생하는 장애)를 이해하는 것의 중요성을 강조합니다. 상관 관계를 고려하지 않는 모델은 노드 장애의 영향을 수십 배 과소평가할 수 있기 때문입니다. 15년 후에도 이 연구의 통찰력은 여전히 유효합니다.

### 10.1.4 기계 수준 장애 (Machine-level failures)

#### 10.1.4.1 재시작과 다운타임

결함 감내 분산 시스템을 설계하려면 서버 수준에서의 가용성을 이해해야 합니다. 먼저 원인(계획된 유지 보수, 운영 체제 버그 또는 하드웨어 장애 등)에 관계없이 전체 다운타임을 고려해 봅시다.

Google의 기계 수준 장애 및 다운타임 통계는 그림 10.1과 그림 10.2에 요약되어 있습니다. 데이터는 모든 기계 재시작 이벤트와 해당 다운타임에 대한 6개월간의 관찰을 기반으로 하며, 다운타임은 원인에 관계없이 기계가 서비스에 사용할 수 없는 전체 시간 간격에 해당합니다. 이 통계는 Google의 모든 기계를 포함합니다. 예를 들어 수리 파이프라인에 있는 기계, 업그레이드를 위한 계획된 다운타임 및 모든 종류의 기계 충돌(crash)을 포함합니다.

그림 10.1은 2018년 기계 재시작 이벤트의 분포를 보여줍니다. 50% 이상의 기계가 평균적으로 한 달에 한 번 이상 재시작합니다. 꼬리 부분은 상대적으로 깁니다(그림은 11회 이상의 재시작에서 데이터를 자릅니다). 이는 Google의 방대한 기계 모집단 때문입니다. 전체 기계의 약 5%는 일주일에 한 번 이상 재시작합니다.

그러나 이러한 대규모 평균화에는 여러 효과가 섞여 있습니다. 예를 들어, 우리는 일반적으로 새로운 서버 제품 도입 후 첫 몇 달 동안 평소보다 높은 장애율을 목격합니다. 원인에는 제조 초기 효과, 펌웨어 및 커널 버그, 대규모 시스템이 사용된 후에만 눈에 띄게 되는 간헐적 하드웨어 문제 등이 포함됩니다. 마찬가지로 새 기계의 설치 및 "초기 적응(burn-in)" 단계에는 여러 번의 재부팅이 포함됩니다.

> **그림 10.1** 2018년 6개월 동안 Google에서의 기계 재시작

> **그림 10.2** 6개월 동안 관찰된 기계 다운타임

비운영(non-production) 기계를 분석에서 제외하면 평균 재시작 속도는 재시작 간 약 1개월로 변경됩니다. 이는 주로 버그 해결, 기능 향상 및 보안 관련 업그레이드를 지원하는 Google의 펌웨어 및 커널 릴리스 프로세스에 의해 주도됩니다. 또한 빈번하게 재시작하는 기계는 오랫동안 활성 서비스 상태에 있을 가능성이 적다는 점도 주목합니다.

2018년에 수행된 또 다른 내부 연구에서는 계획된 서버 비가용성 이벤트를 전체 기계 비가용성에서 제외했습니다. 주로 서버 충돌이나 네트워크 도달 불가로 인한 나머지 비가용성은 서버가 평균적으로 수년 동안 가동 상태를 유지할 수 있음을 나타냅니다. 이 데이터는 대부분의 재시작이 소프트웨어 및 하드웨어 업그레이드와 같은 계획된 이벤트 때문이라는 직관을 확인시켜 줍니다.

잦은 업그레이드는 커널을 최신 상태로 유지하고 Google이 긴급한 보안 문제에 신중하게 대응할 수 있도록 하는 데 필요합니다. 예를 들어 2018년 Spectre 및 Meltdown CPU 보안 결함이 발견되었을 때, Google은 고객 조치 없이 CPU 마이크로코드를 업데이트할 수 있었고, 취약점이 공개될 때쯤에는 모든 고객이 이미 안전한 상태였습니다.

앞서 논의한 바와 같이, Google Cloud의 Compute Engine은 VM을 재부팅할 필요 없이 실행 중인 인스턴스를 동일한 영역(zone)의 다른 호스트로 마이그레이션하여 가상 머신(VM) 인스턴스를 계속 실행할 수 있도록 하는 라이브 마이그레이션을 제공합니다. 결과적으로 물리적 호스트 서버의 재부팅은 고객에게 거의 보이지 않습니다. 그러나 라이브 마이그레이션은 지연 시간에 매우 민감한 서비스를 사용하는 고객이나 베어 메탈 인스턴스를 사용할 때는 잘 작동하지 않습니다. CPU 마이크로코드 업그레이드와 같은 일부 유지 보수는 고객에게 숨길 수 없지만, 고객 지정 유지 보수 기간(중단이 더 편리한 시간에 발생하도록 함)을 통해, VM 실행에 필요한 코드를 줄여(따라서 업데이트 필요성 감소), 해당 코드를 메인 CPU에서 특수 프로세서로 이동하고, 무중단 업그레이드를 지원하도록 소프트웨어를 설계함으로써 공급자 일정에 따른 중단의 영향과 빈도를 줄일 수 있습니다.

재시작 통계는 결함 감내 소프트웨어 시스템 설계의 핵심 매개변수이지만, Berkeley ROC 프로젝트[14]에서 일찍이 언급했듯이 가용성 그림은 다운타임 데이터와 결합될 때 비로소 완성됩니다. 그림 10.2는 동일한 Google 서버 모집단에서 2018년 다운타임 분포를 보여줍니다. 모든 기계에 걸친 평균 연간 재시작 속도는 12.4회로, 재시작 간 평균 시간이 한 달 미만에 해당합니다. x축은 밀도 및 누적 기계 분포 모두에 대해 다운타임을 표시합니다. 데이터에는 계획된 재부팅과 기타 하드웨어 및 소프트웨어 장애로 인한 재부팅이 모두 포함됩니다. 다운타임에는 기계가 작동을 멈춘 시점부터 재부팅되고 기본 노드 서비스가 다시 시작될 때까지의 모든 시간이 포함됩니다. 즉, 다운타임 간격은 기계가 재부팅을 마칠 때가 아니라 기계가 다시 부하를 처리할 수 있게 될 때 끝납니다.

모든 재시작 이벤트의 약 55%는 3분 미만으로 지속되며, 25%는 3분에서 30분 사이이며, 나머지 재시작의 대부분은 약 하루 만에 완료됩니다. 긴 중단은 일반적으로 물리적 수리와 충돌로 인한 자동 파일 시스템 복구의 조합에 해당합니다. 모든 재시작 이벤트의 약 1%는 하루 이상 지속되며, 일반적으로 수리에 들어가는 시스템에 해당합니다. 평균 다운타임은 10분을 조금 넘습니다. 결과적인 평균 기계 가용성은 99.93%입니다.

#### 10.1.4.2 기계 신뢰성 (Machine reliability)

서버 가용성에 대해 발표된 현장 데이터는 상대적으로 적습니다. Kalyanakrishnan 등[15]의 1999년 연구에 따르면 메일 라우팅 애플리케이션에 관련된 Windows NT 기계의 MTTF는 약 22일, 즉 연간 기계 고장률이 1,600% 이상이었습니다. 2007년 Schroeder와 Gibson[16]은 로스앨러모스 국립 연구소(Los Alamos National Laboratory)의 고성능 컴퓨팅 시스템에서 고장 통계를 연구한 결과 CPU당 연간 0.3건의 고장률을 발견했습니다. 듀얼 소켓 서버의 경우 이는 연간 기계 고장률 60% 또는 약 20개월의 MTTF를 의미합니다. 이 서버 고장률은 Kalyanakrishnan이 관찰한 것보다 25배 이상 낮습니다.

더 최근의 데이터는 성숙한 서버(초기 사망률을 생존한 서버)의 충돌률(crash rate)을 연간 고장률 5% 미만으로 편안하게 낮춥니다. 그림 10.3은 2024년 6개월 동안 Google 기계에서의 고장 이벤트 분포를 보여줍니다(Y축은 로그 스케일). 모든 기계의 97% 이상이 충돌을 경험하지 않았고, 2%는 한 번의 충돌을 경험했으며, 꼬리 부분의 소수 기계 집단이 여러 번의 충돌을 경험했습니다. 분명히 비용 효율적인 서버의 단일 기계 신뢰성은 1999년 연간 고장률 1000% 이상에서 2009년 60%, 오늘날 5% 미만으로 큰 진전을 이루었습니다. 이러한 추세는 우연이 아닌데, 서로 다른 신뢰성 요구사항을 가진 퍼블릭 클라우드 서비스의 도래가 단일 기계 신뢰성에 대한 관심을 증가시켰기 때문입니다.

> **그림 10.3** 2024년 6개월 동안 Google에서의 기계 충돌 분포

그러나 이러한 신뢰성 향상은 여전히 규모를 당해낼 수 없습니다. 연간 5%의 서버 충돌률(또는 19.4년의 서버 MTBF)을 감안할 때, 2,000대의 서버를 사용하는 서비스는 약 3-4일마다 기계 충돌에 대비해야 합니다.

시스템이 실패하는 서버에서 VM을 마이그레이션할 수 있는 경우 VM의 신뢰성은 기본 하드웨어의 신뢰성을 크게 초과할 수 있습니다. 특히 많은 하드웨어 장애는 빈번한 패리티 오류가 실제 데이터 손실 이벤트를 예고할 수 있는 메모리 장애입니다. 메모리가 실제로 복구 불가능한 오류로 실패하더라도, 실패하는 메모리 셀에 매핑된 메모리가 없는 VM은 정상 기계로 마이그레이션될 수 있습니다. 결과적으로 GCE VM은 정기적으로 평균 이론적 수명이 10년 이상에 도달합니다. 즉, VM이 고객에 의해 방해받지 않는다면 실패를 경험하지 않고 10년 이상 실행될 것입니다. 실제로 대부분의 VM은 고객이 게스트 커널을 업그레이드하거나 애플리케이션 변경을 위해 VM을 다시 시작하기 때문에 훨씬 일찍 재시작됩니다.

#### 10.1.4.3 기계 충돌의 원인

개별 기계 충돌의 원인을 고려할 때 하드웨어 때문인지 소프트웨어 때문인지 묻고 싶을 수 있습니다. 확실히 일부 운영 체제 버그는 기계 충돌을 일으킬 수 있으며 이는 엄격히 소프트웨어 영역에 속합니다. 그러나 최신 하드웨어에는 이 구분을 모호하게 만드는 대량의 펌웨어와 마이크로코드가 포함되어 있으며, 실리콘에 새겨진 결함조차도 종종 소프트웨어 업데이트가 사용을 제어할 수 있도록 하는 로직에 의해 보호됩니다.

우리는 기계 충돌이 개별 하드웨어 유닛에 특정한 동작에 의해 발생하는지 아니면 하드웨어 전체에 퍼진 동작에 의해 발생하는지 구분하는 것이 더 유용하다고 생각합니다. 전자의 경우 원인은 일반적으로 제조 결함이나 수명 말기 마모이며, 적절한 대응은 구성 요소를 교체하여 기계를 수리하는 것입니다. 후자의 경우 동일한 구성 요소로 교체해도 도움이 되지 않으며, 대신 식별하고 수정해야 하는 버그를 다루고 있는 것입니다.

기계 충돌의 범인을 확실하게 식별하는 것은 일반적으로 어렵습니다. 많은 상황에서 일시적인 하드웨어 오류는 운영 체제나 펌웨어 버그와 구별하기 어렵기 때문입니다. 우리는 사양을 벗어난 동작을 식별하기 위해 구성 요소 수준의 진단에 의존합니다.

이러한 진단은 지난 10년 동안 크게 향상되었으며 이는 두 가지 이유로 관련이 있습니다. 첫째, 향상된 진단을 통해 수리 파이프라인이 결함이 있는 구성 요소를 더 높은 정확도로 식별하고 교체할 수 있게 되어 반복되는 실패가 줄어들고 전반적인 실패율이 낮아집니다. 둘째, 향상된 진단을 통해 Google 전체에서 기계 충돌률의 다음과 같은 세 가지 추세를 식별할 수 있게 되었습니다.

역사적으로 메모리 결함은 기계 충돌의 가장 일반적인 이유였습니다. 결함은 보통 FIT(Failure In Time)를 사용하여 특성화되며, 이는 10억($10^9$) 시간마다 발생하는 많은 고장을 나타내는 단위입니다. 2009년 연구[17]는 Google 서버 모집단에 대한 DRAM 오류를 평가했으며 여러 DIMM 기술 전반에 걸쳐 이전에 보고된 것보다 상당히 높은 FIT(25,000에서 75,000 사이) 비율을 발견했습니다. 이는 연간 Google 기계의 약 3분의 1에 영향을 미치는 수정 가능한 메모리 오류로 해석됩니다. 그러나 ECC 보호 덕분에 전체 기계의 약 1.3%만이 연간 수정 불가능한 메모리 오류를 경험합니다. 후속 2012년 연구[18]는 DRAM 오류의 상당 부분이 하드(비일시적) 오류에 기인할 수 있음을 발견했으며, 간단한 페이지 은퇴(retirement) 정책이 시스템의 전체 DRAM 중 무시할 수 있는 부분만을 희생하면서 생산 시스템의 DRAM 오류의 큰 부분을 가릴 수 있음을 시안했습니다.

지난 10년 동안 DRAM은 더 신뢰할 수 있게 되었습니다. 2023년 연구[19]는 Google의 여러 데이터 센터에 있는 수만 대의 서버에서 데이터를 수집했습니다. 서버 DRAM은 칩킬(chipkill) ECC[20]를 사용했으며 장치당 70-100의 FIT 비율을 관찰했는데, 이는 몇 년 전 비율보다 훨씬 개선된 것입니다. 흥미롭게도 우주 방사선이 DRAM을 때리고 비트의 전하 수준을 뒤집어서 발생하는 일시적인 오류가 전체 오류의 약 3분의 1을 차지했습니다.

동시에 기계의 운영 체제 부트 디스크를 호스팅하는 부트 드라이브로 인한 장애도 극적으로 감소했습니다. 원래 디스크 드라이브에 구축되어 데이터 저장소와 함께 있었던 부트 디스크 손실은 기계 충돌의 큰 원인이었습니다. 이 아키텍처를 계속 사용하는 대신 작고 가볍게 사용되는 SSD로 구축된 "스토리지 없는(storage-less)" 컴퓨팅 서버로의 전환은 이 종류의 장애를 거의 제거했습니다.

이러한 장애 원인이 해결됨에 따라 처리 장치(CPU 및 오프로드 프로세서)의 장애 비율이 증가하기 시작했습니다. 이에 대응하여 개별 수리를 지시하는 진단의 개선과 부하 테스트 및 대규모 디버깅의 개선으로 처리 장치 고장률을 억제해 왔습니다.

남은 것은 균형 잡힌 장애 분해로, CPU 장애, RAM 장애, NIC 및 오프로드 프로세서 장애, 운영 체제 장애가 대략 동등하게 기여하는 것을 볼 수 있습니다. 모든 장애를 진단할 수 있는 것은 아니며 이는 여전히 개선이 필요한 영역입니다. 우리는 이 균형이 과도한 장애 원인이 점진적으로 감소한 성숙한 기술 생태계를 반영한다고 생각합니다.

ML 학습 워크로드는 클라우드 워크로드보다 장애에 훨씬 더 민감합니다. 실패하는 기계가 전체 학습 실행을 중단시키는 고도로 동기화된 계산을 포함하기 때문입니다. 자동화된 장애 처리가 있더라도 이러한 장애는 사용 가능한 처리량에 영향을 미칩니다. 예를 들어 54일간의 Llama3 학습 실행 중 16K-GPU 클러스터는 약 320회의 하드웨어 장애를 겪어 학습 실행 처리량이 약 10% 감소했습니다[4].

### 10.1.5 스토리지 오류 (Storage errors)

스토리지 장치는 완전히 실패하거나(전체 장치를 사용할 수 없게 됨) 부분적으로 실패(일부 데이터를 더 이상 읽을 수 없음)할 수 있습니다. NetApp과 위스콘신 대학[21], 카네기 멜론[22], 그리고 Google[23]의 데이터를 기반으로 한 세 가지 고전적인 2007년 연구는 디스크 드라이브의 장애 특성을 조사했습니다. 이러한 대규모 현장 연구에서 하드 장애율(교체된 구성 요소의 연간 비율로 측정)은 2%에서 4% 사이였습니다. Bairavasundaram 등[21]은 구체적으로 잠재 섹터 오류(데이터 손상 빈도의 척도)의 비율을 살펴보았습니다. 150만 개 이상의 드라이브 모집단에서 그들은 전체 드라이브의 3.5% 미만이 32개월 동안 어떤 오류라도 발생시킨다는 것을 관찰했습니다.

주변 온도가 디스크 드라이브의 신뢰성에 미치는 영향은 Pinheiro 등[23]과 El Sayed 등[24]에 의해 연구되었습니다. 이 두 현장 연구는 디스크 온도가 45°C 미만인 한 대부분의 디스크 오류가 온도와 상관관계가 없는 것처럼 보인다고 제안했습니다. 그러나 최신 디스크와 SSD는 온도에 더 민감한 것으로 보입니다.

SSD는 원시 비트(raw bit) 저장 계층에서는 상대적으로 신뢰할 수 없지만 광범위한 오류 감지 및 수정 로직을 사용하므로 블록 계층에서는 매우 신뢰할 수 있습니다. 그러나 SSD 신뢰성과 내구성은 두 가지 근본적인 한계에 의해 위협받습니다. 첫째, SSD 비트는 제한된 횟수만 쓸 수 있습니다. 새 데이터를 쓰기 전에 파괴적인 블록 삭제(erase) 작업이 비트를 재설정하고, 고전압 "프로그램" 주기가 새 비트를 설정합니다. 각 프로그램/삭제(P/E) 주기는 저장 매체를 약간 저하시키므로 일정 횟수의 주기 후에는 원시 비트 오류율이 증가하기 시작합니다(즉, 새 쓰기가 모든 비트를 올바르게 설정하지 못할 수 있음)[25]. SSD 삭제 블록은 1~100MB로 꽤 큽니다. 전체 블록보다 작은 쓰기를 허용하기 위해 각 블록은 최소 4-16KB 크기의 페이지로 세분화됩니다.

이 저하가 중요해지기 전의 P/E 주기 횟수는 SSD 수명 또는 내구도(endurance)[26]입니다. 대체 지표는 드라이브 일일 쓰기(DWPD), 즉 주어진 보증 기간 동안 드라이브가 지속되려면 매일 각 바이트를 몇 번 쓸 수 있는가입니다. 불행히도 내구도는 밀도에 따라 저하됩니다. 저용량(SLC) SSD는 10,000회 이상의 삭제 주기를 제공하지만 최고 용량 SSD(현재 TLC 및 QLC)는 각각 약 3,000회 및 1,500회 P/E 주기를 제공합니다. 따라서 4TB TLC 드라이브가 하루에 12TB의 쓰기 트래픽(140MB/sec), 즉 3 DWPD를 받으면 1000일 후에 실패하기 시작합니다. 다행히 고밀도 SSD는 더 많은 바이트를 포함하므로 내구도가 1,500주기인 8TB SSD는 내구도가 3,000주기인 4TB SSD와 동일한 평균 쓰기 속도를 흡수할 수 있습니다. 따라서 실제로는 내구도가 해결 가능한 문제입니다[27], [28], [29].

쓰기 증폭(Write amplification)은 SSD에 대한 물리적 쓰기가 논리적 쓰기보다 클 때 발생합니다. SSD는 페이지 단위로만 다시 쓸 수 있으므로 가장 작은 물리적 쓰기는 페이지 쓰기입니다. 애플리케이션이 SSD에 저장된 파일의 단일 바이트를 업데이트하면 장치는 여전히 전체 물리적 페이지를 씁니다(그리고 결국 이전 페이지를 삭제합니다). 따라서 1바이트 논리적 쓰기는 4KB 또는 16KB 물리적 쓰기로 바뀌어 쓰기 증폭 계수가 4,000-16,000이 됩니다! 결과적으로 SSD는 쓰기를 추가 전용 쓰기 로그로 병합하는 로그 구조(log-structured) 스토리지를 사용하는 시스템과 가장 잘 맞습니다.

SSD의 부상은 신뢰성에 대한 추가적인 발표된 연구를 가져왔습니다. 2015년 연구[30]는 원시 NAND 오류 패턴 수준에서 Google SSD 신뢰성에 대한 통찰력을 제공했습니다. 2019년 알리바바 연구는 약 100만 개의 SSD 중 상관된 SSD 장애를 조사했는데[31], [13]과 꽤 유사하지만 하드 드라이브가 아닌 SSD를 대상으로 했습니다.

마지막으로 SSD 데이터는 드라이브 전원이 꺼져 있어도 저하됩니다! NAND SSD는 "게이트" 영역에 배치된 전자에 데이터를 저장합니다. DRAM과 달리 그 게이트는 오랫동안 전하를 유지하도록 설계되었습니다. 활성 작동(프로그래밍) 중에는 더 높은 온도가 좋습니다. 실리콘을 더 전도성 있게 만들어 프로그램/삭제 작업 중 전류 흐름을 증가시키기 때문입니다. 이는 터널 산화물에 대한 스트레스를 줄여 게이트 영역 내부에 전자를 유지하는 능력을 향상시키므로 셀의 내구도가 향상됩니다.

그러나 전원이 꺼지면 온도가 상승함에 따라 전도성이 다시 증가하고 플로팅 게이트에 저장된 전하가 더 빨리 누출됩니다. 결국 충분한 누출은 셀의 전압 상태를 변경하고 데이터를 읽을 수 없게 만듭니다[25], [32], [33]. 저하 속도는 아레니우스의 법칙(Arrhenius’ Law)을 따르며, 10°C 온도 상승마다 $k$배 더 빠른 저하를 초래합니다(여기서 $k$는 기술별 요인). 현실은 더 복잡합니다[34], [25]. 엔터프라이즈 NAND SSD는 일반적으로 실온에서 최소 3개월의 전원 차단 내구성을 보장하지만, 적대적으로 높은 온도에서는 보존 시간이 상당히 짧을 수 있습니다.

전반적으로 SSD 신뢰성은 전체 스토리지 시스템의 신뢰성을 지배하지 않습니다. 시스템 수준에서 Azure 중단에 대한 2018년 연구[35]는 112개의 보고된 사고(사용자 또는 시스템 수준 모니터에 표시됨)를 분류하려고 시도했습니다. 대부분의 사고는 논문의 초점이었던 소프트웨어 근본 원인을 가진 것으로 간주되었으며, 하드웨어 장애는 보고된 사고의 5% 미만을 차지했습니다. 그 연구가 스토리지 시스템에 국한된 것은 아니었지만, 하드웨어 장애는 실제로 스토리지 시스템 장애에 비교적 작은 기여자이며 가까운 미래에 이것이 바뀔 것으로 예상하지 않습니다. 근본적으로 시스템 신뢰성의 정도는 우리가 얼마나 빨리 혁신하고 변화해야 하는지에 달려 있습니다. 하드웨어/플랫폼 추상화(블록 장치, NVMe 등)는 비교적 안정적이므로 과거에 잘 작동했던 신뢰성 프레임워크(중복 코드, 표준화된 모니터링 및 장애 처리)는 계속해서 잘 작동할 것입니다. 한편 소프트웨어 및 운영자 측면에서는 새로운 비즈니스 요구, 워크로드, API 및 기타 요구사항을 충족하기 위해 지속적으로 진화하고 변경해야 합니다. 혁신의 속도는 궁극적으로 전체 신뢰성에 대한 가장 큰 한계를 나타냅니다.

### 10.1.6 결함 예측 및 위치 파악 (Predicting and localizing faults)

결함 가능성이 있는 구성 요소가 미래의 애플리케이션 장애를 일으킬지 예측하면, 특히 충분히 일찍 예측하는 경우 사용자로부터 결함을 가릴 수 있습니다. 이러한 예측은 짧은 시간 범위를 포함합니다. 메모리 모듈이 향후 10년 내에 실패할 것이라고 100% 정확도로 예측하는 것은 특별히 유용하지 않습니다.

머신 러닝 모델은 애플리케이션에 영향을 미치는 장애의 초기 징후를 식별할 수 있으며 기존 휴리스틱을 대체했습니다. 모델은 오탐(false positive) 및 미탐(false negative) 비용을 절충하도록 보정될 수 있습니다. 또한 새로운 신호를 표면화하고 새로운 하드웨어 세대에 적응할 수 있습니다.

결함 예측은 모델의 정확도가 완벽하지 않더라도(종종 그렇듯이) 가치가 있을 수 있습니다. 예를 들어 침묵 데이터 손상(Silent Data Corruption)을 일으키는 결함이 있는 CPU를 감지하기 위해 Google에서 개발한 모델[36]은 가장 큰 수익을 얻을 수 있는 곳에 비용이 많이 드는 선별 검사를 지시하는 데 매우 성공적이었습니다. 결함이 있는 기계는 드물기 때문에(1000대 중 1대 미만), 정밀도가 30%에 불과한 모델이라도 선별 검사를 크게 개선합니다. 이 모델은 회수율(recall)이 나쁜 수작업 규칙을 대체했습니다.

Google의 또 다른 모델은 수정 가능한 오류의 공간적 및 시간적 패턴을 기반으로 DRAM 장애를 예측합니다. 높은 오탐률에도 불구하고 이 모델은 기존 휴리스틱 솔루션보다 성능이 뛰어납니다. 이 모델의 낮은 예측 정확도(~50%)는 기계를 수리로 직접 보내기에 충분히 강력한 신호를 제공하지 않지만, 결함이 있는 것으로 예측된 기계는 워크로드를 마이그레이션한 다음 불량 DIMM을 확인하기 위해 매우 침습적인 메모리 스캔을 실행하기에 좋은 후보입니다.

Pinheiro 등[23]은 SMART(Self-Monitoring Analysis and Reporting Technology) 표준을 통해 사용할 수 있는 디스크 상태 매개변수를 기반으로 디스크 드라이브 장애에 대한 예측 모델을 생성하려는 Google의 초기 시도 중 하나를 설명했습니다. 그들은 그러한 모델이 충분한 정확도로 개별 드라이브 장애를 예측할 가능성은 낮지만, 교체 장치 프로비저닝을 최적화하는 데 유용할 수 있는 장치 그룹의 예상 수명을 추론하는 데 유용할 수 있다고 결론지었습니다. Google과 Seagate 간의 더 최근 협력은 ML 기반 모델로 높은 정확도(98%)에 도달했지만 전체 장애의 35%만 예측했습니다[37]. 학술 논문들[38], [39], [31], [40]도 비슷한 결론에 도달했습니다. 개별 디스크 장애를 예측하는 것은 건초더미에서 바늘 찾기 문제입니다. 반면 얼마나 많은 디스크가 실패했는지(연간 고장률)를 식별하는 것은 더 쉬운 문제이며, 스토리지 중복성 구성(redundancy scheme) 선택이 모집단 수준의 장애를 기반으로 하기 때문에 여전히 유용합니다.

### 10.1.7 결함 수리 (Repairing faults)

효율적인 수리 프로세스는 WSC의 전체 비용 효율성에 매우 중요합니다. 수리 중인 기계는 사실상 작동하지 않으므로 기계가 수리 중인 시간이 길수록 전체 시스템의 가용성이 낮아집니다. 또한 수리 작업은 교체 부품과 숙련된 노동력 측면에서 비용이 많이 듭니다. 마지막으로 수리 품질(수리 작업이 실제로 문제를 해결할 가능성과 결함이 있는 구성 요소를 정확하게 결정할 가능성)은 구성 요소 비용과 평균 기계 신뢰성 모두에 영향을 미칩니다.

고장 난 기계는 결국 수리해야 합니다. 수리가 즉시 이루어져야 하는 전통적인 시나리오에서는 교체 부품의 값비싼 준비와 서비스 기술자를 현장으로 부르기 위한 추가 비용이 필요합니다. 수리를 일괄 처리할 수 있다면 수리당 비용을 낮출 수 있습니다. 참고로 24시간 이내 현장 수리를 제공하는 IT 장비 서비스 계약은 일반적으로 장비 가치의 5-15%에 해당하는 연간 비용이 듭니다. 4시간 응답 시간은 그 비용을 두 배로 늘릴 수 있습니다.

이에 비해 대규모 서버 팜의 수리는 더 저렴합니다. 요점을 설명하기 위해 WSC가 전임 수리 기술자를 바쁘게 유지할 만큼 충분한 규모를 가지고 있다고 가정해 봅시다. 수리당 1시간, 연간 고장률 5%를 가정하면 40,000대의 서버가 있는 시스템이면 충분합니다. 실제로는 동일한 기술자가 설치 및 업그레이드도 처리할 수 있으므로 그 숫자는 상당히 적을 것입니다. 또한 기술자의 시간당 비용이 $100이고 평균 수리에 시스템 비용의 10%에 해당하는 교체 부품이 필요하다고 가정해 봅시다. 이 두 가정 모두 넉넉하게 높게 잡은 것입니다. 그래도 (가정적으로) 각각 $10,000의 비용이 드는 서버 클러스터의 경우 서버당 연간 비용은 $5\% \times (\$100 + 10\% \times \$10,000) = \$55$, 즉 연간 0.55%에 도달합니다. 즉, 대규모 클러스터를 건강하게 유지하는 것은 꽤 저렴할 수 있습니다.

또한 수천 대의 기계가 작동 중일 때 대량의 기계 상태 데이터를 수집하고 분석하여 건강 판단 및 진단을 위한 자동화된 시스템을 만들 수 있습니다. 그림 10.4에 설명된 Google의 시스템 건강 인프라는 이 방대한 데이터 소스를 활용하는 모니터링 시스템의 예입니다. 구성, 활동, 환경 및 오류 데이터에 대해 모든 서버를 지속적으로 모니터링합니다. 이 정보는 확장 가능한 저장소에 시계열로 저장되어 머신 러닝 방법을 사용하여 가장 적절한 수리 조치를 제안하는 자동화된 기계 장애 진단 도구를 포함한 다양한 종류의 분석에 사용될 수 있습니다.

> **그림 10.4** Google의 시스템 건강 모니터링 및 분석 인프라

개별 기계 진단을 수행하는 것 외에도 시스템 건강 인프라는 다른 방식으로도 유용합니다. 예를 들어 새 시스템 소프트웨어 버전의 안정성을 모니터링하고 대규모 기계 전체에서 특정 배치의 결함 구성 요소를 정확히 찾아내는 데 도움이 되었습니다. 또한 이전 섹션에서 언급한 디스크 장애 연구 및 데이터 센터 규모의 전력 프로비저닝 연구[41]와 같은 장기 분석 연구에서도 가치가 있었습니다.

### 10.1.8 결함 감내 (Tolerating faults)

시스템 설명에서 우리는 종종 최대 부하에서 서비스를 제공하는 데 필요한 서버 수를 나타내기 위해 $N$을 사용하므로, $N + 1$은 중복성을 위한 하나의 추가 복제본이 있는 시스템을 설명합니다. 일반적인 배열에는 $N$(중복 없음), $N + 1$(단일 장애 감내), $N + 2$("동시 유지 보수 가능", 한 유닛이 계획된 유지 보수를 위해 오프라인 상태일 때도 단일 장애 감내), $2N$(모든 유닛 미러링)이 포함됩니다.

용량을 늘리기 위해 많은 양의 내부 복제(수평 확장)를 갖춘 시스템은 매우 낮은 비용으로 중복성을 제공합니다. 예를 들어 일일 최대 부하를 처리하기 위해 100개의 복제본이 필요한 경우 $N + 2$ 설정은 고가용성을 위해 2%의 오버헤드만 발생시킵니다. 여러 지역에 분산된 하이퍼스케일 서비스는 종종 지역적으로 $N + 1$ 또는 $N + 2$로 구성됩니다. 즉, 한 지역이 유지 보수(일반적으로 소프트웨어 업그레이드)를 받는 동안 다른 지역의 장애를 견딜 수 있습니다. 지역은 매우 신뢰할 수 있으므로 서비스 소유자는 $N + 1$ 구성을 선택하고 최대 트래픽 시간 외에 업그레이드를 수행할 수 있습니다. 낮은 요청 수준이 서비스를 $N + 2$ 상태에 놓이게 하기 때문입니다(일일 최저점 동안 $N$이 낮으므로).

이러한 시스템은 장애를 매우 잘 견딜 수 있어 운영자는 내부 여유가 얼마나 남았는지, 즉 벼랑 끝에 얼마나 가까운지 모를 수 있습니다. 이러한 경우 건강한 행동에서 멜트다운으로의 전환은 급격할 수 있습니다. 한 순간에는 모든 것이 괜찮아 보이다가(요청의 100% 성공) 다음 순간 모든 것이 폭발합니다. 따라서 결함이 잘 감내되면서도 운영자에게 보일 수 있도록 서비스 수준과 기계 인프라 수준 모두에서 가려진 결함과 여유 용량을 모니터링하는 것이 필수적입니다. 좋은 모니터링은 내부 중복의 양이 결함 감내 소프트웨어 계층이 처리할 수 있는 한계에 접근하기 전에 신속한 시정 조치를 가능하게 합니다.

### 10.1.9 영역 및 지역 장애 (Zonal and regional failures)

Google 검색과 같은 하이퍼스케일 서비스는 많은 지역에서 전 세계적으로 실행되지만 대부분의 소규모 애플리케이션은 단일 지역(region)에서 실행되며 일부 엔터프라이즈 애플리케이션은 단일 영역(zone)에서 실행될 수도 있습니다. 이에 대한 한 가지 이유는 운영 비용입니다. 소규모 서비스는 확장을 위해 여러 복제본이 필요하지 않으므로 가용성을 위해 여러 복제본을 유지하는 것이 비쌉니다. 또 다른 이유는 개발 비용입니다. 특히 원래 단일 온프레미스 데이터 센터용으로 구축된 엔터프라이즈 애플리케이션의 경우 다중 지역 서비스를 구축하는 것이 더 복잡합니다.

이러한 사용 사례를 지원하려면 저수준 시스템 스택이 충분한 신뢰성과 내구성을 제공해야 합니다. 단일 소프트웨어 또는 물리적 장애(예: 잘못된 업데이트 또는 오작동하는 회로 차단기)가 영역을 다운시킬 수 있으므로 높은 단일 영역 가용성(예: 99.99%)을 제공하기는 어렵습니다. 단일 건물에 저장된 데이터는 화재나 홍수와 같은 국지적 재해로 인해 손실될 수 있으므로 높은 단일 영역 내구성을 제공하는 것도 거의 불가능합니다. 따라서 높은 가용성 또는 내구성 요구사항이 있는 애플리케이션은 단일 영역에서 실행되어서는 안 됩니다. 동시에 최저 수준의 영역 시스템 스택은 가장 중요한 이벤트만이 심각한 중단이나 데이터 손실을 유발하도록 그러한 장애의 영향을 최소화하는 것을 목표로 해야 합니다.

이 목표를 해결하기 위해 영역 인프라는 일반적으로 특정 가용성 보장과 함께 자원 할당 및 작업 스케줄링을 제공하며 동시 유지 보수를 허용하여 계획된 중단을 오버헤드의 원인으로 제거합니다.

그림 10.5는 그러한 클러스터 스택의 개략도를 보여줍니다[42], [43], [44].

스택의 기반은 기계에 전력과 냉각을 제공합니다. 종종 설계 목표는 전력 및 냉각을 위한 결함 감내 및 동시 유지 보수 가능한 물리적 아키텍처입니다.

> **그림 10.5** 고가용성 클러스터 아키텍처

고가용성을 위해 클러스터 스케줄링은 작업을 이러한 장애 단위 전체에 분산시킵니다. 마찬가지로 스토리지 시스템에 필요한 중복성은 전력 또는 냉각 이벤트로 인해 동시에 실패할 수 있는 클러스터의 비율에 의해 부분적으로 결정됩니다. 따라서 더 큰 클러스터는 여전히 가용성 및 내구성 요구사항을 충족하면서 더 낮은 스토리지 오버헤드와 더 효율적인 작업 스케줄링으로 이어집니다.

6장에서 설명한 Jupiter 설계와 같은 일반적인 클러스터 패브릭은 패브릭의 중복성과 배포의 물리적 다양성을 통해 고가용성을 달성합니다. 또한 필요한 프로토콜을 위한 견고한 소프트웨어와 신뢰할 수 있는 대역 외(out-of-band) 제어 평면에 중점을 둡니다. 앞서 논의한 클러스터 스케줄러에 대한 분산 요구사항을 감안할 때 Jupiter는 클러스터 전체에 걸친 균일한 대역폭을 지원하고 데이터 경로 다양성 외에도 네트워크 제어 평면의 탄력성 메커니즘을 지원합니다.

Borg(Google의 대규모 클러스터 관리 시스템)에서 실행되는 애플리케이션은 복제, 분산 파일 시스템에 영구 상태 저장, (해당되는 경우) 가끔 체크포인트 수행과 같은 기술을 사용하여 단일 기계 장애 이벤트를 처리할 것으로 예상됩니다. Borg는 실패한 작업을 자동으로 다시 스케줄링(필요한 경우 새 기계에서)하여 장애 이벤트의 영향을 완화하고, 기계, 랙, 전력 도메인과 같은 장애 도메인 전체에 애플리케이션을 분산시켜 상관된 장애를 줄입니다.

마지막으로 전체 스토리지 시스템은 고가용성이며 앞서 10.1.5절 및 6장에서 설명한 대로 오류를 처리합니다.

이러한 기능들이 결합되어 수많은 크고 작은 서비스에 적합한 고가용성 클러스터를 제공합니다. Google Cloud Platform(GCP)과 같은 클라우드 플랫폼에서 가용성은 영역(zone) 및 지역(region)의 개념을 사용하여 고객에게 노출됩니다[45]. 단일 영역 애플리케이션은 가장 낮은 신뢰성(약 99.7%[46])을 갖는 반면 이중 지역 애플리케이션은 99.998% 가용성에 도달할 수 있습니다.

지역 및 영역 정의는 주요 퍼블릭 클라우드에서 매우 유사하지만 Azure에는 두 개의 영역만 있는 지역이 다수 있습니다.

Google은 적절한 제품 서비스 수준 계약(SLA)을 충족할 수 있도록 클러스터 대 영역의 지도를 내부적으로 유지 관리합니다(예: Google Compute Engine SLA[48]). 클라우드 고객은 GCP에서 고가용성 및 결함 감내 애플리케이션을 개발할 때 영역 및 지역 추상화를 활용하도록 권장됩니다.

그림 10.6은 컴퓨팅 영역 및 지역의 대표적인 매핑을 보여줍니다. Compute Engine과 같은 서비스는 지역 수준에 존재하여 영역 간의 지역 로드 밸런싱이나 임의의 영역에서 배치 작업 시작과 같은 기능을 제공할 수 있습니다. 또한 단일 영역에 특정한 기능을 수행하는 영역 API로도 제공됩니다. 이 구분을 통해 고객은 특정 요구에 따라 신뢰성 태세를 선택할 수 있습니다. 예를 들어 클라우드 네이티브 애플리케이션은 지역 로드 밸런싱 및 지역 데이터베이스 서비스를 사용하여 영역 장애 처리를 클라우드 제공업체에 위임할 수 있습니다.

> **그림 10.6** 영역과 지역이 있는 클라우드 서비스 구조

두 영역에 기본 및 보조 인스턴스가 있는 고가용성 애플리케이션을 실행하는 또 다른 고객은 영역별 API만 사용하여 클라우드 제공업체의 지역 API 중단에 노출되는 것을 피할 수 있습니다.

### 10.1.10 침묵 데이터 손상 및 손상된 실행 오류

우리는 컴퓨터가 계산에서 오류 정지(fail-stop)하고 정확하다고 생각하는 데 익숙하며 대부분의 시스템 소프트웨어는 암묵적으로 그 가정에 의존합니다. 예를 들어 두 숫자를 더하면 결과가 실제로 올바른 합계가 될 것이라고 암묵적으로 가정합니다.

불행히도 하드웨어는 거의 항상 이러한 기대에 부응하지만 아주 드물게(예: 1,000조 명령 중 한 번) 그렇지 않을 수 있습니다. 칩 제조는 완벽하지 않지만 정교한 제조 테스트 배열은 사양 내에서 작동하도록 보장합니다. 수십 년 동안 이러한 제조 테스트는 신뢰할 수 있는 것으로 입증되어 백만 개당 몇 개의 결함 부품만 있는 제품을 생산했습니다.

그러나 6장에서 설명한 무어의 법칙의 둔화는 새롭고 교활한 신뢰성 문제로 이어졌습니다. 구체적으로 점점 더 작아지는 기능 크기와 더 정교한 계산 구조, 성능 향상을 위한 실리콘 전문화 증가로 인해 제조 테스트 중에 감지되지 않는 일시적인 계산 오류가 더 흔해졌습니다[36], [47], [49], [50]. 이러한 결함은 마이크로코드 업데이트와 같은 기술로 항상 완화할 수 있는 것은 아니며 진단하기 매우 어렵습니다. 예를 들어 프로세서 내의 특정 구성 요소와 상관 관계가 있을 수 있어 작은 코드 변경으로 신뢰성에 큰 변화를 줄 수 있습니다. 설상가상으로 이러한 장애는 종종 "침묵(silent)"합니다. 유일한 증상은 잘못된 계산입니다. (오류 감지 또는 수정 로직으로 모든 프로세서 경로가 보호되는 것은 아니기 때문에 침묵 오류가 가능합니다.)

Google의 광범위한 조사에 따르면 소프트웨어 엔지니어가 예상하는 것보다 훨씬 높은 비율로 여러 종류의 침묵 손상 실행(silent corrupt execution) 오류가 밝혀졌습니다[36]. 이것들은 단지 제조 하드웨어 결함의 배경 비율이 점진적으로 증가한 것이 아닙니다. 때로는 초기 설치 후 한참 뒤에 나타날 수 있습니다. 잡히지 않은 오류에 내재된 위험 외에도 이러한 침묵 데이터 손상은 디버깅에 상당한 소프트웨어 노고(toil)를 초래합니다. 워크로드가 기계 간에 마이그레이션되는 환경이나 통신 "폭발 반경(blast radius)"이 큰 분산 시스템에서는 연쇄 장애로 이어질 수도 있습니다.

이러한 증가하는 침묵 데이터 손상 오류는 신뢰할 수 있는 계산을 위한 새로운 WSC 기술에 대한 강조를 필요로 합니다. 첫째, 이러한 오류의 발생을 더 성공적으로 방지해야 합니다. 장치 수준 분석[49]은 이것이 표준 장치 결함(한계 트랜지스터)에 의해 발생하지만 한계 트랜지스터가 여전히 작동하지만 상위 수준 데이터 흐름에 타이밍 오류를 도입하는 경우 기존 제조 테스트에서 잡히지 않는다고 제안합니다. 둘째, 이러한 버그 중 다수는 WSC 규모 배포에서만 명백하며 트랜지스터는 장치가 노후화됨에 따라 일정 시간이 지난 후에만 실패할 수 있으므로, 현장 노후화 및 마모를 모니터링하고 낮은 오버헤드로 규모에 맞게 신뢰성 일탈을 사전에 감지하고 대응할 수 있는 개선된 현장 테스트 소프트웨어가 필요합니다. 또한 하드웨어(설계, 검증, 제조, 선별 및 번인)에서 애플리케이션(체크섬 및 복제 계산, 데이터 중복성, 샘플링된 불변량 등과 같은 계산 및 데이터 불변량)에 이르기까지 WSC 설계의 모든 측면에 걸쳐 새로운 WSC 시스템 수준 최적화를 고려해야 합니다. 자세한 내용은 [51]에서 논의됩니다.

### 10.1.11 운영 관행 (Operational practices)

지금까지 이 섹션에서는 신뢰성의 물리적 및 기술적 측면에 초점을 맞췄습니다. 종종 운영 관행은 이를 개선하기 위해 설계된 하드웨어 또는 소프트웨어 기능보다 신뢰성에 더 큰 영향을 미칩니다. 이 분야에 대한 광범위한 논의는 SRE Book[2]을 참조하기 바라며 아래에서는 몇 가지 주요 측면을 간략하게 강조하겠습니다.

변경 관리는 아마도 다른 어떤 운영 관행보다 신뢰성에 더 많은 영향을 미칠 것입니다. 대부분의 중단은 변경으로 인해 발생합니다. 새 바이너리에 테스트 중에 발견되지 않은 버그가 있거나, 새 스위치가 잘못 케이블링되거나, 근처의 새 장치가 설치되는 동안 장치 전원이 손실되거나, 기술자가 수리 준비를 위해 잘못된 회로 차단기를 열거나, 구성 변경이 잘린 구성 파일로 인해 서버를 삭제하는 등의 이유입니다.

이러한 위험 때문에 전통적인 IT 관행은 변경을 권장하지 않지만 하이퍼스케일러는 많은 서비스 및 애플리케이션에 대해 매일 또는 매주 변경하는 높은 제품 속도를 지원해야 합니다. 또한 하이퍼스케일은 지속적이고 빈번한 운영 변경을 필요로 합니다. 매일 수천 대의 서버가 설치되거나 수리되고 건물이나 네트워크 경로가 추가되거나 폐기되며 피어링 포트가 켜지는 등입니다. 따라서 변경 관리는 신뢰성을 염두에 두고 자동화되어야 합니다.

다섯 가지 주요 관행이 일반적입니다[52].

1.  **장애 도메인 억제(Failure domain containment).** 시스템 설계자는 장애 도메인(예: 전원 공급 장치, 냉각, 소프트웨어 제어 등을 공유하는 물리적 엔터티 그룹)을 식별하고 이러한 도메인 간에 가능한 한 많은 격리를 강제해야 합니다. 억제는 장애의 "폭발 반경"이 도메인 경계를 넘어 확장될 위험을 최소화합니다. 장애 도메인 격리는 또한 재해 복구 중 잠금(lockout) 위험을 줄입니다. 적절한 격리를 위해서는 장애 도메인을 부트스트랩하는 것과 관련된 순환 종속성을 피해야 하기 때문입니다(예: 클러스터 네트워크 서비스가 분산 스토리지 서비스에 의존할 수 없으며, 이는 다시 클러스터 네트워크 서비스에 의존합니다). 실제로 시스템 설계자는 폭발 반경 최소화와 시스템 복잡성 및 비용 최소화 사이에서 균형을 맞춰야 합니다.
2.  **점진적, 감독된 변경 롤아웃(Progressive, supervised change rollout).** 잘못된 운영은 서비스 중단이나 장애 도메인 억제 위반으로 이어질 수 있습니다. 이러한 위험을 완화하기 위해 변경 사항은 점진적으로(예: 영역별로) 롤아웃되어야 합니다. 점진적 롤아웃만으로는 안전이 향상되지 않습니다. 롤아웃 내내 시스템 상태를 면밀히 감독해야 합니다. 그렇지 않으면 서비스 동작이 점진적이지만 지속적으로 저하되는 "느린 난파(slow wrecks)"에 취약한 상태로 남게 됩니다. 비즈니스를 유지하는 데 필요한 운영 규모를 따라잡기 위해 시스템은 안전과 운영 속도(예: 동시성 수준) 사이에서 좋은 균형을 맞춰야 합니다.
3.  **불변량 모니터링 및 유지(Monitoring and upholding invariants).** 시스템 상태를 유지하기 위해 변경 롤아웃 중에 안전 불변량을 모니터링해야 합니다. 블랙박스 메트릭(네트워크의 내부 구현 세부 정보를 가정하지 않고 따라서 사용자의 관점을 포착하는 것)은 근본 원인에 관계없이 서비스 동작이 저하되기 시작하자마자 장애를 감지하는 데 효과적입니다. 사용자가 경험하는 정확한 조건을 포착하기 때문입니다. 그러나 저하가 발생하기 전에 감지하는 데는 효과적이지 않습니다. 화이트박스 불변량, 즉 하위 시스템의 구현 세부 정보를 활용하는 것(예: "최소 2개의 복제본이 응답해야 함")은 문제를 조기에 감지할 수 있지만 시스템 상호 작용의 광범위한 다양성을 포괄하도록 일반화하기 어렵습니다.
4.  **용량 여유 프로비저닝(Provisioning capacity headroom).** 트래픽을 수용하는 데 꼭 필요한 것보다 더 많은 용량을 프로비저닝하면 안전이 향상됩니다. 예를 들어 여유 공간은 관리 작업(특히 인간 기술자의 개입이 필요한 작업)이 중단될 때 영향을 받지 않는 네트워크 부분의 혼잡 위험 없이 작업이 일시 중지된 상태로 유지될 수 있도록 보장합니다.
5.  **빠른 복구(Fast recovery).** 빠른 복구는 중단 시간을 제한합니다. 실제로 복구는 종종 인간이 상황을 이해하고 복구 전략을 결정해야 합니다. 진행 중인 변경 사항이 많은 시스템에서는 무엇을 롤백해야 하는지 항상 명확하지 않습니다. 시스템은 또한 롤백으로 복구할 수 없는 방식으로 실패할 수 있습니다. 예를 들어 중단으로 인해 데이터가 지워지거나 시스템 자체에 대한 액세스가 차단되는 경우입니다.

이러한 관행은 교육 및 변경 검토와 같은 표준 관행을 통해 시행될 수 있으며 소프트웨어 시스템의 지원을 받을 수 있습니다. 예를 들어 Google의 CAPA 시스템[52]은 중앙 프록시를 사용합니다. 자동화된 관리 시스템은 변경 사항을 직접 시행할 수 없으며 대신 이를 실행할 프록시에 제출합니다. 프로덕션 상태의 유일한 변경자로서 프록시는 점진적 롤아웃과 같은 불변량을 강제할 수 있습니다.

마찬가지로 물리적 변경은 신뢰성을 갖기 위해 자동화가 필요합니다[53]. 하이퍼스케일 네트워크에서는 많은 위치에서 많은 설치, 턴업, 업그레이드 및 수리가 발생하므로 많은 물리적 변경이 동시에 발생합니다. 네트워크의 수백만 장치를 정확하게 설명하는 상세한 모델 없이는 안전한 변경이 불가능할 것입니다.

## 10.2 웨어하우스 규모에서의 보안 (Security at warehouse scale)

클라우드 사용자는 사용자 및 고객 데이터의 책임감 있는 관리자로서의 WSC 제공업체의 능력에 의존합니다. 따라서 대부분의 제공업체는 정교하고 계층화된 보안 태세에 상당한 자원을 투자합니다. 범위는 규모만큼 넓으며 심각한 WSC 인프라는 심각한 정보 보안 전문 지식을 필요로 합니다.

### 10.2.1 데이터 센터 보안: 건물 및 시스템

물리적 수준에서 보안은 물리적 데이터 센터 구내를 보호하여 장비의 물리적 조작이나 도난을 방지하는 것을 수반합니다. WSC 제공업체는 생체 인식 식별, 1인용 잠금 장치, 금속 탐지, 카메라, 컴퓨터 비전, 차량 장벽, 레이저 기반 침입 탐지 시스템 및 기타 기술 수단을 사용하여 무단 존재를 감지하거나 방지합니다. 또한 데이터 센터 바닥에 허용된 인원도 제한합니다[54]. Google의 데이터 센터는 6개의 별도 액세스 계층을 정의합니다[55]. 예를 들어 민감한 구역에 들어가는 각 사람이 인증되고 승인되었는지 확인하기 위해 원형 1인용 문은 승인된 사람 뒤에 아무도 꼬리물기(tailgate)를 할 수 없도록 보장합니다(그림 10.7).

높은 수준의 물리적 보안을 보장하기 위해 모범 사례는 정기적인 운영 성능 테스트를 요구합니다. 이러한 테스트는 탁상 훈련(시뮬레이션된 이벤트에 대한 반응 관찰)이거나 숙련된 제3자가 폭력을 제외한 필요한 모든 수단을 통해 데이터 센터에 액세스하려고 시도하는 적대적("레드 팀") 훈련일 수 있습니다.

또한 제공업체는 시설을 운영하는 제어 시스템을 보호하여 장비를 비활성화하거나 손상시키는 전자 공격으로 인한 중단을 방지해야 합니다. 데이터 센터 캠퍼스에는 수천 개의 임베디드 컨트롤러가 포함되어 있으며 각각은 변압기, 전기 개폐기 및 차단기, 백업 발전기, 냉각탑, 펌프, 팬 등과 같은 장비의 적절한 작동에 필수적입니다. 또한 건물 관리 시스템(BMS)을 통해 운영자는 건물의 전반적인 운영을 관찰하고 제어할 수 있습니다.

> **그림 10.7** 꼬리물기를 방지하기 위한 원형 1인용 문

BMS 및 연결된 컨트롤러는 일반적으로 인터넷과 "에어 갭(air gapped)"으로 분리된 망 분리 네트워크에 있어 외부 공격자가 액세스할 가능성을 줄입니다. 그러나 에어 갭만으로는 보안을 보장하지 않습니다. 유명한 사례로 이란의 나탄즈 시설은 USB 스틱에 담겨 들어온 Stuxnet 바이러스의 공격을 받았으며, 이 바이러스는 BMS를 감염시키고 원심 분리기의 제어 매개변수를 조작하여 파괴에 이르게 했습니다[56].

불행히도 WSC에서 사용되는 시스템을 포함한 현재의 산업 제어 시스템은 보안 관행에 있어 최신 기술보다 뒤쳐져 있으며, 그 격차가 상당합니다. 많은 경우 이러한 문제는 보안 패치를 일관되고 확실하게 적용하고, 인증 및 권한 부여를 책임감 있게 관리하며, 적절한 네트워크 분할을 구현하는 등 현재의 모범 사례를 채택하여 쉽게 예방할 수 있습니다. 최근의 사건과 고객 요구로 인해 보안에 대한 업계의 관심이 높아졌지만 현재로서는 이러한 시스템의 전반적인 보안 태세는 여전히 열악합니다.

### 10.2.2 서버 보안

WSC의 대부분의 서버는 이 목적을 위해 맞춤 제작되었으며 보안을 매우 강조합니다. 예를 들어 Google은 클러스터의 서버 보드와 대부분의 네트워킹 장비를 설계합니다. 여기에는 구성 요소 공급업체 심사 및 신중한 구성 요소 선택, 공급업체와 협력하여 구성 요소가 제공하는 보안 속성 감사 및 검증이 포함됩니다. Google은 또한 서버, 장치 및 주변 장치에 배포하는 하드웨어 보안 칩(다음 섹션에서 논의할 Titan)을 포함한 맞춤형 칩을 설계합니다. 이 칩을 통해 합법적인 Google 장치를 하드웨어 수준에서 식별하고 인증할 수 있으며 부팅 보안 및 자격 증명 프로비저닝을 위한 하드웨어 신뢰 루트(Hardware Root of Trust) 역할을 합니다. Titan 하드웨어 칩의 변형은 Pixel 장치 및 Titan 보안 키에도 사용됩니다.

데이터 센터의 각 서버에는 고유한 ID가 있습니다. 이 ID는 하드웨어 신뢰 루트 및 기계가 부팅하는 소프트웨어와 연결될 수 있습니다. 이 ID는 기계의 저수준 관리 서비스에 대한 API 호출을 인증하는 데 사용됩니다. 이 ID는 상호 서버 인증 및 전송 암호화에도 사용됩니다. 예를 들어 Google의 ALTS(Application Layer Transport Security)[57] 시스템은 오픈 소스 mTLS[58]와 유사하지만 확장성이 개선된 원격 프로시저 호출(RPC) 통신을 보호합니다. 보안 사고에 대응하기 위해 기계 ID를 중앙에서 취소할 수 있습니다. 또한 인증서와 키는 정기적으로 교체되고 이전 것은 취소됩니다.

자동화된 시스템은 다음을 보장합니다:

*   서버가 소프트웨어 스택의 최신 버전(보안 패치 포함)을 실행하도록 보장.
*   하드웨어 및 소프트웨어 문제 감지 및 진단.
*   검증된 부팅(verified boot) 및 암시적 증명(implicit attestation)을 통해 기계 및 주변 장치의 무결성 보장.
*   의도한 소프트웨어 및 펌웨어를 실행하는 기계만 프로덕션 네트워크에서 통신할 수 있는 자격 증명에 액세스할 수 있도록 보장.
*   더 이상 필요하지 않을 때 서비스에서 기계를 제거하거나 재할당.

이 인프라에 배포된 서비스는 서비스 간 통신에 암호화를 사용하고, 서비스 ID, 무결성 및 격리를 위한 시스템을 구축하며, ID 기본 요소 위에 액세스 제어 메커니즘을 구현하고, 궁극적으로 최종 사용자 또는 고객 데이터에 대한 액세스를 신중하게 제어하고 감사합니다.

최종 사용자 신원 인증과 같은 개별적인 과제는 그 자체로 풍부한 보안 및 프라이버시 과제를 야기합니다. 비밀번호 재사용 및 열악한 비밀번호 관행에 직면하여 대규모로 사용자를 인증하고 로그인 또는 계정 복구 남용을 방지하는 것은 종종 전담 전문가 팀을 필요로 합니다.

저장 데이터(data at rest)는 항상 암호화되어야 하며, 종종 독립적인 키와 관리 도메인을 가진 서비스 중심 및 장치 중심 시스템 모두를 사용합니다. 스토리지 서비스는 데이터가 삭제되거나 파괴되었다고 진정으로 확신할 수 있는 요구사항과 내구성 요구사항 사이의 균형을 맞추는 인지 부조화의 형태에 직면합니다. 스토리지 서비스는 대역폭 집약적이므로 네트워크 트래픽이 적절하게 암호화되도록 추가적인 계산 리소스가 필요합니다.

내부 네트워크 보안은 인터넷 규모의 통신을 보호하기 위한 적절한 인프라와 함께 관리되어야 합니다. 워크로드 마이그레이션 인프라에 동기를 부여하는 것과 동일한 신뢰성 제약 조건은 기본 기계에서 분리된 워크로드 모델을 수용하는 네트워크 보안 프로토콜을 요구합니다. 한 가지 예는 Google 애플리케이션 계층 전송 보안(ALTS)[57]입니다. SSL/TLS의 보편화 증가와 항상 존재하는 DoS 우려에 적절하게 대응해야 할 필요성은 암호화 계산 및 트래픽 관리를 위한 전담 계획 및 엔지니어링을 필요로 합니다.

모범 사례 인프라 보안의 이점을 완전히 실현하려면 중요하고 지속적인 운영 보안 노력이 필요합니다. 침입 탐지, 내부자 위험, 직원 장치 및 자격 증명 보호, 안전한 소프트웨어 개발을 위한 모범 사례 시행은 모두 적지 않은 투자를 필요로 합니다.

정보 보안이 WSC 인프라에서 진정으로 일급 시민으로서의 역할을 누리기 시작함에 따라 일부 역사적 산업 규범이 재평가되고 있습니다. 예를 들어 WSC가 인프라의 중요한 부분을 불투명한 블랙박스 타사 구성 요소에 맡기는 것은 어려울 수 있습니다. 기계를 펌웨어 수준까지 알려진 상태로 되돌려 한 워크로드에서 다음 워크로드로 재할당하는 능력은 필수 요소가 되고 있습니다.

### 10.2.3 신뢰 루트 (Root of trust)

시스템을 신뢰하려면 해당 시스템에서 실행되는 소프트웨어가 올바른 소프트웨어임을 신뢰해야 합니다. 소프트웨어 버전을 검증하려면 아래 소프트웨어 계층에 대한 전이적(transitive) 신뢰가 필요합니다. 예를 들어 OS가 변조된 경우 애플리케이션 무결성에 대해 거짓말을 하고 애플리케이션이 자체적으로 체크섬을 시도하는 것을 방해할 수 있습니다. 마찬가지로 악성 저수준 펌웨어는 메모리나 연결된 장치를 조작하여 OS 자체를 훼손할 수 있습니다.

명백히 신뢰할 수 있는 시작점을 제공하기 위해 최신 서버는 실리콘 신뢰 루트(RoT)를 사용합니다. 실리콘 RoT는 승인되고 검증 가능한 코드를 사용하여 중요한 시스템 구성 요소가 안전하게 부팅되는지 확인함으로써 하드웨어 인프라와 그 위에서 실행되는 소프트웨어가 의도된 신뢰할 수 있는 상태로 유지되도록 도울 수 있습니다. 실리콘 RoT는 다음을 지원하여 많은 보안 이점을 제공할 수 있습니다:

*   서버 또는 장치가 올바른 펌웨어로 부팅되고 저수준 맬웨어에 감염되지 않았는지 확인.
*   운영자가 서버 또는 장치가 합법적인지 확인할 수 있도록 암호화적으로 고유한 기계 ID 제공.
*   물리적 액세스가 있는 사람(예: 서버 또는 장치가 배송되는 동안)에 대해서도 암호화 키와 같은 비밀을 변조 방지 방식으로 보호.
*   권위 있고 변조가 분명한 감사 기록 및 기타 런타임 보안 서비스 제공.

실리콘 RoT는 서버 마더보드, 네트워크 카드, 클라이언트 장치(예: 노트북, 전화기), 소비자 라우터, IoT 장치 등에 사용될 수 있습니다. 예를 들어 Google은 맞춤형 RoT 칩인 Titan[59]에 의존하여 Google 데이터 센터의 기계와 가속기와 같은 주변 장치가 검증된 코드로 알려진 신뢰할 수 있는 상태에서 부팅되도록 보장합니다.

RoT를 사용하는 장치의 전원이 켜지면 RoT는 CPU나 다른 어떤 장치보다 먼저 실행을 시작하는 프로세서입니다. 이를 통해 RoT는 부팅 프로세스의 일부인 모든 소프트웨어의 서명을 검증할 수 있습니다. 이 검증은 RoT가 메인 프로세서의 전원을 켜고 첫 번째 부팅 이미지의 첫 번째 명령어를 가리키기 전에 수행됩니다. 그 후 부팅 체인의 후속 요소는 실행하기 전에 상위 수준 요소를 검증합니다. 예를 들어 OS 이미지가 기계에 로컬로 저장되지 않을 수 있으므로 부트로더는 스토리지나 네트워크에서 이를 로드하고 시작하기 전에 암호화 서명을 검증합니다.

RoT는 펌웨어 이미지를 업데이트하는 동안에도 유사하게 중요한 역할을 합니다. RoT 기반 시스템에서는 어떤 장치도(OS를 실행하는 CPU조차도) 펌웨어 이미지를 업데이트할 수 없습니다. 따라서 OS가 손상되더라도 공격자가 백도어를 남기기 위해 펌웨어를 수정하여 재부팅 후에도 액세스를 유지하려는 시도는 저지되거나 감지될 것입니다. 이러한 종류의 쓰기 보호는 일반적으로 하드웨어에서 보장됩니다. 펌웨어 EPROM의 쓰기 핀은 RoT에만 연결되고 RoT에만 연결됩니다.

RoT는 로컬 CPU를 신뢰하지 않으므로 로컬 프로세스가 펌웨어를 업데이트하는 데 사용할 수 있는 API가 없습니다. 대신 RoT는 업데이트 배포를 담당하는 오프 서버 제어 시스템과 안전하게 통신합니다. 따라서 공격자는 서버 펌웨어를 손상시키기 위해 이 작은 중앙 제어 시스템을 손상시켜야 하며, 일반 서버 중 하나 이상을 손상시키는 것으로는 성공할 수 없습니다.

실리콘에 신뢰를 고정하는 것의 중요성을 인식하여 RoT는 설계 및 제조 과정에서 많은 관심을 받습니다. 예를 들어 Google의 Titan 칩에는 물리적 조작(제조 중 또는 작동 시), 공급망 공격(배송 상자에 "가짜" Titan 칩 삽입), 소프트웨어 조작에 대한 방어 기능이 포함되어 있습니다. 예를 들어 Titan 칩의 초기 소프트웨어 이미지는 일회성 메커니즘을 통해 보안 시설에 물리적으로 설치되고 암호화적으로 인증됩니다. 초기 이미징 후 온칩 퓨즈가 물리적으로 파괴되어 더 이상의 물리적 설치가 불가능하므로, 모든 향후 업데이트에는 Google만이 알고 있는 개인 키가 필요합니다.

Google 클라우드 인프라의 무결성에 대한 신뢰를 더욱 높이기 위해 OpenTitan RoT는 Google과 파트너가 만든 고품질 구현을 갖춘 전체 실리콘 RoT를 오픈 소스로 제공합니다[60]. 그 목표는 신뢰할 수 있는 실리콘 RoT 칩의 이점을 나머지 업계로 확산시키는 것입니다. Google은 또한 Caliptra 오픈 소스 RoT 프로젝트[61]에도 기여합니다. OpenTitan과 같은 독립형 칩과 달리 Caliptra는 더 큰 칩의 일부인 통합 RoT 블록을 위한 IP 및 펌웨어로 구성됩니다. Caliptra는 CPU, GPU, DPU 및 TPU와 같은 데이터 센터급 SoC를 대상으로 합니다. 여기에는 SoC 내부의 RTM(Root of Trust for Measurement) 블록을 구현하기 위한 사양, 실리콘 로직, ROM 및 펌웨어가 포함됩니다. Caliptra 통합은 SoC에 ID, 측정 부팅(Measured Boot) 및 증명(Attestation) 기능을 제공합니다. 업데이트 가능한 스토리지가 있는 별도의 외부 RoT와 애플리케이션 프로세서 내부 RoT 로직의 조합을 통해 설계자는 전체 기계 수준에서 물리적 변조 저항의 기준을 높여 공격자의 비용을 크게 증가시킬 수 있습니다.

### 10.2.4 암호화 (Encryption)

암호화는 오랫동안 중요한 데이터를 보호하는 데 사용되었지만 광범위한 사용은 비교적 최근의 일입니다. 예를 들어 2010년 Gmail은 SSL을 기본적으로 사용하는 최초의 이메일 서비스였으며, 2014년에는 이를 의무화한 최초의 서비스였습니다. 부분적으로 이러한 광범위한 사용은 암호화에 대한 하드웨어 지원 개선으로 가능했습니다. 가장 큰 채택 장애물은 실제로 SSL/TLS 연결을 설정하고 유지하는 데 드는 계산 비용이었습니다.

저장 데이터 암호화(Encryption-at-rest)는 모든 스토리지 시스템에 저장된 데이터를 보호합니다. 종종 데이터는 스토리지 스택의 여러 수준에서 다른 키로 여러 번 암호화됩니다. 예를 들어 디스크 드라이브는 온보드 키와 암호화 엔진을 사용하여 쓰기 전에 데이터를 암호화할 수 있습니다. 따라서 디스크를 도난당하더라도 키도 도난당하지 않는 한 데이터를 읽을 수 없습니다. 그러나 임베디드 소프트웨어는 감사할 수 없고 버그가 포함될 수 있으므로 스토리지 소프트웨어 계층도 모든 데이터를 암호화합니다. 아마도 각 스토리지 볼륨 또는 각 스토리지 사용자마다 다른 키를 사용할 것입니다[62]. Google은 데이터베이스 스토리지 시스템이나 하드웨어 디스크에 기록되기 전에 데이터를 암호화합니다. 암호화는 나중에 추가되는 것이 아니라 모든 스토리지 시스템에 내재되어 있습니다.

모든 Google 스토리지 시스템은 유사한 암호화 아키텍처를 사용하지만 구현 세부 사항은 시스템마다 다릅니다. 데이터는 저장을 위해 서브파일 청크(chunk)로 나뉩니다. 각 청크의 크기는 최대 몇 기가바이트일 수 있습니다. 각 청크는 개별 데이터 암호화 키(DEK)를 사용하여 스토리지 수준에서 암호화됩니다. 동일한 고객이 소유하거나 동일한 기계에 저장되어 있더라도 두 청크는 동일한 DEK를 갖지 않습니다.

데이터 청크가 업데이트되면 기존 키를 재사용하는 대신 새 키로 암호화됩니다. 각각 다른 키를 사용하는 이러한 데이터 분할은 잠재적인 데이터 암호화 키 손상 위험을 해당 데이터 청크로만 제한합니다.

각 데이터 청크에는 고유 식별자가 있습니다. 액세스 제어 목록(ACL)은 승인된 역할로 작동하는 Google 서비스만이 해당 시점에 액세스 권한을 부여받아 각 청크를 해독할 수 있도록 보장합니다. 이러한 액세스 제한은 승인 없는 데이터 액세스를 방지하여 데이터 보안 및 프라이버시를 강화합니다.

각 청크는 스토리지 시스템 전체에 분산되며 백업 및 재해 복구를 위해 암호화된 형태로 복제됩니다. 고객 데이터에 액세스하려는 공격자는 두 가지를 알고 액세스할 수 있어야 합니다. 원하는 데이터에 해당하는 모든 스토리지 청크와 청크에 해당하는 모든 암호화 키입니다.

다음 다이어그램은 고객 데이터가 업로드된 후 저장을 위해 암호화된 청크로 나뉘는 방식을 보여줍니다(그림 10.8).

Google은 저장 데이터를 암호화하기 위해 AES 알고리즘을 사용합니다. 스토리지 수준의 모든 데이터는 기본적으로 AES-256을 사용하는 DEK로 암호화됩니다. AES-256과 AES-128 모두 미국 국립표준기술연구소(NIST)에서 장기 스토리지 사용을 위해 권장하며 AES는 종종 고객 규정 준수 요구사항의 일부로 포함되기 때문에 널리 사용됩니다.

전송 중 암호화(Encryption in transit)는 네트워크를 통해 전송되는 데이터를 보호합니다[63]. 예를 들어 암호화는 데이터가 고객 사이트와 클라우드 제공업체 간에 이동하거나 두 서비스 간에 이동하는 동안 통신이 가로채지는 것을 방지합니다. 이 보호는 전송 전 데이터 암호화, 엔드포인트 인증, 도착 시 데이터가 수정되지 않았는지 해독 및 확인하여 달성됩니다. 예를 들어 전송 계층 보안(TLS)은 전송 보안을 위해 전송 중 데이터를 암호화하는 데 자주 사용되며, S/MIME(Secure/Multipurpose Internet Mail Extensions)은 이메일 메시지 암호화에 사용됩니다.

저장 데이터 암호화와 마찬가지로 데이터는 여러 계층에서 여러 번 암호화될 수 있습니다. 예를 들어 라우터는 링크별 키[64]를 사용하여 모든 발신 패킷을 암호화할 수 있으며, 해당 링크를 사용하는 네트워크 연결은 TLS를 사용하여 전체 연결을 암호화하고, 해당 연결은 암호화된 이메일 메시지를 보내는 데 사용될 수 있습니다. 이러한 여러 수준의 암호화는 이론적으로는 바람직하지 않지만 적절한 계층화, 심층 방어를 가능하게 하며 (유비쿼터스 하드웨어 가속을 통해) 비용이 상당히 저렴합니다.

> **그림 10.8** Google 스토리지의 저장 데이터 암호화

암호화, 그리고 보안 전반은 그 자체로 책 한 권이 될 만한 매혹적인 분야입니다. 예를 들어 "단순한" 전송 중 암호화조차도 여기에 설명된 것보다 훨씬 더 많은 시스템을 포함합니다. 그 예로 Google Cloud 암호화의 개요를 참조하십시오[63]. 안전하고 신뢰할 수 있는 시스템 구축에 대한 심층적인 조언은 같은 이름의 책[3]을 추천합니다.

### 10.2.5 컨피덴셜 컴퓨팅 (Confidential computing)

기밀(Confidential) VM은 데이터가 처리되는 동안 사용 중인 데이터(data-in-use)를 암호화하여 클라우드에서 데이터의 기밀성을 보호할 수 있습니다. 컨피덴셜 컴퓨팅을 사용하면 모든 데이터가 칩 패키지를 떠날 때(DRAM에서도) 암호화되므로 공격자가 실행 중인 서버에 물리적으로 완전히 액세스하더라도 기밀성을 침해할 수 없습니다. CPU 내부에 대한 직접적인 공격이 필요합니다. 기밀 VM은 AMD, Intel, ARM 등의 최신 세대 CPU의 보안 기능을 활용합니다. 컨피덴셜 컴퓨팅을 통해 고객은 데이터가 클라우드에서 처리되는 동안에도 비공개 상태로 유지되고 암호화될 것이라고 확신할 수 있습니다.

기밀 VM 인스턴스는 두 가지 주요 이점을 제공합니다. 첫째, 격리(isolation): 암호화 키는 하이퍼바이저조차 액세스할 수 없는 전용 온칩 하드웨어에 의해 생성되고 그 안에만 존재합니다. 따라서 기계에서 가장 권한이 있는 서비스(하이퍼바이저)조차 암호화되지 않은 데이터에 액세스할 수 없습니다. 둘째, 증명(attestation): 고객은 VM의 ID와 상태를 확인하여 주요 구성 요소가 변조되지 않았는지("중간자" 공격 없음) 확인할 수 있습니다. 이러한 유형의 하드웨어 격리 및 증명은 신뢰 실행 환경(TEE)[65]으로 알려져 있습니다. 휴대폰에도 생체 인증 등을 위한 TEE가 포함되어 있습니다.

각 공급업체는 컨피덴셜 컴퓨팅 기능을 다르게 구현합니다. 2016년 AMD 보안 암호화 가상화(SEV [66])는 처음으로 출시된 구현으로 AMD Secure Processor를 통한 하드웨어 기반 메모리 암호화와 외부 TPM을 통한 부팅 시 증명을 제공합니다. CPU 내부의 고성능 암호화는 온칩 캐시와 메인 메모리 간에 교환되는 데이터를 해독하거나 암호화합니다. SEV 기밀 VM과 표준 VM 간의 성능 차이는 일반적으로 미미합니다.

SEV는 고성능을 허용하지만 악의적인 하이퍼바이저가 암호화된 데이터 재생(replay) 및 메모리 재매핑을 통해 기밀 환경을 손상시키는 일부 정교한 공격으로부터 보호하지 못합니다. AMD 보안 암호화 가상화-보안 중첩 페이징(SEV-SNP [67])은 SEV를 확장하여 하드웨어 기반 무결성 보호를 추가합니다. SEV-SNP 무결성의 기본 원칙은 VM이 개인(암호화된) 메모리 페이지를 읽을 수 있다면 항상 마지막으로 쓴 값을 읽어야 한다는 것입니다. 이 추가 보호의 단점은 특히 I/O 집약적 애플리케이션의 경우 추가 오버헤드입니다.

Intel Trust Domain Extensions(TDX [68])는 또 다른 하드웨어 기반 TEE입니다. TDX는 VM 내에 격리된 신뢰 도메인을 생성하고 메모리 관리 및 암호화를 위해 하드웨어 확장을 사용합니다. TDX는 메모리 콘텐츠 캡처, 수정, 재배치, 스플라이싱 및 별칭(aliasing)을 포함하여 DRAM에 대한 물리적 액세스를 사용하는 많은 형태의 공격으로부터 보호합니다. 그러나 재생 공격으로부터 보호하지는 않습니다. ARM의 CCA[69]는 운영 체제나 하이퍼바이저와 같은 권한 있는 소프트웨어로부터 워크로드를 보호하는 Realm이라는 하드웨어 격리 보안 환경 내에서 데이터와 코드를 보호합니다. SEV-SNP와 마찬가지로 CCA는 메모리 재생 공격을 저지하도록 설계되었습니다.

이러한 CPU 기능을 보완하기 위해 클라우드 제공업체는 고객이 클라우드 제공업체의 공간 외부에 호스팅된 하드웨어 보안 장치에 암호화 키를 저장할 수 있는 외부 키 관리(EKM)도 제공합니다[70]. TEE가 있는 CPU는 ID 상호 증명을 사용하여 클라우드 제공업체의 변조 또는 키 액세스 위험 없이 이러한 EKM에서 직접 키를 얻을 수 있습니다.

### 10.2.6 CPU 취약점

펌웨어 및 소프트웨어 무결성을 검증하는 실리콘 신뢰 루트 외에도 서버 보안은 하드웨어 자체의 신뢰성, 특히 버그 없는 CPU 명령어 성능에 의존합니다. 메모리 보호의 올바른 구현은 특히 중요합니다. 모든 운영 체제는 가상 메모리에 의존하여 별도 프로세스의 주소 공간을 격리하므로, 한 프로세스가 명시적으로 권한을 부여받지 않고 다른 프로세스의 메모리 내용을 읽는 것이 불가능합니다. 메모리 보호가 없으면 단일 CPU에서 서로 불신하는 워크로드를 효율적으로 실행하는 것이 어렵거나 불가능해집니다.

그러나 수십억 개의 회로로 구성된 CPU와 같은 복잡한 칩에는 공급업체의 최선 노력에도 불구하고 버그가 포함되어 있습니다. 이러한 버그는 일반적으로 새 칩 출시 초기에 발견되며 "errata(정오표)" 문서에 설명되어 있습니다. 대부분의 최신 칩에는 마이크로코드 형태의 온칩 소프트웨어가 포함되어 있으며, 종종 마이크로코드를 업데이트하여 이러한 버그를 수정할 수 있습니다(이러한 업데이트 자체는 실리콘 신뢰 루트에 의해 보호됨).

2017년까지 이러한 버그는 매우 드물었으며 최고의 보안 우려 사항으로 간주되지 않았습니다. 그러나 2017년 6월의 분기점에서 Google의 Project Zero 연구원들과 학계 연구원들은 거의 동시에 두 개의 버그를 발견했는데, 나중에 "아마도 역대 최악의 CPU 결함"으로 묘사되었습니다. 이는 두 버그 자체의 심각성 때문일 뿐만 아니라, 이것이 새로운 종류의 CPU 버그의 첫 번째 예시이며 앞으로 더 많이 나타날 것이고 거의 모든 최신 CPU에 영향을 미친다는 것이 즉시 명백했기 때문입니다[71].

이러한 첫 몇 가지 버그(Spectre[72], Meltdown[73], Foreshadow[74])는 공유 CPU 자원으로 인해 생성된 부채널(side channel) 공격의 위험을 보여주었습니다. 부채널 공격은 대상 기능(예: 메모리 보호)을 직접 공격하지 않고 대신 대상과 상관 관계가 있는 "부채널"을 사용하여 대상에 대한 비밀 정보를 알아내는 공격입니다. 예를 들어 비밀번호 검증 함수가 입력된 비밀번호와 저장된 비밀번호라는 두 문자열을 비교한다고 가정해 봅시다. 불일치를 발견했을 때 문자열 비교가 일찍 종료되고, 비밀번호 확인의 실행 시간을 충분히 높은 정밀도로 측정할 수 있다면, 이 타이밍 부채널을 통해 올바른 비밀번호를 추측할 수 있습니다. 그렇게 하려면 'a'로 시작하는 무작위 비밀번호를 선택한 다음 'b' 등을 선택하여 다른 것보다 오래 걸린 실행을 발견합니다. 그 실행은 문자열 비교의 첫 번째 반복이 성공하고 두 번째 반복이 실패한 실행이었으므로 이제 비밀번호의 첫 문자를 알게 됩니다.

실제로는 상황이 그리 간단하지 않습니다. 그러나 모든 CPU는 매우 정확한 타이머를 제공하며(성능 튜닝 및 기타 기능에 필요함), 물리적 CPU 테이블이 여러 프로세스를 제공하는 많은 인스턴스가 있습니다. 이러한 조합은 특히 투기적 실행(speculative execution)을 추가로 사용하는 CPU에서 악용될 수 있는 부채널을 생성합니다. 이러한 익스플로잇은 꽤 복잡하므로 여기서 설명하지 않겠지만, 원래의 두 버그는 광범위하게 문서화되었으며 이를 발견하는 복잡성뿐만 아니라 완화하는 복잡성도 잘 보여줍니다.

동시에 Spectre와 Meltdown 공격은 WSC, 따라서 퍼블릭 클라우드의 잠재적인 보안 이점을 보여주었습니다. 특히 Google은 보안 버그가 공개되기 6개월 전부터 알고 있었고 완화에 결정적인 기여를 할 수 있었습니다. 더 일반적으로 WSC 제공업체는 위험을 찾아 평가하고 완화 조치를 개발하거나 배포할 수 있는 고도로 숙련된 팀을 유지할 것으로 예상할 수 있습니다. WSC 제공업체는 CPU 마이크로코드, 시스템 소프트웨어 업데이트 또는 기타 패치를 신속하지만 제어된 방식으로 배포할 수 있는 조정된 관리 시스템을 사용합니다. 일부(Google Cloud 등)는 고객 워크로드 실행을 중단하지 않고 기본 기계를 업그레이드할 수 있도록 고객 VM의 라이브 마이그레이션도 지원합니다. 결과적으로 Spectre와 Meltdown이 공개되었을 때 Google Cloud 고객은 인프라가 이미 패치되었기 때문에 조치를 취할 필요 없이 이미 보호받고 있었습니다.

### 10.2.7 안전한 서비스 배포

앞서 논의한 바와 같이 클러스터 오케스트레이션 서비스는 WSC 인프라에서 직접 실행되는 서비스를 제어합니다. 이 섹션에서는 Google에 배포된 서비스에 대한 추가 보안 조치를 설명합니다. 다른 WSC 제공업체도 유사한 조치를 취할 수 있습니다.

인프라는 인프라에서 실행되는 서비스 간의 어떠한 신뢰도 가정하지 않습니다. 이 신뢰 모델을 제로 트러스트(zero-trust) 보안 모델이라고 합니다. 제로 트러스트 보안 모델은 네트워크 내부에 있든 외부에 있든 어떤 장치나 사용자도 기본적으로 신뢰하지 않는다는 것을 의미합니다.

인프라는 멀티 테넌트로 설계되었기 때문에 서로 다른 고객(소비자, 기업 및 Google 자체 데이터)의 데이터가 공유 인프라 전체에 분산됩니다. 이 인프라는 수만 대의 기계로 구성됩니다. 인프라는 고객이 Compute Engine의 단독 테넌트 노드(sole-tenant nodes)[75]를 사용하는 경우와 같은 특정 상황을 제외하고는 고객 데이터를 단일 기계나 기계 세트에 분리하지 않습니다.

#### 10.2.7.1 서비스 ID, 무결성 및 격리

서비스 간 통신을 가능하게 하기 위해 애플리케이션은 암호화 인증 및 권한 부여를 사용합니다. 인증 및 권한 부여는 관리자와 서비스가 이해할 수 있는 추상화 수준과 세분성으로 강력한 액세스 제어를 제공합니다.

서비스는 기본 보안 메커니즘으로 내부 네트워크 분할이나 방화벽에 의존하지 않습니다. 네트워크의 다양한 지점에서 수신 및 발신 필터링은 IP 스푸핑을 방지하는 데 도움이 됩니다. 고객은 VPC 서비스 제어[76] 및 Cloud Interconnect[77]와 같은 추가 보안 메커니즘을 구성할 수 있습니다.

인프라에서 실행되는 각 서비스에는 연관된 서비스 계정 ID가 있습니다. 서비스에는 RPC를 만들거나 받을 때 다른 서비스에 자신의 신원을 증명하는 데 사용할 수 있는 암호화 자격 증명이 제공됩니다. 이러한 ID는 보안 정책에 사용됩니다. 보안 정책은 클라이언트가 의도한 서버와 통신하고 있으며 서버가 특정 클라이언트가 액세스할 수 있는 방법과 데이터를 제한하고 있는지 확인합니다.

다양한 격리 및 샌드박스 기술은 동일한 기계에서 실행되는 다른 서비스로부터 서비스를 보호하는 데 도움이 됩니다. 이러한 기술에는 Linux 사용자 분리, 언어 기반(예: Sandboxed API[78]) 및 커널 기반 샌드박스, 컨테이너용 애플리케이션 커널(예: gVisor[79]) 및 하드웨어 가상화가 포함됩니다. 일반적으로 더 위험한 워크로드에는 더 많은 격리 계층을 사용합니다. 더 위험한 워크로드에는 추가 처리가 필요한 사용자 제공 항목이 포함됩니다. 예를 들어 사용자 제공 데이터에 대해 복잡한 파일 변환기를 실행하거나 App Engine 또는 Compute Engine과 같은 제품에 대해 사용자 제공 코드를 실행하는 것이 포함됩니다.

추가 보안을 위해 클러스터 오케스트레이션 서비스 및 일부 키 관리 서비스와 같은 민감한 서비스는 전용 기계에서 독점적으로 실행됩니다. 워크로드의 훨씬 더 강력한 암호화 격리와 사용 중인 데이터 보호를 위해 Google Cloud는 VM, GKE(Google Kubernetes Engine) 노드 등에 대해 컨피덴셜 컴퓨팅 서비스를 지원합니다[80].

#### 10.2.7.2 서비스 간 액세스 관리

서비스는 인프라에서 제공하는 액세스 관리 기능을 사용하여 해당 서비스와 통신할 수 있는 다른 서비스를 정확히 지정할 수 있습니다. 예를 들어 서비스는 허용된 다른 서비스 목록으로만 들어오는 RPC를 제한할 수 있습니다. 해당 서비스는 허용된 서비스 ID 목록으로 구성될 수 있으며 인프라는 이 액세스 제한을 자동으로 강제합니다. 시행에는 감사 로깅, 정당성 확인 및 일방적인 액세스 제한(예: 엔지니어 요청의 경우)이 포함됩니다.

서비스를 운영하는 엔지니어에게도 개별 ID가 발급됩니다. 서비스는 엔지니어의 ID에 따라 액세스를 허용하거나 거부하도록 구성될 수 있습니다. 이러한 모든 ID(기계, 서비스 및 직원)는 인프라가 유지 관리하는 글로벌 네임스페이스에 있습니다.

이러한 ID를 관리하기 위해 인프라는 승인 체인, 로깅 및 알림을 포함하는 워크플로 시스템을 제공합니다. 예를 들어 보안 정책은 다자간 승인을 강제할 수 있습니다. 이 시스템은 2인 규칙(two-person rule)을 사용하여 혼자 행동하는 엔지니어가 다른 승인된 엔지니어의 승인을 먼저 받지 않고 민감한 작업을 수행할 수 없도록 합니다. 이 시스템을 통해 안전한 액세스 관리 프로세스를 인프라에서 실행되는 수천 개의 서비스로 확장할 수 있습니다.

인프라는 또한 사용자, 그룹 및 멤버십 관리를 위한 표준 서비스를 서비스에 제공하여 필요한 경우 사용자 정의의 세분화된 액세스 제어를 구현할 수 있도록 합니다.

#### 10.2.7.3 서비스 간 통신 암호화

인프라는 네트워크상의 RPC 데이터에 대한 기밀성과 무결성을 제공합니다. 모든 Google Cloud 가상 네트워킹 트래픽은 암호화됩니다. 인프라 서비스 간의 모든 통신은 인증되며 대부분의 서비스 간 통신은 암호화되어, 네트워크가 도청되거나 네트워크 장치가 손상되더라도 통신을 보호하는 데 도움이 되는 추가 보안 계층을 추가합니다. 서비스 간 통신 암호화 요구사항에 대한 예외는 지연 시간 요구사항이 낮고 데이터 센터의 여러 물리적 보안 계층 내에서 단일 네트워킹 패브릭을 벗어나지 않는 트래픽에 대해서만 허용됩니다.

인프라는 데이터 센터 간 네트워크를 통과하는 인프라 RPC 트래픽에 대해 자동으로 효율적으로(하드웨어 오프로드의 도움으로) 엔드 투 엔드 암호화를 제공합니다.

### 10.2.8 추가 읽기 자료

시스템을 신뢰할 수 있고 안전하게 만드는 것은 어렵습니다. 물리적 캠퍼스에서 클라우드 사용자가 작성한 애플리케이션 코드에 이르기까지 WSC의 모든 수준에서 주의가 필요하기 때문입니다. 추가 읽기 자료로 Google SRE 및 보안 전문가가 저술한 "Site Reliability Engineering"[81] 및 "Building Secure and Reliable Systems"[3]라는 두 권의 책을 추천합니다.

이 장에서 가져가야 할 공통된 주제가 하나 있다면, 규모가 신뢰성과 보안을 모두 촉진할 수 있다는 것입니다. 규모가 그것들을 더 쉽게 만들지는 않지만, WSC 제공업체가 원칙적이고 확장 가능한 솔루션을 찾도록 강요합니다.

---

## References

1. J. R. Hamilton et al., “On designing and deploying internet-scale services.,” in LISA, vol. 18, 2007, pp. 1–18.
2. B. Beyer, N. R. Murphy, D. K. Rensin, K. Kawahara, and S. Thorne, The Site Reliability workbook: practical ways to implement SRE. ” O’Reilly Media, Inc.”, 2018.
3. H. Adkins, B. Beyer, P. Blankinship, et al., Building secure and reliable systems: best practices for designing, implementing, and maintaining systems. O’Reilly Media, 2020.
4. A. Grattafiori, A. Dubey, A. Jauhri, et al., The llama 3 herd of models, 2024. arXiv: 2407.21783 [cs.AI].
5. Red Hat, What is Linux kernel live patching? https://www.redhat.com/en/topics/linux/what-is-linux-kernel-live-patching.
6. Amazon Web Services, Amazon S3 service disruption information (July 20, 2008), 2008.
7. B. Chandra, M. Dahlin, L. Gao, et al., “Resource management for scalable disconnected access to web services,” in Proceedings of the 10th International Conference on World Wide Web, ser. WWW ’01, New York, NY, USA: Association for Computing Machinery, Apr. 1, 2001, pp. 245–256.
8. Z. S. Bischof, K. Pitcher, E. Carisimo, et al., “Destination unreachable: Characterizing internet outages and shutdowns,” in Proceedings of the ACM SIGCOMM 2023 Conference, ser. ACM SIGCOMM ’23, New York, NY, USA: Association for Computing Machinery, Sep. 1, 2023, pp. 608–621.
9. E. Brewer, “Lessons from giant-scale services,” IEEE Internet Computing, vol. 5, no. 4, pp. 46–55, 2001.
10. D. Oppenheimer, A. Ganapathi, and D. A. Patterson, “Why do internet services fail, and what can be done about it?” In 4th Usenix Symposium on Internet Technologies and Systems (USITS 03), 2003.
11. J. Gray, “A census of tandem system availability between 1985 and 1990,” IEEE Transactions on Reliability, vol. 39, no. 4, pp. 409–418, 1990.
12. W. Jiang, C. Hu, Y. Zhou, and A. Kanevsky, “Are disks the dominant contributor for storage failures? A comprehensive study of storage subsystem failure characteristics,” Article 7, vol. 4, New York, NY, USA: Association for Computing Machinery, Nov. 24, 2008, pp. 1–25.
13. D. Ford, F. Labelle, F. I. Popovici, et al., “Availability in globally distributed storage systems,” in Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation, ser. OSDI’10, USA: USENIX Association, Oct. 4, 2010, pp. 61–74.
14. D. Patterson, A. Brown, P. Broadwell, et al., “Recovery-oriented computing (ROC): Motivation, definition, techniques, and case studies,” Citeseer, Tech. Rep., 2002.
15. M. Kalyanakrishnam, Z. Kalbarczyk, and R. Iyer, “Failure data analysis of a LAN of Windows NT based computers,” in Proceedings of the 18th IEEE Symposium on Reliable Distributed Systems, 1999, pp. 178–187.
16. B. Schroeder and G. A. Gibson, “Understanding failures in petascale computers,” in Journal of Physics: Conference Series, IOP Publishing, vol. 78, 2007, p. 012 022.
17. B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM errors in the wild: A large-scale field study,” 1, New York, NY, USA: Association for Computing Machinery, Jun. 15, 2009, pp. 193–204.
18. A. A. Hwang, I. A. Stefanovici, and B. Schroeder, “Cosmic rays don’t strike twice: Understanding the nature of DRAM errors and the implications for system design,” in Proceedings of the seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS XVII, New York, NY, USA: Association for Computing Machinery, Mar. 3, 2012, pp. 111–122.
19. M. V. Beigi, Y. Cao, S. Gurumurthi, et al., “A systematic study of DDR4 DRAM faults in the field,” in 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA), 2023, pp. 991–1002.
20. Wikipedia contributors, Chipkill, Wikipedia https://en.wikipedia.org/wiki/Chipkill, 2025.
21. L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and J. Schindler, “An analysis of latent sector errors in disk drives,” in Proceedings of the 2007 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, ser. SIGMETRICS ’07, New York, NY, USA: Association for Computing Machinery, Jun. 12, 2007, pp. 289–300.
22. B. Schroeder and G. A. Gibson, “Understanding disk failure rates: What does an MTTF of 1,000,000 hours mean to you?,” 3, vol. 3, New York, NY, USA: Association for Computing Machinery, Oct. 1, 2007, 8–es.
23. E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large disk drive population,” in Proceedings of the 5th USENIX Conference on File and Storage Technologies, ser. FAST ’07, San Jose, CA: USENIX Association, 2007.
24. N. El-Sayed, I. A. Stefanovici, G. Amvrosiadis, A. A. Hwang, and B. Schroeder, “Temperature management in data centers: Why some (might) like it hot,” in Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems, ser. SIGMETRICS ’12, New York, NY, USA: Association for Computing Machinery, Jun. 11, 2012, pp. 163–174.
25. Y. Luo, S. Ghose, Y. Cai, E. F. Haratsch, and O. Mutlu, “Heatwatch: Improving 3D NAND flash memory device reliability by exploiting self-recovery and temperature awareness,” in 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), 2018, pp. 504–517.
26. SNIA, SSD Endurance, https://www.snia.org/forums/cmsi/ssd-endurance.
27. S. Maneas, K. Mahdaviani, T. Emami, and B. Schroeder, “A study of SSD reliability in large scale enterprise storage deployments,” in 18th USENIX Conference on File and Storage Technologies (FAST 20), 2020, pp. 137–149.
28. Solidigm, QLC NAND technology is ready for the data center, https://www.solidigm.com/content/dam/solidigm/en/site/products/technology/qlc-nand-technology-is-ready/documents/qlc-nand-ready-for-data-center-white-paper.pdf.
29. I. Stoica, D. Song, R. A. Popa, et al., “Cloud programming simplified: A Berkeley view on serverless computing,” in Flash Memory Summit, 2019.
30. B. Schroeder, R. Lagisetty, and A. Merchant, “Flash reliability in production: The expected and the unexpected,” in 14th USENIX Conference on File and Storage Technologies (FAST 16), 2016, pp. 67–80.
31. S. Han, P. P. Lee, F. Xu, et al., “An in-depth study of correlated failures in production SSD-based data centers,” in 19th USENIX Conference on File and Storage Technologies (FAST 21), 2021, pp. 417–429.
32. Y. Kong, M. Zhang, X. Zhan, R. Cao, and J. Chen, “Retention correlated read disturb errors in 3-d charge trap nand flash memory: Observations, analysis, and solutions,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, no. 11, pp. 4042–4051, 2020.
33. A. Cox, DDR5: Compelling Total Cost of Ownership (TCO), https://www.jedec.org/sites/default/files/Alvin_Cox%20%5BCompatibility%20Mode%5D_0.pdf.
34. Q. Li, M. Ye, T.-W. Kuo, and C. J. Xue, “How the common retention acceleration method of 3D NAND flash memory goes wrong?” In Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems, ser. HotStorage ’21, New York, NY, USA: Association for Computing Machinery, Jul. 27, 2021, pp. 1–7.
35. H. Liu, S. Lu, M. Musuvathi, and S. Nath, “What bugs cause production cloud incidents?” In Proceedings of the Workshop on Hot Topics in Operating Systems, ser. HotOS ’19, New York, NY, USA: Association for Computing Machinery, May 13, 2019, pp. 155–162.
36. P. H. Hochschild, P. Turner, J. C. Mogul, et al., “Cores that don’t count,” in Proceedings of the Workshop on Hot Topics in Operating Systems, ser. HotOS ’21, New York, NY, USA: Association for Computing Machinery, Jun. 3, 2021, pp. 9–16.
37. Google Cloud, Seagate and google predict hard disk drive failures with ML, Google Cloud Blog: https://cloud.google.com/blog/products/ai-machine-learning/seagate-and-google-predict-hard-disk-drive-failureswith-ml.
38. H. S. Gunawi, R. O. Suminto, R. Sears, et al., “Fail-slow at scale: Evidence of hardware performance faults in large production systems,” Article 23, vol. 14, New York, NY, USA: Association for Computing Machinery, Oct. 3, 2018, pp. 1–26.
39. S. Lu, B. Luo, T. Patel, et al., “Making disk failure predictions SMARTer,” in 18th USENIX Conference on File and Storage Technologies (FAST 20), 2020, pp. 151–167.
40. R. Lu, E. Xu, Y. Zhang, et al., “Perseus: A fail-slow detection framework for cloud storage systems,” in 21st USENIX Conference on File and Storage Technologies (FAST 23), 2023, pp. 49–64.
41. X. Fan, W.-D. Weber, and L. A. Barroso, “Power provisioning for a warehouse-sized computer,” in Proceedings of the 34th Annual International Symposium on Computer Architecture, ser. ISCA ’07, New York, NY, USA: Association for Computing Machinery, Jun. 9, 2007, pp. 13–23.
42. D. Serenyi, “Cluster-level storage @Google,” in Keynote at the 2nd Joint International Workshop on Parallel Data Storage and Data Intensive Scalable Intensive Computing Systems, 2017.
43. A. Verma, L. Pedrosa, M. Korupolu, et al., “Large-scale cluster management at Google with Borg,” in Proceedings of the Tenth European Conference on Computer Systems, ser. EuroSys ’15, New York, NY, USA: Association for Computing Machinery, Apr. 17, 2015, pp. 1–17.
44. A. Singh, J. Ong, A. Agarwal, et al., “Jupiter Rising: a decade of Clos topologies and centralized control in Google’s data center network,” 4, vol. 45, New York, NY, USA: Association for Computing Machinery, Aug. 17, 2015, pp. 183–197.
45. Google Cloud, Regions and zones, Compute Engine Documentation: https://cloud.google.com/compute/docs/regions-zones/.
46. Google Cloud, Deployment architectures - Infrastructure Reliability Guide, Google Cloud Architecture Center: https://cloud.google.com/architecture/infra-reliability-guide/design#deployment_architectures.
47. H. D. Dixit, S. Pendharkar, M. Beadon, et al., “Silent data corruptions at scale,” arXiv preprint arXiv:2102.11245, 2021.
48. Google Cloud, Compute Engine Service Level Agreement (SLA), https://cloud.google.com/compute/sla.
49. M. Shamsa and D. Lerner, “Defect mechanisms responsible for silent data errors,” in 2024 IEEE International Reliability Physics Symposium (IRPS), 2024, pp. 1–5.
50. D. F. Bacon, “Detection and prevention of silent data corruption in an exabyte-scale database system,” in The 18th IEEE Workshop on Silicon Errors in Logic – System Effects, 2022.
51. D. F. Bacon, “Detection and prevention of silent data corruption in an exabyte-scale database system,” in The 18th IEEE Workshop on Silicon Errors in Logic-System Effects (SELSE), 2022.
52. B. Liu, C. Scott, M. Tariq, et al., “CAPA: An architecture for operating cluster networks with high availability,” in Proceedings of the 21st USENIX Symposium on Networked Systems Design and Implementation, ser. NSDI’24, USA: USENIX Association, Apr. 16, 2024, pp. 1995–2010.
53. M. Al-Fares, V. Beauregard, K. Grant, et al., “Change management in physical network lifecycle automation,” in 2023 USENIX Annual Technical Conference (USENIX ATC 23), 2023, pp. 635–653.
54. Google, Security - Google Data Centers, https://www.google.com/about/datacenters/data-security/.
55. dup.
56. Wikipedia contributors, Stuxnet, Wikipedia, The Free Encyclopedia: https://en.wikipedia.org/wiki/Stuxnet.
57. Google Cloud, Application Layer Transport Security (ALTS), Google Cloud Security Documentation: https://cloud.google.com/docs/security/encryption-in-transit/application-layer-transport-security.
58. Google Cloud, Mutual TLS authentication overview, Cloud Load Balancing Documentation: https://cloud.google.com/load-balancing/docs/mtls.
59. U. Savagaonkar, N. Porter, N. Taha, B. Serebrin, and N. Mueller, “Titan in depth: Security in plaintext,” Google Cloud Identity and Security Blog, 2017.
60. OpenTitan, OpenTitan - Open Source Silicon Root of Trust, https://opentitan.org/.
61. Chips Alliance, Caliptra - Hardware Root of Trust, GitHub Repository: https://github.com/chipsalliance/Caliptra.
62. Google Cloud, Default encryption at rest, Google Cloud Security Documentation: https://cloud.google.com/docs/security/encryption/default-encryption.
63. Google Cloud, Encryption in transit, Google Cloud Security Documentation: https://cloud.google.com/docs/security/encryption-in-transit.
64. Google Cloud, MACsec for Cloud Interconnect overview, https://cloud.google.com/network-connectivity/docs/interconnect/concepts/macsec-overview.
65. Wikipedia contributors, Trusted execution environment, Wikipedia, The Free Encyclopedia: https://en.wikipedia.org/wiki/Trusted_execution_environment.
66. D. Kaplan, J. Powell, and T. Woller, AMD memory encryption, https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/memory-encryption-white-paper.pdf, 2016.
67. AMD, Strengthening VM isolation with integrity protection and more, White Paper, https://www.amd.com/content/dam/amd/en/documents/epyc-business-docs/white-papers/SEV-SNP-strengthening-vm-isolation-with-integrity-protection-and-more.pdf, 2020.
68. Intel, Intel®Trust Domain Extensions (Intel®TDX) Documentation, https://www.intel.com/content/www/us/en/developer/tools/trust-domain-extensions/documentation.html.
69. C. Garcia-Tobin and M. Knight, “Elevating security with ARM CCA,” 10, vol. 67, New York, NY, USA:Association for Computing Machinery, Sep. 26, 2024, pp. 34–39.
70. Google Cloud, Cloud Key Management Service, https://cloud.google.com/security/products/security-keymanagement#manage-encryption-keys-outside-the-cloud.
71. HotChips keynote: Spectre/Meltdown history, https://www.youtube.com/watch?v=d5XzVF0sAZo, 2018.
72. P. Kocher, J. Horn, A. Fogh, et al., “Spectre attacks: Exploiting speculative execution,” in 2019 IEEE Symposium on Security and Privacy (SP), 2019, pp. 1–19.
73. M. Lipp, M. Schwarz, D. Gruss, et al., “Meltdown,” arXiv preprint arXiv:1801.01207, 2018.
74. J. V. Bulck, M. Minkin, O. Weisse, et al., “Foreshadow: Extracting the keys to the intel SGX kingdom with transient Out-of-Order execution,” in 27th USENIX Security Symposium (USENIX Security 18), Baltimore, MD: USENIX Association, Aug. 2018, 991–1008.
75. Google Cloud, Sole-tenant nodes, Compute Engine Documentation: https://cloud.google.com/compute/docs/nodes/sole-tenant-nodes.
76. Google Cloud, VPC Service Controls Overview, VPC Service Controls Documentation: https://cloud.google.com/vpc-service-controls/docs/overview.
77. Google Cloud, Cloud Interconnect overview, Cloud Interconnect Documentation: https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview.
78. Google Developers, Sandboxed API (SAPI), https://developers.google.com/code-sandboxing/sandboxed-api.
79. Google, gVisor - Application Kernel for Containers, https://gvisor.dev/.
80. Google Cloud, Confidential Computing, https://cloud.google.com/security/products/confidential-computing.
81. B. Beyer, C. Jones, J. Petoff, and N. R. Murphy, Site Reliability Engineering: How Google runs production systems. ” O’Reilly Media.”, 2016.