# 11. 25년간의 WSC 여정

1998년 Google이 설립되었을 때, 성공적인 웹 검색을 위해서는 엄청난 양의 컴퓨팅 파워와 스토리지가 필요하며, 단일 컴퓨터로는 이 작업을 처리할 수 없다는 점이 이미 명백했습니다. 결과적으로, Google의 인프라 설계는 웨어하우스 스케일 컴퓨팅(Warehouse-Scale Computing, WSC)으로의 근본적인 전환을 알리는 신호탄이 되었습니다. 웹 검색에 최적화된 틈새 접근 방식으로 시작한 WSC는 지난 25년 동안 극적으로 진화했습니다. 오늘날 WSC는 모든 하이퍼스케일 기업과 클라우드 플랫폼을 뒷받침하는 주류 접근 방식이며, 클라우드에서의 차세대 AI/ML 컴퓨팅을 위한 토대가 될 준비를 마쳤습니다. 이 장에서는 WSC의 진화를 연대기순으로 기록하며, 중추적인 이정표, 얻은 교훈, 그리고 앞에 놓인 거대한 기회들을 강조합니다.

## 11.1 Google WSC 25년의 역사적 회고

이 책 전체에서 논의해 온 웨어하우스 스케일 컴퓨팅(Warehouse-scale computing)은 컴퓨팅 인프라 제공을 전체론적으로 고려하여, 전체 웨어하우스 규모의 데이터 센터를 하나의 컴퓨터로 설계하는 것입니다. 웨어하우스 스케일 컴퓨터(WSC)는 여러 하드웨어 인프라 구성 요소(컴퓨팅 및 스토리지 서버, 네트워킹 및 분산 시스템 설계, 데이터 센터 전력 및 냉각)뿐만 아니라 여러 소프트웨어 구성 요소(예: 데이터 센터 스케줄링, 관리, 하이퍼스케일/클라우드 워크로드)를 포괄합니다. 이들은 애플리케이션 성능, 총 소유 비용(TCO) 개선, 전력 효율성, 지속 가능성, 신뢰성, 규모에 따른 관리 용이성 등 여러 지표를 최적화합니다. 아래에서는 지난 25년간의 WSC를 5년 단위의 시대(Epoch)로 나누어 논의합니다(그림 11.1).

---

**[그림 11.1: Google의 웨어하우스 스케일 컴퓨팅 25년]**

**1기 (1999-2003)**
*   **주제:** 스케일 아웃 컴퓨팅 도입, 거칠지만 실용적인(Scrappy) 전문화
*   **개발 사항:**
    *   웹 인덱싱 및 서빙 애플리케이션
    *   저렴하고 임시변통적인 시스템 지름길(shortcuts)
    *   미니멀리스트 저비용 PC 서버, 4포트 스위치, 온보드 하드 드라이브
    *   제3자 코로케이션(Colo) 데이터 센터
    *   최적화의 총 비용, 낮은 하드웨어 신뢰성/관리 용이성에 초점

**2기 (2004-2008)**
*   **주제:** 규모의 적정화(Getting scale right), 기초적인 WSC 혁신
*   **개발 사항:**
    *   새로운 서비스 지원 (Gmail, Maps, YouTube, Android 등)
    *   Borg, Colossus, MapReduce, Bigtable, Chubby, CDN
    *   커스텀 서버, 멀티코어 수용, Clos 네트워크
    *   최초의 웨어하우스 스케일 데이터 센터
    *   효율성(에너지 비례성, PUE) + 소프트웨어의 신뢰성 강조

**3기 (2009-2013)**
*   **주제:** 네트워킹/보안 확립, 시스템 설계 확장
*   **개발 사항:**
    *   더 큰 이질성: 검색, 광고, 데이터베이스, 데이터 처리 등
    *   Spanner와 TrueTime, Colossus 스핀들/바이트 분할, SSD
    *   OpenCompute 메인스트림 WSC 서버, 스토리지 분리(disaggregation), SDN
    *   지속적인 데이터 센터 혁신: 오버서브스크립션(oversubscription), 랙 레벨 UPS
    *   정교한 공격에 대응하여 보안이 최우선 순위가 됨

**4기 (2014-2018)**
*   **주제:** 가속기와 소프트웨어 정의 하드웨어(SW-defined HW), 무어의 법칙 확장
*   **개발 사항:**
    *   머신러닝 워크로드가 범용 컴퓨팅 사이클을 추월
    *   ML을 위한 하드웨어-소프트웨어 공동 설계: TPU 칩, Pod 시스템, TensorFlow
    *   소프트웨어 정의 하드웨어 + 이기종 서버, Titan RoT(Root of Trust), 중전압(medium voltage) 전력
    *   OCS, 스파인 없는(spine-free) 패브릭, 엣지 및 클라우드 네트워킹
    *   더 높은 관리 용이성, 신뢰성; 킬러 마이크로초(killer microseconds) 및 데이터 센터 세금(tax)

**5기 (2019-2023)**
*   **주제:** 클라우드와 AI를 위한 설계, 지속 가능한 사회적 인프라
*   **개발 사항:**
    *   클라우드 및 AI 수요 폭발, 퍼블릭 클라우드의 하이퍼스케일 사용자들
    *   다양한 가속기 세대: 머신러닝, 비디오, NIC, 보안 등
    *   모든 곳에서의 소프트웨어 정의 인프라
    *   미니 분산 시스템으로서의 서버; 모듈식 데이터 센터 설계
    *   차세대 컴퓨팅을 위한 사회적 인프라로서의 WSC, 지속 가능성

---

### 11.1.1 1기: 스케일 아웃 컴퓨팅, 거칠지만 실용적인 전문화

Google 인프라 설계의 초기 5년(1999-2003)은 웹 검색 인덱싱 및 서빙에 최적화된 대규모 스케일 아웃 분산 컴퓨팅을 구축하고 배포하는 법을 배우는 시기였습니다. 해결해야 할 핵심 문제는 신뢰성(특히 스토리지), 더 큰 검색 인덱스와 더 높은 쿼리 볼륨을 위한 확장성, 그리고 쿼리당 비용 절감이었습니다.

이 기간 동안의 인프라 설계는 저렴하고 임시변통적인(scrappy) 최적화로 구성되었습니다. "제대로 할" 시간이 없었기 때문에 수많은 실용적인 지름길이 만들어졌습니다. 예를 들어, 초기 BigFile 스토리지 시스템은 장애에 대비해 미러링된 디스크를 사용했지만, 교체 디스크를 정의하고 살아남은 복사본을 새 디스크로 복제하는 작업을 수동으로 해야 했습니다. 마찬가지로, 파이썬으로 작성된 "베이비시터(babysitter)" 작업 스케줄링 스크립트는 각 작업을 특정 머신에 수동으로 배치하고, 머신이 고장 나면 스크립트를 업데이트해야 했습니다. 이 첫 번째 기간이 끝날 무렵, 이러한 애드혹(ad-hoc) 시스템들은 Google File System(GFS) [1], Global Work Queue(Borg의 초기 버전 [2]), 그리고 모든 Google 서비스 앞단의 리버스 프록시인 Google Front-End(GFE)를 포함한 더 견고한 설계로 대체되었습니다.

모든 하드웨어는 개인용 컴퓨터에서 흔히 볼 수 있는 범용 부품으로 만든 미니멀리스트 저가 서버로 구성되었습니다 [3]. 초기 서버 설계 중 하나는 단열을 위해 코르크를 사용하여 "코르크 보드 서버"라고 불리기도 했습니다. 이러한 선택은 하드웨어의 불안정성으로 이어졌으므로, 이 시기의 중요한 기여는 하드웨어 장애를 감안하고 이를 견딜 수 있도록 웹 검색 시스템을 설계한 것입니다. 시간이 지나며 하드웨어의 신뢰성은 높아졌지만, 규모가 커지면 일일 장애는 여전히 피할 수 없으며, 결함 허용(fault tolerant) 소프트웨어와 공동 설계된 하드웨어에 대한 이러한 강조는 오늘날에도 WSC 시스템의 핵심 원칙으로 남아 있습니다.

**[그림 11.2: 최초의 Google 코로케이션 주문서]**
*(참고: 1998년 9월 28일자 주문서 이미지로, Google Inc.의 초기 서버 호스팅 주문 내용을 보여줌)*

CPU는 주로 속도와 비용에 최적화되었습니다. 스토리지는 각 서버에 호스팅된 타사의 주류 하드 디스크 드라이브로 구성되었습니다. "스케일 업" 클러스터 네트워킹은 1Gbps 링크와 높은 오버서브스크립션(oversubscription)을 갖춘 4-post 576포트 스위치를 기반으로 하여 서버당 100Mbps의 오프 랙(off-rack) 대역폭을 허용했습니다. 이 설계는 웹 검색과 같은 대규모 클러스터 전체 애플리케이션에 대해 랙 수준의 데이터 배치를 요구했으며, 다시 한번 인프라와 애플리케이션의 공동 설계를 강조했습니다. 아직 Google 데이터 센터는 없었고, 모든 인프라는 제3자 코로케이션(colo) 데이터 센터에 호스팅되었습니다.

비용 최적화가 무엇보다 중요했으며, "총 소유 비용"(TCO)에 초점을 맞췄습니다. 하드웨어의 신뢰성과 관리 용이성은 덜 강조되었지만(경우에 따라 무시됨), 소프트웨어는 장애를 우회하도록 작성되었습니다. 코로케이션 데이터 센터는 전력이 아닌 공간 단위로 요금을 청구했으므로(그림 11.2), 처음에는 전력 효율성보다 공간 효율성이 더 중요했습니다. 이 시대가 끝날 무렵, 비효율적인 서버 전원 공급 장치와 온보드 DC-DC 변환 비용이 명확해지면서, 최초의 12V 전용 마더보드와 고효율 전원 공급 장치가 도입되었습니다.

### 11.1.2 2기: WSC의 기초적인 혁신들

Google의 두 번째 5년(2004-2008)은 검색 분야의 엄청난 성장(더 많은 카테고리 출시, 더 크고 빠르고 저렴해짐)과 새로운 서비스(Gmail, Maps, YouTube, Android, Chrome)의 도입으로 대변됩니다. 엔지니어링 리소스가 늘어나면서 더 야심 찬 솔루션이 가능해졌습니다. Borg [2]의 첫 번째 프로덕션 버전은 컨테이너를 사용하여 클러스터 활용도를 두 배로 높였습니다. Colossus는 GFS의 확장성 한계를 극복했습니다. MapReduce [4]와 Bigtable [5]은 데이터 처리를 단순화했으며, Chubby 락 서비스 [6], YouTube를 위한 콘텐츠 전송 네트워크(CDN), 고효율 Google 자체 구축 데이터 센터를 포함한 많은 다른 기본 서비스가 등장했습니다. 여러 면에서 이 기간은 기초적인 인프라를 바로잡는 시기였습니다.

인프라는 더 맞춤화된 설계로 전환되었습니다. Google은 선적 컨테이너를 사용하여 서버, 스토리지, 네트워킹, 전력, 냉각의 모듈식 집합체를 구축하기 시작했고, 이후 동일한 접근 방식을 웨어하우스 및 데이터 센터 규모로 적용했습니다. 뜨거운 공기 포집, 따뜻한 공기 냉각, 랙 레벨 UPS 등을 통해 데이터 센터 효율성이 크게 향상되었으며, 이는 오늘날까지 표준 관행이 되었습니다. 이 기간 동안 두 가지 핵심 개념이 개발되었습니다. (1) 서버가 완전히 활용되지 않을 때 전력 소비를 줄이는 에너지 비례성(energy proportionality) [7], (2) 데이터 센터 내 전력 전달 및 냉각 효율성을 추적하고 최적화하는 전력 사용 효율성(PUE)입니다.

서버는 클럭 속도 시대에서 멀티코어 시대로의 CPU 전환에 적응하고, 효과가 없었던 일부 기발한 아이디어(더 이상 코르크 보드나 벨크로는 없음!)에서 벗어나 더 맞춤형으로 제작된 설계로 전환되었습니다. 디스크는 여전히 서버에 직접 연결되었지만, 수직 자기 기록 방식의 도입으로 훨씬 더 높은 밀도를 허용했습니다.

이 시대의 맨 마지막에는 상용 실리콘(merchant silicon)을 기반으로 한 Clos 데이터 센터 네트워크가 도입되어 서버당 사용 가능한 오프 랙 대역폭이 크게 향상되었습니다. 대규모 애플리케이션은 계속해서 클러스터를 공유했고, 글로벌 캐시는 사용자에게 더 가까운 곳에서 트래픽을 처리하는 데 도움을 주었습니다.

효율성은 계속해서 가장 중요한 요소였으며, 전통적인 서버 비용을 넘어 전력/냉각 효율성까지 포함하게 되었습니다. 워크로드 계층(서로 다른 서비스 등급)과 리눅스 컨테이너는 여러 워크로드가 동일한 서버를 공유할 수 있게 했습니다. 하드웨어 신뢰성은 향상되었지만, 규모 면에서 장애는 여전히 예외적인 사건이 아닌 기본값으로 간주되었습니다. 증가된 규모에 대처하는 것은 여전히 중요한 관심사였습니다.

### 11.1.3 3기: 네트워킹과 보안의 확립

Google의 다음(세 번째) 5년(2009-2013)은 훨씬 더 큰 규모와 더 큰 워크로드 이질성, 그리고 이에 대응하여 시스템 설계를 지속적으로 확장하는 혁신으로 특징지어집니다. 이 시대의 특히 주목할 만한 세 가지 하이라이트는 네트워킹, 보안, 데이터베이스 분야의 혁신이었습니다.

WSC 워크로드 측면에서 검색과 광고는 여전히 중요했지만, 데이터베이스 및 데이터 처리 워크로드 또한 제 몫을 하기 시작했습니다. Google Drive는 2012년에 도입되었고, App Engine은 2011년에 프리뷰에서 벗어났습니다. Google Brain이라는 소규모 연구 그룹이 딥러닝을 연구하기 시작했습니다. 클러스터 및 스케줄링 연구를 돕기 위해 Google은 한 달 분량의 클러스터 작업 추적 데이터(trace) [8]를 공개했고, 2019년에 업데이트 [9]가 이어졌습니다.

Spanner [10]는 NoSQL의 결과적 일관성(eventual consistency) 전통에서 벗어나 분산 타임스탬핑(TrueTime)을 사용하여 강력한 일관성과 함께 고성능을 제공했습니다. Colossus는 더 나은 스토리지 효율성을 위해 삭제 코딩(erasure coding)을 도입했으며, 비용 상각 개선을 위해 바이트와 스핀들(탐색)의 개별 프로비저닝을 허용했습니다.

서버 설계는 대부분 CPU 공급업체의 로드맵을 따랐습니다. WSC 서버 원칙은 주류의 수용을 받기 시작했습니다. 많은 엔터프라이즈 기업들이 "하이퍼스케일" 사업부를 만들었고, OpenCompute [11]는 2011년에 설립되어 하이퍼스케일러를 넘어 WSC 컴퓨팅을 확산시켰습니다.

사내 FPGA 제어 NAND 플래시는 처음에는 검색용으로, 나중에는 전체 퓨릿(fleet) 워크로드를 위한 새로운 스토리지 계층을 제공했습니다. Clos 네트워크의 증가된 단면 대역폭(cross-section bandwidth)은 스토리지 분리(disaggregation)를 가능하게 하여, 디스크를 서버에서 분리해 더 나은 성능, 유지보수, 더 효율적인 스토리지 프로비저닝을 위해 설계된 별도의 디스크 어플라이언스로 옮겼습니다.

데이터 센터 네트워크를 위한 페타비트 규모의 단면 대역폭은 거의 모든 애플리케이션이 랙 지역성(rack locality)을 무시할 수 있게 했으며, SSD 스토리지조차 더 나은 활용과 프로비저닝을 위해 서버에서 분리할 수 있게 했습니다. 다른 지속적인 네트워킹 혁신은 분리 및 시스템 발전 [12]을 뒷받침했습니다. 특히 캠퍼스/WAN 수준 [13]에서의 비용 효율적인 엔터프라이즈 수준 상용 실리콘 네트워킹, 중앙 집중식 트래픽 엔지니어링을 갖춘 WAN 내 소프트웨어 정의 네트워킹(SDN) [14], 다중 우선순위 트래픽 전반에 걸친 종단 간 대역폭 강제 등이 있습니다. 지연 시간 변동성으로 인해 "규모에 따른 테일(tail at scale)" [15]에 대한 지속적인 집중이 강요되었습니다.

해외 행위자들의 정교한 공격과 스노든 유출 사건과 같은 사건들로 인해 보안이 큰 우선순위가 되었습니다. 시스템 아키텍처는 "제로 트러스트(zero-trust)"를 가정하고 저장 및 전송 중인 데이터의 광범위한 암호화를 채택한 설계로 대응했습니다. WSC 전력 오버서브스크립션은 실제 낮은 서버/클러스터 활용률에 맞춰 배포 규모를 조정함으로써 사용 가능한 데이터 센터 공간을 늘렸습니다.

### 11.1.4 4기: 가속기와 무어의 법칙 확장

다음 5년(네 번째) 시대(2014-2018)는 무어의 법칙과 관련된 성능 대 비용 스케일링의 급격한 둔화에 대응하여 실리콘 가속기, 더 이기종적인 하드웨어, 소프트웨어 정의 서버로 응답했습니다.

영감에 찬 베팅을 통해, Google은 WSC 혁신의 방향을 인공지능(AI) 및 머신러닝(ML) 워크로드를 위한 커스텀 실리콘으로 전환했습니다(그림 11.3). 그러한 첫 번째 가속기인 텐서 처리 장치(TPU) [16]는 2015년에 생산에 도달하여 CPU 및 GPU 대비 10배 더 나은 성능과 전력 효율성을 입증했습니다. 핵심 WSC 교리와 일치하게, 솔루션에는 하드웨어-소프트웨어 공동 설계(예: Tensorflow) 및 분산 시스템 설계(예: TPU "pod" 슈퍼컴퓨터)의 수많은 혁신이 포함되었습니다. ML 컴퓨팅은 기하급수적으로 성장하여 곧 비-ML 컴퓨팅을 위한 거대한 기존 컴퓨팅 기반을 따라잡았습니다. 2017년 말, ML 사이클은 전체 퓨릿(fleet)에서 비-ML 사이클을 넘어섰습니다.

**[그림 11.3: TPU의 세대들]**
*   **v1 (2015):** 칩당 1x 추론. 내부 추론 가속기.
*   **v2 (2018):** 칩당 1x / 포드당 1x. 분산 공유 메모리.
*   **v3 (2020):** 칩당 3x / 포드당 12x. 액체 냉각.
*   **v4 (2022):** 칩당 6.6x / 포드당 100x. 광학 재구성 가능.
*   **v5e (2023):** 칩당 4x 추론. 대규모 학습 및 추론을 위한 비용 효율성.
*   **v5p:** 칩당 21x / 포드당 750x. 가장 유연한 AI 가속기.
*   **Trillium (2024):** 100x v2 성능. 차세대 AI 모델 구현.
*   **Ironwood (2025):** 9,216 칩/포드, 6배 더 많은 HBM. 최첨단 칩, 가장 큰 포드.

무어의 법칙을 확장하기 위해, 그리고 그때까지 WSC의 동질적인 서버 구성과의 결별을 알리며, 시스템은 이기종 CPU, GPU, TPU 전반에 걸쳐 다양한 워크로드에 최적화되었습니다. 소프트웨어 정의 서버 [17]는 하드웨어 복잡성을 추상화했으며, 컴파일러/스케줄러/관리 소프트웨어는 워크로드를 그에 가장 적합한 컴퓨팅과 매칭했습니다. 이는 또한 맞춤형 설계에 대한 Google 접근 방식의 변화를 의미했습니다. 사내 전문화는 전략적인 신흥 워크로드에 더 집중하는 반면, 성숙한 WSC 하드웨어에 대해서는 더 많은 산업 표준화를 수용했습니다. Borg는 이 기간 동안 워크로드와 서버 모두의 더 큰 다양성을 수용하도록 상당히 진화하면서 평균 활용률을 크게 높였습니다 [18], [9]. 2014년은 Borg에서 영감을 받은 오픈 소스 컨테이너 관리자인 Kubernetes가 처음 출시된 해였습니다.

산업 트렌드는 CPU의 더 많은 코어 수, 더 높은 밀도와 더 낮은 전력을 위한 헬륨 충전 디스크, SSD 및 3D 적층 NAND 플래시를 위한 산업용 마이크로컨트롤러로 이어졌습니다. Google이 주도하여 서버 효율성은 주 전력 전달을 12V에서 48V DC로 전환함으로써 더욱 개선되었습니다. 네트워킹 혁신은 고속 NIC, 광 회로 스위칭(OCS) [19], "스파인 없는(spine-free)" 클러스터 패브릭 [20], 향상된 신뢰성과 기능 속도를 위한 소프트웨어 정의 네트워킹 관리 평면 최적화, 더 스마트한 엣지 최적화(Espresso [21]) 및 클라우드 가상 네트워킹(Andromeda [22])으로 계속되었습니다. 데이터 센터 인프라의 큰 혁신은 중전압 전력 전달 [23]의 성공적인 배포였습니다. 공격적인 계층별 워크로드 관리와 결합되어, 이는 훨씬 더 높은 전력 오버서브스크립션을 가능하게 했습니다. 또한 액체 냉각은 TPU 클러스터의 훨씬 더 높은 전력 밀도를 가능하게 했습니다.

성능, 비용, 전력, 보안, 관리 용이성, 신뢰성에 대한 전체론적 초점은 계속되었습니다. 사용자 공간에서의 OS 우회 기술을 포함한 베어 메탈 통신("Snap" [24])은 저지연 고-IOPS 스토리지를 허용했고, 새로운 지연 완화 기술은 "킬러 마이크로초" 시대 [25]를 다루었습니다. WSC 데이터 관리 및 분산 시스템 처리를 담당하는 주요 "데이터 센터 세금(tax)" 기능 [26]이 식별되어 최적화의 물결로 이어졌습니다. 전력 관리는 동적 주파수 및 절전 상태 제어를 도입했으며, 환경적 지속 가능성이 더 중요한 고려 사항이 되기 시작했습니다. 보안 개선은 기계 무결성과 민감한 데이터의 기밀성 및 무결성을 강조했습니다. Google은 2017년에 Titan 신뢰 루트(root-of-trust) 보안 칩을 도입했으며, 기조 연설자의 작은 귀걸이에서 극적으로 공개하여 작은 크기를 보여주었습니다!

### 11.1.5 5기: 클라우드와 AI, 지속 가능한 사회적 인프라 구축

가장 최근 5년(2019-2023)은 AI/ML 및 퍼블릭 클라우드 컴퓨팅을 중심으로 놀라운 혁신의 폭발을 목격했습니다. 검색 및 전통적인 하이퍼스케일 워크로드는 계속 성장하고 있지만, 이제는 더 큰 퍼블릭 클라우드 컴퓨팅 인프라 내에서 대규모 프라이빗 워크로드를 나타낼 뿐입니다. AI/ML 워크로드는 규모가 계속 증가했으며, 다양한 워크로드(학습, 튜닝, 서빙)와 프레임워크(예: Jax, Pytorch)가 사용되었습니다. WSC는 이제 차세대 컴퓨팅을 위한 기초적인 사회적 인프라입니다.

이 기간은 기술 스케일링의 지속적인 역풍 속에서도 WSC 인프라 규모의 또 다른 단계적 기능 증가를 기록했습니다. 여러 세대의 새로운 TPU 시스템은 칩 레벨(예: 희소 연산 지원)과 시스템 레벨(광학, 냉각을 포함한 새로운 상호 연결) 모두에서 새로운 혁신을 도입했습니다. 하드웨어 가속기의 여러 세대는 비디오 워크로드를 도왔으며, 극적인 비용 이점과 새로운 비디오 중심 기능 [27]을 가능하게 했습니다. (이러한 클라우드 규모의 비디오 가속이 미디어 산업에 미친 영향은 2024년 기술 에미상 수상으로 인정받았습니다.)

소프트웨어 정의 서버는 소프트웨어 정의 메모리 [28], 다중 ISA 이기종성(예: ARM 서버) 및 기타 하드웨어 기능(예: 칩렛 인식 스케줄링)을 위해 추가로 최적화되었습니다. 소프트웨어 정의 전력 관리는 데이터 센터의 여러 수준, 그리고 전력 및 냉각 전반에 걸쳐 더 광범위한 유형의 오버서브스크립션을 포함하도록 확장되었습니다 [29]. 네트워킹은 가속화된 네트워크 오프로드를 갖춘 WSC 규모의 SmartNIC 도입을 보았고, 지연 기반 혼잡 제어는 테일 레이턴시(tail latency)를 10배 개선했습니다 [30]. 소프트웨어 정의 네트워킹은 계속 진화했으며(예: 효율성을 위한 토폴로지 엔지니어링, 호스트 기반 경로 재설정), 새로운 분산 프로그래밍 모델(예: 더 선언적인 프로그래밍 수용)도 마찬가지였습니다.

기본 서버 설계는 계속 발전했습니다: 산업 표준 및 개방형 참조 구현; 폼 팩터 확산을 돕기 위한 모듈식 트레이; 공급망, 신뢰 루트, 기밀 컴퓨팅 및 기타 보안 최적화; 환경적 지속 가능성 및 수명 주기 탄소 발자국 관리. 전통적인 모놀리식 설계를 넘어, 서버는 관리, 보안, 네트워킹, 스토리지 등을 위해 자체 CPU를 가진 도터보드(daughterboard)인 여러 "아레나(arenas)" 전반에서 관리되는 미니 분산 시스템으로 진화했습니다.

스토리지 스케일링은 밀도와 바이트당 비용 모두에서 둔화되었습니다. HSMR(하이브리드 싱글 자기 기록) 디스크는 데이터 밀도를 개선했지만 새로운 데이터 배치 알고리즘을 필요로 했습니다. 플래시 어플라이언스는 컴퓨팅과 SSD 스토리지의 독립적인 확장을 지원했습니다. 데이터 센터 설계는 더 작은 단위로 배포되는 모듈식 조립식(prefab) 인프라로 비용과 건설 시간을 줄였으며, 물리적 서버 관리는 반복 작업을 자동화하기 위해 로봇 공학을 사용하기 시작했습니다.

이 5년 기간은 또한 지속 가능성을 일급(first-class) 설계 기준으로 더욱 집중했습니다. 2030년까지 넷제로(net-zero) 배출 및 연중무휴(24/7) 무탄소 에너지를 달성하겠다는 야심 찬 목표는 하드웨어(예: 내재 탄소 감소), 소프트웨어(예: 탄소 인식 스케줄링), 운영(예: 공급과 수요의 24/7 매칭) 전반에 걸친 여러 새로운 최적화를 필요로 했습니다. 점점 더 작아지는 트랜지스터 크기로 인한 새로운 신뢰성 변화는 증가된 결함(조용한 데이터 손상 [31])으로 나타났으며, 테스트 및 신뢰성에 대한 추가적인 집중을 동기 부여했습니다. 언제나 그랬듯 성능과 비용 효율성은 계속해서 가장 중요했지만, 이기종 멀티 테넌트 클라우드 인프라 환경에서 이러한 목표를 달성해야 하는 추가적인 과제가 있었습니다.

그림 11.4는 Google의 25년 WSC 설계에 걸친 6가지 서로 다른 서버 설계를 보여줍니다. 상단 세 사진은 초기 세대(일부 보드에 있는 코르크 보드와 벨크로에 주목)이고, 하단 세 개는 최근 세대입니다.
그림 11.5는 코로케이션 시설에 호스팅된 초기 Google 검색 서버 랙(왼쪽), 2005년의 컨테이너 기반 데이터 센터(가운데), 네덜란드의 현대적인 데이터 센터 캠퍼스(오른쪽)를 보여줍니다.
그림 11.6은 이 25년 여정의 일부로 개발된 주요 소프트웨어 시스템을 요약합니다.

**[그림 11.4: 여러 세대의 서버들]**
*(이미지 설명: 초기 코르크/벨크로 서버부터 현대적인 고성능 서버까지의 발전)*

**[그림 11.5: 웨어하우스 스케일 컴퓨터 내 서버 하우징의 진화]**
*(이미지 설명: 코로케이션 랙 -> 컨테이너형 데이터 센터 -> 현대적 데이터 센터 캠퍼스)*

**[그림 11.6: 주요 WSC 혁신의 타임라인 (내부 및 오픈 소스)]**
*   2002: GFS
*   2004: MapReduce, Bigtable
*   2006: Borg, Pregel
*   2008: Colossus
*   2010: FlumeJava
*   2012: Spanner, Dremel/BigQuery, Kubernetes (시작)
*   2014: Kubernetes, Protobuf, gRPC
*   2016: TensorFlow, Andromeda
*   2018: Istio
*   2020: ServiceWeaver
*   2022: Pathways, JAX
*   2024: OpenXLA

## 11.2 25년의 WSC 경험에서 얻은 주요 교훈

우리의 논의는 Google에 초점을 맞췄지만, 이는 WSC를 둘러싼 더 광범위한 산업 동향에 대한 개요 역할도 합니다. 아래에서 우리는 (때로는 어렵게!) 배운 것들을 요약하고 WSC를 위한 몇 가지 중요한 설계 원칙을 스케치합니다.

### 11.2.1 1급 설계 고려 사항으로서의 규모(Scale)

WSC 인프라는 서비스의 성장을 충족시키기 위해 전 세계적으로 확장되어야 합니다. 규모는 많은 전통적인 설계 고려 사항을 증폭시킵니다. 작은 구성 요소가 수억 달러의 비용을 추가할 수 있습니다. 작은 전력 오버헤드도 비슷하게 수백 메가와트의 전력으로 불어날 수 있습니다. 전통적으로 "드문"(예: 백만 분의 1) 또는 "불가능한"(소가 광섬유 링크의 간헐적 고장을 일으키는 것과 같은) 장애가 규모에 따라 나타날 수 있으며(실제로 나타납니다), 이는 신뢰성에 대한 다른 접근 방식을 동기 부여합니다. 대규모 WSC 퓨릿(fleet)을 유지 관리하는 데는 연간 수억 개의 부품을 처리하는 일이 포함될 수 있으므로, WSC의 물류 및 공급망 측면은 다른 설계 선택으로 이어질 수 있습니다. 단순함이 중요합니다: 복잡한 시스템은 본질적으로 배포 및 유지 관리가 더 어렵고, 장애가 발생하기 쉽고, 확장하기 어렵습니다.

WSC 설계는 컴퓨팅, 스토리지, 네트워킹 전반에 걸쳐 분산 시스템 중심입니다. 하지만 그러한 접근 방식이라도 10배의 규모 성장은 기존 접근 방식을 깨뜨리기 때문에 새로운 사고를 요구합니다. 아키텍트는 결코 끝나지 않습니다: 중요한 선택들은 성장의 매 시대마다 재검토되어야 합니다.

### 11.2.2 리스크 관리

리스크를 관리할 수 없다면 혁신할 수 없습니다. 검색과 같은 상위 레벨 서비스의 가용성을 위협하지 않으면서 신뢰할 수 없을지도 모르는 새로운 하드웨어 또는 소프트웨어를 도입할 수 있는 능력은 초기 WSC 컴퓨팅 혁신의 핵심 원동력이었습니다. 기술적 방어는 여러 수준에서 작동합니다: 시스템은 고장 난 서버나 디스크를 우회하여 일정을 조정하고, 블록 레벨 체크섬은 데이터 손상을 감지하며, 테스트 주도 소프트웨어 배포는 버그를 조기에 감지합니다.

하지만 기술적 수단을 넘어, 문화가 혁신의 가장 큰 원동력이며, 특히 가장 저평가된 WSC 측면인 "운영(operations)"의 경우 더욱 그렇습니다. 장애는 발생할 것이며, 이를 배움의 기회로 포용하는 것(비난 없는 포스트모텀을 통해)은 안전함(너무 느리게 감)과 무모함(너무 빠르게 감) 사이의 합리적인 균형을 확립할 수 있게 합니다. 사이트 신뢰성 엔지니어링(SRE) 분야는 WSC 설계의 핵심 요소로 부상하여 이미 거대한 시스템에서도 깊은 변화를 가능하게 했습니다. 예를 들어, Google이 소프트웨어 정의 네트워킹(SDN) 배포를 대규모로 발표했을 때, 한 통신사 임원은 "놀라운 것은 당신들이 최초의 SDN 백본을 설계했다는 것이 아니라, 그것을 실제로 롤아웃할 수 있었다는 점이다"라고 논평하며, 솔루션과 그 롤아웃을 공동 설계함으로써 리스크를 관리하는 운영팀의 독특한 능력을 지적했습니다.

### 11.2.3 심층적인 보안 관리

보안은 깊게 들어가야 합니다. 초기 세대의 WSC는 인터넷처럼 비교적 개방적이고 신뢰 기반으로 시작되었으며, 상업적 동기를 가진 외부 공격자에 대해서는 잘 방어했습니다. 그러나 국가 주도 행위자(nation-state actors)를 방어하려면 훨씬 더 깊은 방어가 필요합니다. 특히, 서버는 펌웨어와 운영 체제를 검증하고 보호하기 위해 안전하고 별도인 실리콘 신뢰 루트(root of trust)를 가져야 합니다; 모든 데이터는 암호화되어야 하며, 이상적으로는 처리 중일 때도 암호화되어야 합니다; 모든 직원은 피싱 저항성이 있는 2차 인증을 사용해야 합니다; 시스템은 제로 트러스트(zero trust)를 가정해야 합니다; 사용자 또는 고객 정보에 대한 중요한 조치나 접근은 다자간 승인을 사용해야 합니다; 모든 프로덕션 코드는 검토되어야 하며, 모든 바이너리는 검증 가능한 출처를 가져야 합니다. 고도로 숙련된 레드 팀(Red Teams)은 정기적으로 방어(물리적 보안 포함)를 테스트해야 합니다.

이러한 과거의 투자는 결과를 낳았습니다: 오늘날 퍼블릭 클라우드와 하이퍼스케일러 보안은 일반적인 엔터프라이즈 보안을 훨씬 능가합니다. 그러나 보안은 해결된 문제와는 거리가 멉니다. 오픈 소스 및 상용 소프트웨어에 대한 공급망 공격은 모든 시스템을 위협하며 모든 단계에서 깊은 방어가 필요합니다. 시스템은 구성하기 너무 복잡하여 실수를 방지하기 위해 내장된 자동 감사가 필요합니다. AI 지원 도구는 계속 증가하는 로그 더미 속에서 타협의 징후인 '바늘'을 감지해야 합니다. 더 나은 남용 탐지 기능은 악의적인 행위자가 퍼블릭 클라우드를 사용하여 타인을 공격하는 것을 방지해야 합니다. 통신 시스템은 프라이버시를 보존하면서 피싱 또는 사칭 공격을 방지해야 합니다. 디지털 인프라의 중요성이 계속 커짐에 따라 보안에 대한 기대치도 계속 커질 것입니다. 보안을 무시하면 위험에 처할 것입니다; 편집증적인 자만이 살아남습니다(only the paranoid survive).

### 11.2.4 수직 통합 시스템 설계

WSC는 시스템 설계의 여러 계층을 수직적으로 통합합니다: 데이터 센터 인프라(전력, 냉각, 건물), 실리콘 및 하드웨어(가속기, 서버, 스위치, 스토리지), 시스템(소프트웨어 정의 네트워킹, 스케줄러, 데이터 센터 관리), 서비스/플랫폼(IaaS, PaaS). 마찬가지로, 설계는 전체 수명 주기에 걸쳐 최적화합니다: 공급망, 엔지니어링 설계, 배포, 운영, 지속 가능성.

공동 설계(co-design)에 대해 의도적이어야 하며, 잘 정의된 인터페이스와 추상화를 통해 계층 간의 기능 분할과 관심사 분리를 명확히 지정해야 합니다. 좋은 예는 하드웨어의 모든 리스크를 제거하는 비용이 너무 많이 들 때 하드웨어 결함 허용을 보완하기 위해 소프트웨어를 사용하는 것입니다. 동시에, 규율 없는 공동 설계는 너무 밀접하게 결합된 시스템을 만들거나 "기술의 섬(tech islands)"(아래 참조)을 만들 수 있습니다.

마찬가지로 공동 설계를 위한 올바른 지표를 선택하는 것이 중요합니다. 하드웨어별 지표가 도움이 될 수 있지만, 상위 수준의 애플리케이션 수준 지표(및 서비스 수준 목표[SLO])는 전역 목표 함수(예: 쿼리당 서비스 비용)를 최적화할 기회를 식별하는 데 추가적인 도움을 줍니다. 계측(Instrumentation) 및 모니터링 [32]은 종종 없어서는 안 될 요소입니다: 데이터 수집에 약간의 오버헤드가 발생할 수 있지만, 우리는 계측 기반 최적화가 이러한 오버헤드보다 훨씬 더 크다는 것을 지속적으로 발견했습니다. 머신러닝은 신중하게 선별된 운영 데이터의 가치를 더욱 강화합니다.

### 11.2.5 하드웨어-소프트웨어 공동 설계 및 전문화

WSC는 항상 하드웨어-소프트웨어 공동 설계를 강조해 왔으며, 무어의 법칙에 따른 일반적인 개선이 감소함에 따라 워크로드 전문화의 타당성이 더 강해졌습니다. AI 학습 및 추론을 위한 TPU는 여러 계층에 걸친 전문화의 가장 두드러진 예를 나타냅니다: (1) 하드웨어 계층의 행렬 곱셈을 위한 시스톨릭 배열(systolic arrays) 및 광학 스위치; (2) 시스템 계층의 재구성 가능한 광학 네트워크를 갖춘 스케일 업 및 스케일 아웃 시스템; (3) 컴파일러의 딥 퓨전(deep fusion), 유연한 병렬화 및 통신 스케줄링; (4) 모델/워크로드 계층의 bfloat16 및 fp8 수치와 SparseCores; (5) 애플리케이션 및 알고리즘 계층의 전문가 혼합(mixture-of-experts) 또는 신경망 아키텍처 탐색(NAS)과 같은 최적화; (6) 데이터 센터 계층의 액체 냉각 시스템을 포함한 에너지 최적화.

우리의 VCU 비디오 가속기 설계는 여러 계층에 걸친 공동 설계에 유사한 접근 방식을 따릅니다 [27]. 두 예시 모두 명령어 집합 아키텍처(ISA) 수준에서의 고전적인 공동 설계를 넘어 시스템 스택 전반에 걸친 모든 옵션을 수용하는 사고방식의 이점을 보여줍니다. 흥미롭게도, 새로운 가속기의 성공적인 공동 설계를 위한 핵심 기술은 잘 설계된 소프트웨어 시스템을 뒷받침하는 기술과 크게 다르지 않습니다: 복잡성을 최소화하기 위한 문제의 신중한 분해; 로컬 정보를 추상화하고 의존성을 최소화하는 인터페이스의 신중한 정의 등입니다. 동시에, 규모에 따른 하드웨어 개발의 긴 리드 타임과 비용을 다루는 것은 모듈식 개발/테스트 및 프로젝트 간 재사용을 선호하게 합니다.

### 11.2.6 기술의 고립(Islands)과 산업 생태계

WSC의 초기 성공은 남다름(different)에 의해 주도되었으며, 필연적으로 당시의 전통적인 시스템 설계 접근 방식을 재창조해야 했습니다. 그러나 WSC가 확장되고 퍼블릭 클라우드 제공업체를 통한 채택이 증가함에 따라, 이제 광범위한 산업 생태계가 WSC 사용 사례를 지원하여 "구축 대 구매(build vs buy)" 결정을 가능하게 합니다.

커스텀 설계는 시장의 기존 솔루션으로 비용 효율적으로 충족되지 않는 WSC 워크로드 또는 시스템의 고유한 요구 사항을 목표로 할 때 가장 잘 작동합니다(예: WSC 머신러닝 워크로드를 위한 TPU 가속기 설계). 그러나 더 성숙한 시장의 경우, 대량 경제가 종종 비용을 줄이고 속도를 높여 산업 표준 위에 구축된 제품을 선호하게 합니다(예: 서버 및 랙 폼 팩터).

표준화된 인터페이스를 기반으로 구축된 모듈식, 구성 가능한(composable), 상호 운용 가능한 아키텍처를 구축하는 데 집중하세요. 구성 가능성과 표준에 집중하지 않으면, 하나의 커스텀 구성 요소가 다른 모든 것도 커스텀이 되도록 강제하는 자신만의 고유한 "기술의 섬(tech island)"에 갇힐 수 있습니다. 여러 면에서 이는 모놀리스 대 마이크로서비스의 트레이드오프에 해당하는 하드웨어적 등가물입니다.

### 11.2.7 출시(Launch)보다 안착(Landing)을 중시하는 문화

25년 넘게 WSC를 설계하면서 우리는 팀 문화에 대한 몇 가지 중요한 교훈을 얻었습니다. 그중 하나는 신제품이나 기술의 출시에 집중하는 대신 "안착(landing)한다는 것이 무엇을 의미하는지"에 집중하는 것이 훨씬 더 중요하다는 것입니다. 결국 중요한 것은 발사가 아니라 아폴로 11호의 착륙이었습니다. 제품 출시는 팀들에게 잘 이해되고 있으며, 이를 축하하기 쉽습니다. 그러나 출시 자체가 성공을 만들어내지는 않습니다. 안착은 항상 자명하지 않으며, 더 행복한 사용자, 기뻐하는 고객 및 파트너, 더 효율적이고 견고한 시스템과 같은 명시적인 성공 정의가 필요하며, 달성하는 데 더 오래 걸릴 수 있습니다.

그러한 안착 지표를 선택하는 것이 쉽지는 않을 수 있지만, 그 결정을 조기에 내리도록 강제하는 것이 성공에 필수적입니다. 안착은 프로젝트의 "이유(why)"입니다.

### 11.2.8 성공적인 결정을 재검토하는 문화

여러 시대에 걸쳐, 우리는 과거의 기술적 결정을 재검토해야 했습니다. 심지어 원래의 결정이 모범적인 성공으로 이어졌을 때조차도 말입니다. 예를 들어, 수년 동안 우리는 커스텀 실리콘 구축을 거부했습니다. 커스텀 설계는 일반 CPU의 개선 속도에 쉽게 추월당할 수 있고, 구축에 필요한 엔지니어링 노력을 상각하기 위해 대량의 볼륨이 필요하기 때문입니다.

하지만 딥러닝이 실용화되었고 CPU 대비 고도로 전문화된 프로세서의 속도 향상이 엄청났기에, 우리는 첫 번째 TPU를 만들었습니다. 거의 10년 후, TPU는 ML 기반 제품의 핵심 차별화 요소로 남아 있습니다. 마찬가지로, 스트리밍 비디오 압축 칩 시장의 부재와 결합된 YouTube의 큰 대역폭 비용은 우리가 VCU를 만들도록 이끌었으며, 현재 3세대에 이르렀습니다. 비슷한 이야기가 수년 동안 여러 다른 설계 선택에서 전개되었습니다.

설계 결정을 재검토하는 것을 두려워하지 마십시오. 특히 스케일링의 모든 단계적 기능(step function)에서 그렇습니다.

### 11.2.9 루프샷(Roofshots)을 통한 문샷(Moonshots) 창출

지난 25년 동안 우리는 수십 배(order-of-magnitude)의 개선(널리 알려진 10배 개선 또는 "문샷(moonshots)")을 달성했습니다. 그중 다수는 더 작은(1.3-2배) 기회들의 체계적이고 끈질기며 지속적인 추구, 또는 우리가 내부적으로 "루프샷(roofshots, 지붕을 향한 샷)"이라고 부르는 것(Luiz Barroso가 만든 용어)의 결과였습니다.

예를 들어, 초기 WSC 인프라는 제3자 시설에 있는 기계 더미와 더 잘해보겠다는 의도로 시작되었습니다. 우리는 전기 효율을 계속 개선하고, 공기 흐름 공급 방식을 변경하고, 더 빠른 건설과 더 높은 비용 효율성을 위해 시설 조각들을 사전 제작(pre-fab)하는 법을 배웠으며, 결국 예정된 다운타임을 거의 제거하는 방법을 알아냈습니다. 루프샷에 루프샷을 거듭하다 보니 갑자기 우리는 전통적인 설계와 근본적으로 다른, 세계에서 가장 효율적이고 신뢰할 수 있는 데이터 센터를 갖게 되었습니다.

멀리서 보면 이런 종류의 성과는 문샷으로 오해받을 수 있습니다. 사실 그것들은 일련의 루프샷들이었습니다. 일련의 루프샷은 빠른 수익과 지속적인 혁신적 결과를 모두 낳을 수 있습니다. 우리의 고인이 된 동료 Luiz의 말에 따르면: "우리가 지붕으로 가기로 선택한 것은 그것이 화려해서가 아니라 바로 거기에 있기 때문입니다. 밖으로 나가 거대한 꿈을 꾸고, 다음 날 아침 출근해서 끈질기게 점진적으로 그것들을 성취해 나가십시오."

## 11.3 앞으로의 전망

25년의 WSC 이후, 우리는 흥미로운 변곡점에 있습니다. 한편으로는 클라우드 컴퓨팅과 AI의 성장에 힘입어 컴퓨팅 수요가 폭발할 준비가 되어 있습니다. 다른 한편으로는 기술 스케일링의 둔화가 비용과 에너지 효율성을 확장하는 데 지속적인 과제를 제기합니다. 이 두 가지를 합치면, WSC 설계를 위한 가장 흥미로운 해들이 아직 우리 앞에 있다는 것을 의미합니다 [33]. 아래에서 우리는 커뮤니티를 위한 몇 가지 거대한 과제와 기회를 요약하며, 특히 확장, 민첩성, 신뢰, 지속 가능성, 그리고 WSC 설계를 위한 AI 사용의 파괴적 잠재력을 핵심 주제로 강조합니다.

### 11.3.1 무어의 법칙을 확장하기 위한 효율성 혁신

위에서 설명한 과제에 대응하려면 효율적인 설계(커스텀 실리콘), 효율적인 활용(소프트웨어 정의 하드웨어), 효율적인 소프트웨어(더 나은 알고리즘) 전반에 걸친 지속적인 혁신이 필요합니다.

우리는 이기종 가속기 수준 병렬 처리에 최적화된, 세분화되고(fine-grained) 굵직한(coarse-grained) 수많은 커스텀 실리콘 가속기를 계속 보게 될 것입니다. "기존의 것을 더 빠르고 저렴하게 하는 것"을 넘어, 가속기는 이전에는 불가능했던 애플리케이션 수준의 새로운 기능을 위한 기회를 창출합니다.

소프트웨어 정의 하드웨어는 여러 소프트웨어 수준에 걸쳐 더 정교한 제어와 자동화를 필요로 할 것입니다. WSC 환경에서의 물리적 자동화를 포함한 데이터 센터 관리를 위한 자동화가 널리 보급될 것입니다. "최상층의 여유 공간(Room at the top)" [34] 최적화는 상위 수준 언어 및 알고리즘, 소프트웨어 개발 관행, 하드웨어별 튜닝의 비효율성을 해결해야 합니다. 하드웨어와 소프트웨어 전반에 걸친 WSC 규모의 공동 설계는 이를 가능하게 하는 데 중요할 것입니다.

### 11.3.2 개발 속도 최적화

무어의 법칙의 전통적인 공식(성능은 동일한 비용으로 2년마다 두 배가 됨)은 성능, 비용, 시간이라는 세 가지 변수에 중점을 둡니다. 성능과 비용 개선이 둔화됨에 따라, 시간(하드웨어 개발 속도)에 집중하는 것은 지속적인 개선을 위해 "곡선 아래 면적(area-under-the curve)"을 최적화하는 좋은 방법이 될 수 있습니다. 점진적인 더 작은 이익들이 복리화되면 여전히 기하급수적인 이익을 달성할 수 있습니다.

이러한 민첩하고 빠른 개선을 달성하려면 인터페이스, 표준 등에 적절한 투자를 하여 더 모듈화된 하드웨어 플랫폼을 구축해야 합니다. 특히 칩렛(Chiplets)은 다중 다이 시스템 맥락에서 공동 설계를 가능하게 하여 [33], 다이 기하학적 구조의 비용 이점뿐만 아니라 이기종 IP 블록과 다양한 공정 기술 전반에 걸친 혼합 및 매치 통합을 허용합니다.

특히 흥미로운 발전인 오픈 소스 하드웨어는 하드웨어 설계자가 기반으로 삼을 수 있는 협업적이고 더 빠른 속도의 생태계를 가능하게 합니다. 오픈 소스 IP 블록(예: Caliptra 신뢰 루트), 검증 및 테스트 제품군(예: CHIPS alliance, OpenCompute), 심지어 오픈 소스 도구/PDK(예: OpenRoad) 등이 있습니다. 오픈 소스 소프트웨어가 WSC에 얼마나 심오했는지를 감안할 때, 오픈 소스 하드웨어의 기회는 상당합니다.

### 11.3.3 신뢰할 수 있는 컴퓨팅과 주권(Sovereignty)

새로운 신뢰성 과제, 특히 최근의 조용한 데이터 손상(silent data corruption) 증가는 발견 가능성, 국소화, 근본 원인 분석부터 서비스 가능성 및 복원력, 새로운 아키텍처 개선에 이르기까지 새로운 연구를 필요로 합니다. WSC는 향상된 소프트웨어 정의 복원력과 적응형 서비스 수준 최적화를 통해 대규모 현장(in-situ) 모니터링 및 테스트를 지원하도록 설계되어야 합니다.

마찬가지로, 새로운 프라이버시 및 보안 과제(새로운 내부 및 외부 위협 모두)는 새로운 솔루션을 필요로 합니다. (우리는 인프라 속성인 보안과 애플리케이션 속성인 프라이버시를 구분합니다.) 기밀 컴퓨팅(Confidential computing), 동형 암호화(homomorphic encryption), 그리고 스택의 상위 수준에서 더욱 정교한 사기/남용/위협 탐지는 모두 상당한 투자를 필요로 합니다. 주권(Sovereignty)은 데이터 거주지와 관할권의 명확성을 제공하고 더 구획화된 보안 속성을 가능하게 하기 위해 1급 WSC 설계 고려 사항이 될 것입니다.

### 11.3.4 사회적 인프라로서의 지속 가능성

환경적 지속 가능성은 온실가스 배출, 물 사용, 유해 물질과 같은 다양한 영역에 걸친 도전이자 기회입니다. 배출은 세 가지 범주로 나뉩니다: Scope 1 배출(예: 지구 온난화 지수가 높은 공정 가스로부터), Scope 2 배출(예: 구매한 전기로부터), Scope 3 배출(예: 가치 사슬 내 간접 배출). 이 세 가지 범주 모두를 줄일 기회가 풍부합니다: 운송 발자국 감소, 물 활용 및 효율성 개선, 유해 물질 최소화, 에너지 소비 감소 및 더 청정한 에너지원, 순환 경제 채택, 클라우드로의 컴퓨팅 이동 등입니다. 업계는 환경 영향을 측정하고 보고하기 위한 일관된 지표를 채택해야 하며, 더 광범위한 산업 생태계는 모범 사례를 표준화하고 공유해야 합니다.

### 11.3.5 WSC 설계를 위한 AI

WSC 설계는 현재 AI/ML 워크로드에 초점을 맞추고 있지만, WSC를 *설계*하는 데 AI를 사용하는 것에 대한 동등한 초점은 아직 보지 못했습니다. 미래의 WSC는 AI가 만든 하드웨어와 소프트웨어를 특징으로 할 것입니다. AI는 커버리지와 다양성을 높이면서 더 빠른 반복을 가능하게 할 수 있지만, 테스트 및 검증에 대한 새로운 과제도 제기할 것입니다. AI 지원 도구는 대규모 소프트웨어 시스템의 재작성을 잠재적으로 가능하게 하여, 오래된 생태계에서 빠르게 진화하고 복잡한 상호 의존 시스템 전반의 인터페이스를 진화시킬 새로운 기회를 열어줍니다. AI/ML 기술은 또한 제어 및 자동화를 최적화할 수 있으며, 특히 소프트웨어 정의 하드웨어의 맥락에서 그렇습니다.

AI는 칩 설계에 특히 혁신적일 수 있습니다 [35]. 지난 몇 년 동안 수명 주기 전반에 걸쳐 ML을 사용하는 데 있어 상당한 진전이 있었습니다: RTL 합성; 배치 및 배선; 검증 및 유효성 검사. 더 큰 상태 공간에 걸친 더 정교한 모델은 전력, 성능, 면적, 타이밍 규칙, 혼잡도 등을 아우르는 여러 복잡한 목표 전반에서 최적화할 수 있습니다. 떠오르는 대규모 언어 모델(LLM)은 개발자 경험을 극적으로 단순화할 수 있습니다. 칩을 설계하는 데 수백 명의 사람이 몇 달 동안 작업해야 하는 오늘날과 달리, 머신러닝은 몇 주 또는 며칠 만에 칩을 설계하는 꿈을 가능하게 할 수 있습니다. 우리는 이 분야에서 이제 막 시작했습니다.

## 11.4 맺음말

우리의 작업은 지난 25년 동안 WSC에서 일해 온 수천 명의 재능 있는 엔지니어와 연구원들의 작업 위에 세워졌습니다. 그들의 작업은 WSC를 실험실에서 기초적인 웹 검색 알고리즘을 실행하는 수작업 컴퓨터 세트에서 오늘날의 기초적인 사회적 인프라로 발전시키는 데 도움이 되었습니다.

우리는 특히 이 책을 우리의 고인이 된 동료, 루이스 안드레 바로소(Luiz André Barroso)에게 바치고 싶습니다. 루이스는 2000년대 초반 Google에 합류하여 이 논문에서 논의된 많은 아이디어를 실현하는 데 핵심적인 역할을 했습니다. 루이스의 작업은 전체 산업의 기초를 닦았습니다. 그는 단순히 WSC를 확장하는 방법만 연구한 것이 아니라, WSC 혁신을 확장하는 방법에 대해서도 깊이 생각했습니다. 우리는 이 장이 WSC 혁신의 역사를 요약할 기회를 준 것에 감사하며, 이 논문이 커뮤니티의 다른 사람들이 그의 위대한 유산을 계속 이어가는 데 영감을 주기를 바랍니다.

---

### References

1. S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,” in *Proceedings of the nineteenth ACM Symposium on Operating systems principles*, ser. SOSP ’03, New York, NY, USA: Association for Computing Machinery, Oct. 19, 2003, pp. 29–43.
2. A. Verma, L. Pedrosa, M. Korupolu, et al., “Large-scale cluster management at Google with Borg,” in *Proceedings of the Tenth European Conference on Computer Systems*, ser. EuroSys ’15, New York, NY, USA: Association for Computing Machinery, Apr. 17, 2015, pp. 1–17.
3. L. Barroso, J. Dean, and U. Hölzle, “Web search for a planet: The Google cluster architecture,” *IEEE Micro*, vol. 23, no. 2, pp. 22–28, 2003.
4. J. Dean and S. Ghemawat, “MapReduce: Simplified data processing on large clusters,” 1, vol. 51, New York, NY, USA: Association for Computing Machinery, Jan. 1, 2008, pp. 107–113.
5. F. Chang, J. Dean, S. Ghemawat, et al., “Bigtable: A distributed storage system for structured data,” Article 4, vol. 26, New York, NY, USA: Association for Computing Machinery, Jun. 1, 2008, pp. 1–26.
6. M. Burrows, “The Chubby lock service for loosely-coupled distributed systems,” in *Proceedings of the 7th Symposium on Operating Systems Design and Implementation*, ser. OSDI ’06, USA: USENIX Association, Nov. 6, 2006, pp. 335–350.
7. L. A. Barroso and U. Hölzle, “The case for energy-proportional computing,” *Computer*, vol. 40, no. 12, pp. 33–37, 2007.
8. C. Reiss, A. Tumanov, G. R. Ganger, R. H. Katz, and M. A. Kozuch, “Heterogeneity and dynamicity of clouds at scale: Google trace analysis,” in *Proceedings of the Third ACM Symposium on Cloud Computing*, ser. SoCC ’12, New York, NY, USA: Association for Computing Machinery, Oct. 14, 2012, pp. 1–13.
9. M. Tirmazi, A. Barker, N. Deng, et al., “Borg: The next generation,” in *Proceedings of the 15th European Conference on Computer Systems*, ser. EuroSys ’20, New York, NY, USA: Association for Computing Machinery, Apr. 17, 2020, pp. 1–14.
10. J. C. Corbett, J. Dean, M. Epstein, et al., “Spanner: Google’s globally distributed database,” Article 8, vol. 31, New York, NY, USA: Association for Computing Machinery, Aug. 1, 2013, pp. 1–22.
11. Open Compute Project, Open Compute Project, https://www.opencompute.org.
12. D. Abts and B. Felderman, “A guided tour of data-center networking,” *Commun. ACM*, vol. 55, no. 6, pp. 44–51, Jun. 2012.
13. A. Singh, J. Ong, A. Agarwal, et al., “Jupiter Rising: a decade of Clos topologies and centralized control in Google’s datacenter network,” 4, vol. 45, New York, NY, USA: Association for Computing Machinery, Aug. 17, 2015, pp. 183–197.
14. S. Jain, A. Kumar, S. Mandal, et al., “B4: Experience with a globally-deployed software defined WAN,” in *Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM*, ser. SIGCOMM ’13, New York, NY, USA: Association for Computing Machinery, Aug. 27, 2013, pp. 3–14.
15. J. Dean and L. A. Barroso, “The tail at scale,” 2, vol. 56, New York, NY, USA: Association for Computing Machinery, Feb. 1, 2013, pp. 74–80.
16. N. P. Jouppi, C. Young, N. Patil, et al., “In-datacenter performance analysis of a Tensor Processing Unit,” in *Proceedings of the 44th Annual International Symposium on Computer Architecture*, ser. ISCA ’17, New York, NY, USA: Association for Computing Machinery, Jun. 24, 2017, pp. 1–12.
17. S. Dev, D. Lo, L. Cheng, and P. Ranganathan, “Autonomous warehouse-scale computers,” in *Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference*, ser. DAC ’20, IEEE Press, Nov. 18, 2020, pp. 1–6.
18. M. Schwarzkopf, A. Konwinski, M. Abd-El-Malek, and J. Wilkes, “Omega: Flexible, scalable schedulers for large compute clusters,” in *Proceedings of the 8th ACM European Conference on Computer Systems*, ser. EuroSys ’13, New York, NY, USA: Association for Computing Machinery, Apr. 15, 2013, pp. 351–364.
19. H. Liu, R. Urata, and A. Vahdat, “Optical interconnects for scale-out data centers,” in *Optical Interconnects for Future Data Center Networks*, Springer, 2012, pp. 17–29.
20. L. Poutievski, O. Mashayekhi, J. Ong, et al., “Jupiter evolving: Transforming google’s datacenter network via optical circuit switches and software-defined networking,” in *Proceedings of the ACM SIGCOMM 2022 Conference*, ser. SIGCOMM ’22, New York, NY, USA: Association for Computing Machinery, Aug. 22, 2022, pp. 66–85.
21. K.-K. Yap, M. Motiwala, J. Rahe, et al., “Taking the edge off with Espresso: scale, reliability and programmability for global internet peering,” in *Proceedings of the Conference of the ACM Special Interest Group on Data Communication*, ser. SIGCOMM ’17, New York, NY, USA: Association for Computing Machinery, Aug. 7, 2017, pp. 432–445.
22. M. Dalton, D. Schultz, J. Adriaens, et al., “Andromeda: Performance, isolation, and velocity at scale in cloud network virtualization,” in *15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)*, 2018, pp. 373–387.
23. V. Sakalkar, V. Kontorinis, D. Landhuis, et al., “Data center power oversubscription with a medium voltage power plane and priority-aware capping,” in *Proceedings of the 25th International Conference on Architectural Support for Programming Languages and Operating Systems*, ser. ASPLOS ’20, New York, NY, USA: Association for Computing Machinery, Mar. 13, 2020, pp. 497–511.
24. M. Marty, M. de Kruijf, J. Adriaens, et al., “Snap: A microkernel approach to host networking,” in *Proceedings of the 27th ACM Symposium on Operating Systems Principles*, ser. SOSP ’19, New York, NY, USA: Association for Computing Machinery, Oct. 27, 2019, pp. 399–413.
25. L. Barroso, M. Marty, D. Patterson, and P. Ranganathan, “Attack of the killer microseconds,” *Commun. ACM*, vol. 60, no. 4, pp. 48–54, Mar. 2017.
26. S. Kanev, J. P. Darago, K. Hazelwood, et al., “Profiling a warehouse-scale computer,” in *Proceedings of the 42nd Annual International Symposium on Computer Architecture*, ser. ISCA ’15, New York, NY, USA: Association for Computing Machinery, Jun. 13, 2015, pp. 158–169.
27. P. Ranganathan, D. Stodolsky, J. Calow, et al., “Warehouse-scale video acceleration,” *IEEE Micro*, vol. 42, no. 4, pp. 18–26, 2022.
28. A. Lagar-Cavilla, J. Ahn, S. Souhlal, et al., “Software-defined far memory in warehouse-scale computers,” in *Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems*, ser. ASPLOS ’19, New York, NY, USA: Association for Computing Machinery, Apr. 4, 2019, pp. 317–330.
29. S. Li, Z. Zhao, K. Chen, and Y. Zhou, “Can we trust run-time profiling for optimizing memory placement decisions?” In *14th USENIX Symposium on Operating Systems Design and Implementation (OSDI ’20)*, 2020, pp. 867–882.
30. S. Arslan, Y. Li, G. Kumar, and N. Dukkipati, “Bolt: Sub-RTT congestion control for ultra-low latency,” in *20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)*, Boston, MA: USENIX Association, Apr. 2023, pp. 219–236.
31. P. H. Hochschild, P. Turner, J. C. Mogul, et al., “Cores that don’t count,” in *Proceedings of the Workshop on Hot Topics in Operating Systems*, ser. HotOS ’21, New York, NY, USA: Association for Computing Machinery, Jun. 3, 2021, pp. 9–16.
32. G. Ren, E. Tune, T. Moseley, et al., “Google-Wide Profiling: A continuous profiling infrastructure for data centers,” *IEEE Micro*, vol. 30, no. 4, pp. 65–79, 2010.
33. P. Ranganathan, “A six-word story on the future of VLSI: AI-driven, software-defined, and uncomfortably exciting,” in *2023 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits)*, 2023, pp. 1–4.
34. C. E. Leiserson, N. C. Thompson, J. S. Emer, et al., “There’s plenty of room at the top: What will drive computer performance after Moore’s law?” *Science*, vol. 368, no. 6495, eaam9744, 2020.
35. J. Dean, The potential of machine learning for hardware design, https://www.youtube.com/watch?v=FraDFZ2t__A, 2021.