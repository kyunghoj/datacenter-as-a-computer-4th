다음은 제공된 PDF 파일의 내용을 기반으로 번역 및 변환한 마크다운 문서입니다.

---

# 2. WSC 아키텍처 개요

## 2.1 리전(Regions)과 존(Zones)

가장 상위 레벨에서 웨어하우스 스케일 컴퓨터(WSC)는 직경 약 50km의 지리적 **리전(Region)** 내에 배치됩니다. 리전은 여러 개의 캠퍼스로 구성되며, 어떠한 두 캠퍼스도 50km 이상 떨어지지 않도록 배치됩니다. 이 최대 거리는 임의로 정해진 것이 아닙니다. 홍수, 화재, 지진과 같은 단일 재난 사건이나 오작동 또는 운영자 실수로 인한 정전이 두 곳 이상의 캠퍼스에 영향을 미칠 가능성을 낮출 만큼 충분히 멀면서도, 중간에 리피터(repeater) 없이 캠퍼스 간 직접적인 광학 경로(optical paths)를 허용할 만큼 충분히 가까운 거리입니다. 그 결과 생성된 링크는 캠퍼스 간 동기식 스토리지 복제(synchronous storage replication)를 허용할 만큼 왕복 지연 시간(round-trip delay)이 짧습니다.

하이퍼스케일러(Google, Meta)가 자체 애플리케이션을 위해 사용하는 리전은 단일 캠퍼스부터 다중 캠퍼스에 이르기까지 다양한 형태를 가질 수 있습니다. YouTube나 Facebook과 같은 하이퍼스케일 서비스는 여러 리전에 걸쳐 있는 글로벌 서비스이므로 가용성이나 내구성(durability)을 위해 리전 내부의 복제에 덜 의존합니다. 대신, 이들의 신뢰성 모델은 어떤 리전이라도 실패할 수 있다고 가정하며(예: 도시 전체의 네트워크 중단 등으로 인해), 결과적으로 데이터는 리전 간에 복제됩니다.

퍼블릭 클라우드용으로 구축된 리전은 일반적으로 2~3개의 별도 캠퍼스를 보유합니다. 대부분의 기업 고객은 단일 리전 아키텍처를 선호하는데, 이는 더 단순하고 저렴하며, 가용성이나 내구성을 위해 스토리지 장치나 데이터베이스가 동기식 복제를 사용하는 기존 애플리케이션 모델에 더 잘 맞기 때문입니다(즉, 쓰기 작업이 완료되려면 두 개의 별도 캠퍼스에서 커밋되어야 함). Google과 AWS는 분산 합의 알고리즘(distributed consensus algorithms)이 깔끔하게 작동하도록 리전당 3개의 존(zone)을 구축합니다. 만약 네트워크 파티션이나 단일 캠퍼스 문제가 발생하면, 두 개의 "건강한" 투표를 받은 시스템이 기록의 주 시스템(primary system of record)이 되거나 유지되며, 나머지 사본은 더 이상의 변경을 수행하지 못하도록 차단됩니다. 현재 Azure 리전은 일반적으로 스토리지에 대한 액티브-액티브(active-active) 또는 액티브-스탠바이(active-standby) 쌍의 일반적인 사용 사례를 반영하여 두 개의 캠퍼스만 보유하고 있습니다. 이 솔루션은 더 단순하고 새로운 위치에 더 빠르게 시장에 진입할 수 있는 경로를 제공할 수 있지만, 자동화된 페일오버(failover)를 복잡하게 만듭니다.

또한 고객이 지역적으로 내결함성이 있는(regionally fault tolerant) 애플리케이션을 구축하는 비용을 더 비싸게 만듭니다. 단일 캠퍼스 장애를 견디기 위해 $N=2$인 경우 $2N$의 용량이 필요한 반면, $N=3$인 경우 1.5배의 용량만 있으면 됩니다(각 캠퍼스가 $0.5N$을 보유).

퍼블릭 클라우드는 **존(zone)**이라는 개념을 통해 개별 WSC 건물이나 캠퍼스를 추상화합니다. 리전은 3개의 존(Azure의 경우 2개)을 가지며, 존은 리소스의 정확한 물리적 위치를 추상화합니다. 정확한 세부 사항은 제공업체마다 다르지만, 두 개의 VM이 같은 존에 있다면 그들 사이의 대역폭은 가장 빠르고 저렴하겠지만, 상관관계가 있는 장애(correlated failures)를 겪을 수 있습니다. 즉, 단일 다운타임 이벤트로 인해 두 VM이 동시에 사용 불가능해질 수 있습니다. 단일 존은 어느 정도의 물리적 다양성(여러 건물 또는 여러 캠퍼스로 구성됨)을 가질 수 있지만, 그러한 세부 사항은 숨겨져 있으며, 모든 구성 요소가 단일 제어 평면(control plane)을 공유하므로 소프트웨어로 인한 장애를 동시에 겪을 수 있습니다 (**그림 2.1**).

리전이 높은 수준의 물리적 가용성을 제공하더라도(50km 떨어진 세 개의 다른 캠퍼스가 단일 재해의 영향을 받을 가능성은 낮으므로), 리전의 제어 평면이 모든 존에서 공유되기 때문에 API 계층에서는 반드시 그 정도의 가용성을 제공할 수 있는 것은 아닙니다. 예를 들어, 리전 로드 밸런서에 버그가 있으면 모든 존의 애플리케이션이 영향을 받습니다. 이러한 이유로 많은 API는 고객이 적절한 것을 선택할 수 있도록 존(zonal) 및 리전(regional) 변형으로 제공됩니다(유일한 차이점은 장애 도메인임). 리전 버전을 선택하는 것은 더 쉽지만 리전 클라우드 API에 대한 의존성을 추가하므로 클라우드 제공업체의 SLO 성능에 대한 더 많은 신뢰가 필요합니다. 존 버전을 선택하면 애플리케이션의 리전 신뢰성 속성에 대한 추가적인 자체 제어권을 제공하지만, 리전 기능을 복제하기 위해 추가적인 자체 노력이 필요합니다.

**그림 2.1** 2025년 3월 기준 Google Cloud Platform 리전

![지도 이미지: 세계 지도에 표시된 Google Cloud Platform 리전들의 위치]

---

## 2.2 건물 및 인프라

캠퍼스는 하나 이상의 데이터 센터 건물로 구성됩니다. 때때로 건물은 각기 독립적인 인프라, 즉 별도의 전력, 냉각 및 네트워킹 인프라를 갖춘 여러 개의 WSC로 구성되며, 한 구역에 화재가 발생하더라도 이웃 구역이 계속 운영될 수 있도록 다중 시간(multi-hour) 방화벽으로 분리되어 있습니다. 예를 들어, **그림 2.2**의 Council Bluffs 캠퍼스 항공뷰에 있는 상단 건물은 3개의 별도 WSC로 구성되어 있습니다. (원래 설계된 왼쪽의 WSC와 그 이후의 두 WSC 사이에 도입된 설계 변경으로 인해 똑같이 보이지 않습니다.) **그림 2.3**은 서버 홀 내부를 보여주며, **그림 2.4**는 냉각 홀의 일부를 보여줍니다.

WSC 건물은 웨어하우스 스케일 컴퓨팅을 구동하는 컴퓨팅, 네트워크 및 스토리지 인프라를 수용하며, 건물의 설계는 주요 WSC 속성에 영향을 미칠 수 있습니다. 예를 들어, 5장에서는 건물 설계 결정(및 데이터 센터 건설의 다양한 등급)이 WSC의 가용성과 가동 시간(uptime)에 어떻게 극적인 영향을 미칠 수 있는지 논의할 것입니다.

WSC는 정교한 전력 공급 설계를 갖추고 있습니다. 운영 규모 면에서 WSC는 종종 중소도시보다 더 많은 전력을 소비할 수 있습니다. 따라서 전력 회사로부터 변전소, 배전 장치(PDU), 버스 덕트(bus ducts)를 거쳐 서버 보드의 개별 전원 레일 및 전압 조정기에 이르기까지 전기를 공급하는 전체적인 전력 공급 설계를 사용합니다. 이 설계는 또한 무정전 전원 공급 장치(UPS), 발전기 및 백업 배터리와 같은 해당 백업 및 중복성을 토폴로지의 다양한 수준에서 제공합니다. 예를 들어 Council Bluffs 캠퍼스에는 오른쪽에 두 개의 대형 고전압 변전소가 있으며, 각각은 해당 지역의 최고 전압 그리드 연결에 연결되어 있고 다중 고용량 변압기를 포함하고 있습니다. 5.2절에서 전력 공급에 대해 더 자세히 논의합니다.

**그림 2.2** 미국 아이오와주 Google 데이터 센터의 항공뷰
(Building 1, Building 2, Building 3, 변전소 1, 변전소 2, 물 탱크, 중앙 유틸리티 건물 등이 표시됨)

**그림 2.3** 전력 및 네트워크 분배
(데이터 센터 내부의 랙과 상단 케이블 트레이 모습)

**그림 2.4** 데이터 센터 냉각 홀
(다양한 색상의 파이프와 펌프가 있는 설비 모습)

그 모든 전력은 결국 열로 변환되므로, WSC는 데이터 센터 바닥의 팬 코일로 냉각되는 순환 공기부터 열 교환기 및 냉동기(chiller) 장치, 증발이나 복사를 통해 공기 중으로 열을 방출하는 외부 냉각 장치에 이르기까지 열 교환 루프의 계층 구조를 갖춘 정교한 엔드 투 엔드(end-to-end) 냉각 솔루션을 사용합니다 (**그림 2.5**). 5.3절에서 냉각에 대해 더 자세히 논의합니다.

건물 설계와 서버 설계는 완전히 분리될 수 없습니다. 예를 들어, 건물의 전력 밀도는 단일 단거리 광케이블 반경 내에 얼마나 많은 서버를 배치할 수 있는지를 제한하므로, 밀집하게 연결된 서버 또는 가속기 클러스터의 최대 크기나 더 비싼 광학 장치를 사용하는 더 큰 클러스터를 구축하는 비용에 영향을 미칩니다. 마찬가지로, 랙에 냉각을 전달하는 방식은 가속기를 위한 액체 냉각 구현을 더 쉽거나 어렵게 만들 수 있습니다. 따라서 서버를 이해하지 않고 "최고의" 데이터 센터 설계를 결정하기 어렵고, 그 반대도 마찬가지입니다. 때로는 데이터 센터 수준에서 약간의 추가 비용이 랙이나 서버 수준에서 더 많은 비용을 절감할 수 있습니다. 마지막으로, 데이터 센터는 20년 이상 지속되므로 현재 서버 세대뿐만 아니라 지금부터 10년 후에 도입될 서버에게도 "최고"여야 합니다.

**그림 2.5** 냉각탑 및 물 저장 탱크와 전력 소비 파이차트

*   **냉각 전력 (Cooling power):** 3.2% (44.6W)
*   **전력 손실 (Power loss):** 16.1% (223W)
*   **네트워킹 (Networking):** 2.2% (30W)
*   **스토리지 (Storage):** 3.2% (44W)
*   **DDR (Memory):** 18.6% (258W)
*   **CPUs:** 56.7% (786W)

---

## 2.3 서버

데이터 센터에서 사용되는 에너지의 대부분은 일반적으로 WSC의 하드웨어 구성 요소인 서버로 전달됩니다. 이러한 일반 상품(commodity) 서버는 랙에 장착되어 로컬 "랙 상단(rack top)" 이더넷 스위치를 사용하여 상호 연결됩니다. 이 랙 레벨 스위치는 (2025년 기준) 100-200Gbps 링크를 사용하며, 차례로 하나 이상의 클러스터 레벨(또는 데이터 센터 레벨) 이더넷 스위치에 대한 다수의 업링크 연결을 갖습니다.

미터법이나 미국 단위계 모두에 만족하지 못한 랙 설계자들은 서버의 높이를 측정하기 위해 "랙 유닛(rack units)"을 사용합니다. 1U는 1.75인치 또는 44.45mm이며, 일반적인 랙은 42U 높이입니다. 이러한 표준은 1922년 AT&T(당시 American Telephone & Telegraph)의 장비 내부 표준으로 제정되어 오늘날까지 살아남았습니다.

때때로 트레이나 인클로저(enclosure)에는 일부 구성 요소를 공유하는 여러 서버가 포함됩니다. 예를 들어, 여러 저전력 서버가 비싼 고속 NIC를 공유할 수 있습니다. 따라서 종종 인클로저 내에 추가적인 첫 번째 수준의 네트워크 집계(aggregation)가 포함되어 있어, 여러 처리 노드가 PCIe와 같은 I/O 버스를 통해 소수의 네트워킹 노드에 연결됩니다.

**그림 2.6**은 왼쪽의 일반 서버 트레이로 시작하여 Google 서버 빌딩 블록을 보여줍니다. WSC는 GPU 및 맞춤형 가속기를 포함한 추가적인 컴퓨팅 하드웨어 빌딩 블록을 특징으로 합니다. 가운데 사진은 TPU 보드를 보여줍니다. 서버와 마찬가지로 이들은 랙(또는 다중 랙 포드) 수준에서 맞춤형 또는 산업 표준 인터커넥트를 통해 연결되어 데이터 센터 네트워크로 이어집니다. 오른쪽에는 수십 개의 디스크가 있는 스토리지 트레이가 있습니다. **그림 2.7**은 이러한 빌딩 블록이 범용 서버와 가속기 모두를 위해 어떻게 서버 랙의 열(row)로 조립되는지 보여줍니다.

**그림 2.6** WSC용 빌딩 블록: 서버, TPU 가속기, 디스크 트레이

---

## 2.4 스토리지

디스크와 플래시(NAND) SSD는 오늘날 WSC 스토리지 시스템의 빌딩 블록입니다. 서버가 일반적으로 WSC에서 전력의 대부분을 사용하는 반면, 스토리지는 일반적으로 공간의 대부분을 차지합니다. 디스크로 가득 찬 랙은 전력을 훨씬 적게 사용하기 때문에, 1MW의 디스크는 1MW의 서버보다 훨씬 더 많은 바닥 공간을 차지합니다. 디스크가 많은 사용 사례에 비해 너무 느림에도 불구하고, 콜드 데이터(cold data)에 대한 바이트당 비용이 타의 추종을 불허하기 때문에 여전히 스토리지 장치의 대다수를 차지합니다.

**그림 2.7** 상단: 기존 컴퓨팅 서버; 하단: TPU 가속기 포드

스토리지 장치는 데이터 센터 네트워크에 연결되며 분산 시스템에 의해 관리됩니다. WSC 시스템 설계자는 요구 사항에 따라 몇 가지 트레이드오프를 결정해야 합니다. 예를 들어, 디스크와 SSD를 컴퓨팅 서버에 직접 연결해야 할까요(Directly Attached Storage, DAS), 아니면 네트워크 연결 스토리지(Network Attached Storage, NAS)의 일부로 분리해야 할까요? DAS는 원시 하드웨어 비용을 줄이고 네트워크 활용도를 높일 수 있지만(네트워크 포트가 컴퓨팅 및 스토리지 작업 간에 동적으로 공유됨), NAS 방식은 배포를 단순화하고 컴퓨팅 작업의 성능 간섭을 피하기 위해 더 높은 QoS를 제공하는 경향이 있습니다. 또한 NAS 스토리지는 많은 시스템 간에 공유되고 집계된 수요를 예측하기 더 쉽기 때문에 활용도(즉, 프로비저닝된 공간 대비 실제 사용된 공간)를 높일 수 있습니다. 따라서 WSC의 스토리지 장치는 원시 하드웨어와 비용뿐만 아니라 대역폭, IOPS, 용량, 테일 레이턴시(tail latency), TCO를 포함한 여러 주요 지표 전반에 걸친 글로벌 최적화의 통합된 뷰를 목표로 해야 합니다.

분산 스토리지 소프트웨어는 스토리지 장치를 관리할 뿐만 아니라 애플리케이션 개발자를 위한 비구조화 및 구조화된 API도 제공합니다. Google File System (GFS)과 이후의 Colossus 및 그 클라우드 버전인 GCS [1]는 공간 효율적인 Reed-Solomon 코드와 고가용성을 위한 빠른 재구성을 사용하는 비구조화 WSC 스토리지의 예입니다. Google의 BigTable [2]과 Amazon의 Dynamo [3]는 데이터베이스와 유사한 기능을 제공하지만 더 약한 일관성 모델을 가진 구조화된 WSC 스토리지의 예입니다. 개발자의 작업을 단순화하기 위해 Spanner [4]와 같은 최신 세대의 구조화된 스토리지 시스템은 SQL과 유사한 인터페이스와 강력한 일관성 모델을 제공합니다.

지난 20년 동안 네트워크 속도는 디스크 속도보다 훨씬 빠르게 향상되었으며, 따라서 데이터 지역성을 고려할 필요 없이 관리를 획기적으로 단순화하는 네트워크 연결 디스크 스토리지를 지원하기가 쉬워졌습니다. 반면, Flash SSD 및 신흥 비휘발성 메모리(NVM)와 같은 저지연 고대역폭 스토리지는 WSC 네트워킹에 새로운 과제를 제기합니다. 20년 전 단일 디스크가 서버 네트워크 링크의 상당 부분을 소비할 수 있었던 것처럼, 오늘날 단일 NVMe SSD는 100Gbps 이더넷 링크의 25~50%를 소비할 수 있습니다. 6장에서는 시스템 균형에 대해 더 자세히 논의합니다.

---

## 2.5 네트워킹 패브릭

WSC를 위한 네트워킹 패브릭을 선택하는 것은 속도, 규모 및 비용 간의 트레이드오프를 포함합니다. 2025년 기준, 스위치는 25-50 Tbps의 총 대역폭을 제공하며 단일 랙 내에서 서버당 400Gbps로 64대의 서버를 지원합니다. 결과적으로 서버 랙 내의 대역폭은 균일하고 풍부한 경향이 있습니다. 그러나 WSC의 다른 모든 서버를 동일한 속도로 연결하는 것은 매우 비용이 많이 듭니다. 단일 스위치로는 그렇게 할 수 없으므로, WSC는 여러 랙을 연결된 랙 블록으로 묶기 위해 집계(aggregation) 스위치 계층을 사용하고, 이러한 블록을 더 큰 블록으로 묶기 위해 또 다른 계층을 사용하는 방식을 취합니다. 모든 서버가 동시에 통신할 경우(bisection bandwidth), 서버가 멀리 떨어져 있을수록 평균 대역폭은 느려집니다. 따라서 같은 랙의 서버는 400Gbps로 통신할 수 있지만, 이웃 랙에 대한 대역폭은 100Gbps에 불과할 수 있고, WSC 전체를 가로지르는 서버에 대해서는 50Gbps에 불과할 수 있습니다. 이러한 네트워크에 대해서는 6.4절에서 논의할 것입니다.

대안으로, 인터커넥트 패브릭에 더 많은 비용을 지출하여 클러스터 수준의 네트워킹 병목 현상을 개선할 수 있습니다. 예를 들어, Infiniband 인터커넥트는 높은 대역폭과 낮은 지연 시간으로 수천 개의 포트까지 확장할 수 있지만, 포트당 기준으로 이더넷보다 훨씬 비용이 많이 듭니다. 네트워킹 대 추가 서버 또는 스토리지에 얼마를 지출할지는 단 하나의 정답이 없는 애플리케이션별 질문입니다. 그러나 경험상(rule of thumb), 랙 내부 연결이 랙 간 연결보다 저렴하다고 가정할 것입니다.

---

## 2.6 에너지 사용량

에너지 관련 비용이 총 소유 비용의 상당 부분을 차지하기 때문에 에너지 사용은 WSC 설계에서 중요한 관심사입니다. **그림 2.8**은 구성 요소 그룹별로 2024년 세대 서버의 피크 전력 사용량을 분류하여 현대 IT 장비에서 에너지가 어떻게 사용되는지에 대한 통찰력을 제공합니다. 이 서버는 768GB의 DRAM과 로컬 SSD 스토리지를 갖춘 2소켓 x86 서버이며 80% 가동률로 실행됩니다.

이 분석은 주어진 워크로드에 대해 시스템이 어떻게 구성되는지에 따라 크게 달라지겠지만, CPU는 WSC에서 지배적인 에너지 소비자이며, 메모리가 그 뒤를 따릅니다. 이 비율은 지난 20년 동안 비교적 일정하게 유지되었습니다. 냉각 오버헤드는 상대적으로 작으며, 이는 이 분야에서의 수세대에 걸친 개선을 반영합니다. 그러나 전력 손실은 시간이 지남에 따라 더 중요해졌는데, 지난 10년 동안 대략 두 배로 증가했습니다. 이는 현대 CPU 전류가 매우 높기 때문입니다. CPU는 500W 이상을 소비하며 코어 전압 또는 약 1V에서 공급됩니다. 전력 손실에는 전류의 제곱에 비례하여 증가하는 모든 전압 조정 모듈(VRM) 변환 손실이 포함됩니다. (VRM은 48V 트레이 전원 공급을 코어 전압으로 낮춥니다.) 손실에는 금속(또는 전력 전달 네트워크, PDN) 손실도 포함되며, 이 역시 전류의 제곱에 비례하여 증가합니다. 9장에서는 WSC 에너지 효율성에 대해 자세히 논의합니다.

**그림 2.8** 2024년 Google 서버의 전력 사용량

*   **냉각 전력 (Cooling power):** 3.2% (44.6W)
*   **전력 손실 (Power loss):** 16.1% (223W)
*   **네트워킹 (Networking):** 2.2% (30W)
*   **스토리지 (Storage):** 3.2% (44W)
*   **메모리 (Memory):** 18.6% (258W)
*   **CPUs:** 56.7% (786W)

---

## 2.7 장애 및 수리 처리

WSC의 엄청난 규모는 인터넷 서비스 소프트웨어가 상대적으로 높은 구성 요소 고장률을 견딜 것을 요구합니다. 예를 들어 디스크 드라이브는 연간 고장률이 4%에 달할 수 있습니다 [5], [6]. 다양한 배포 사례에서 연간 평균 서버 재시작 횟수가 1.2회에서 16회 사이로 보고되었습니다. 이렇게 높은 구성 요소 고장률로 인해 수천 대의 머신에서 실행되는 애플리케이션은 매시간 장애 조건에 대응해야 할 수도 있습니다. 이 주제는 애플리케이션 도메인을 설명하는 3장과 장애 통계를 다루는 10장에서 자세히 다룹니다.

---

### 참고 문헌 (References)

1.  D. Serenyi, “Cluster-level storage @Google,” in *Keynote at the 2nd Joint International Workshop on Parallel Data Storage and Data Intensive Scalable Intensive Computing Systems*, 2017.
2.  F. Chang, J. Dean, S. Ghemawat, et al., “Bigtable: A distributed storage system for structured data,” in *Proceedings of the 7th Symposium on Operating Systems Design and Implementation*, ser. OSDI ’06, USA: USENIX Association, Nov. 6, 2006, pp. 205–218.
3.  G. DeCandia, D. Hastorun, M. Jampani, et al., “Dynamo: Amazon’s highly available key-value store,” in *Proceedings of twenty-first ACM SIGOPS Symposium on Operating Systems Principles*, ser. SOSP ’07, New York, NY, USA: Association for Computing Machinery, Oct. 14, 2007, pp. 205–220.
4.  J. C. Corbett, J. Dean, M. Epstein, et al., “Spanner: Google’s globally-distributed database,” in *Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation*, ser. OSDI’12, USA: USENIX Association, Oct. 8, 2012, pp. 251–264.
5.  E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large disk drive population,” in *Proceedings of the 5th USENIX Conference on File and Storage Technologies*, ser. FAST ’07, San Jose, CA: USENIX Association, 2007.
6.  B. Schroeder and G. A. Gibson, “Understanding disk failure rates: What does an MTTF of 1,000,000 hours mean to you?,” 3, vol. 3, New York, NY, USA: Association for Computing Machinery, Oct. 1, 2007, 8–es.