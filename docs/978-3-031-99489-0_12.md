다음은 제공된 PDF 파일("The Data Center as a Computer"의 12장)의 내용을 바탕으로 작성된 마크다운 변환 결과입니다. 원문이 영어이므로 요청하신 대로 한국어로 번역하여 정리했습니다.

---

# 12. WSC 25년을 위한 엄선된 25편의 주요 논문

기술 분야에서 25년은 매우 긴 시간이며, 웨어하우스 규모 컴퓨팅(Warehouse-Scale Computing, WSC)은 구글 워크로드를 위해 특화된 틈새 최적화 영역에서 일반적인 기술로 발전했습니다. 이러한 진화는 활기찬 학술 연구 논문 컬렉션에 잘 반영되어 있습니다. 하지만 이번 장을 위해 상위 25편을 선정하는 것은 매우 어려운 일이었습니다. 선정에 도움을 얻기 위해 우리는 구글에서 발표된 논문들에 집중했습니다. 이는 구글 내부의 연구에 대한 우리의 친숙함을 반영한 것이기도 하지만(동시에 웨어하우스 규모 컴퓨팅을 발명하고 확장하는 데 있어 구글이 미친 막대한 영향력을 인정한 것이기도 합니다), 이 책의 수많은 참고 문헌에서 볼 수 있듯이 구글 외부에서도 훌륭한 연구가 많이 이루어졌습니다.

우리는 저명한 저자들에게 각자의 상위 논문을 선정해 달라고 요청했고, 이 점수들을 합산하여 상위 50편의 논문을 식별했습니다. 마지막으로 (상당한 어려움 끝에) 여러분이 곧 읽게 될 25편으로 목록을 좁혔습니다. 우리의 목표는 독자 여러분께 이 분야의 최고 논문들에 대한 감각을 전달하는 동시에, WSC의 다양한 영역에 걸쳐 어느 정도의 폭넓은 지식을 제공하는 것이었습니다.

이번 장에서는 AI를 활용한 실험도 진행했는데, Gemini 2.5를 사용하여 상위 논문 목록을 읽고 요약을 제공하게 했습니다. 우리는 그 결과를 우리 자신의 노트와 결합하여 아래에 보이는 최종 요약문을 작성했습니다. 각 논문 소개에는 해당 논문이 무엇에 관한 것인지에 대한 짧은 단락과, 우리가 왜 이 논문을 상위 25개 목록에 선정했는지에 대한 짧은 (주관적인) 코멘트가 포함되어 있습니다.

이 훌륭한 논문들을 읽으며 우리가 목록을 작성할 때 느꼈던 즐거움을 여러분도 함께 느끼시길 바랍니다.

---

## 12.1 검색의 해부: 구글의 초기 아키텍처

*The Anatomy of a Large-Scale Hypertextual Web Search Engine, Sergey Brin and Lawrence Page, Computer Networks, 30 (1998), pp. 107-117, [1]*

이 논문은 웹의 하이퍼링크 그래프가 페이지의 권위를 측정하며(PageRank 알고리즘으로 포착됨) 순위를 크게 향상시킨다는 것을 입증함으로써 현대 웹 검색의 시작을 알렸습니다. 또한 구글 프로토타입은 앵커 텍스트(anchor text)의 중요한 설명력을 밝혀내어, 이러한 외부 레이블을 활용해 정확도를 성공적으로 높였습니다. 우리는 모두 순위(ranking)를 당연하게 여기지만, 이 논문은 콘텐츠 기반 방식보다 우수한 검색 관련성을 얻는 방법을 보여주었습니다. 마지막으로, 이들의 프로토타입은 저렴한 범용 하드웨어에 웹 인덱스를 샤딩(sharding)함으로써 핵심적인 WSC 접근 방식 중 하나를 시연했습니다.

**이 논문을 선정한 이유:** 이것은 고전적인 검색 엔진 논문입니다. 그리고 이 논문은 해당 분야의 주요 컨퍼런스에서 거절당했던 이력이 있습니다.

## 12.2 행성을 위한 웹 검색: 구글 클러스터 아키텍처

*Web Search for a Planet: The Google Cluster Architecture, Luiz Andre Barroso, Jeffrey Dean, Urs Hölzle, IEEE Micro, 23 (2003), pp. 22-28 [2]*

2003년 무렵, 구글의 아키텍처는 초당 쿼리 2개에서 최대치에 도달했던 1998년의 프로토타입에서 상당히 진화해 있었으며, 원래 프로토타입의 개념적 아이디어들은 실무에서 작동함이 입증되었습니다. 15,000대의 범용 PC로 구성된 클러스터는 수평적 확장(horizontal scaling)과 중복성(redundancy)을 제공하기 위해 샤딩을 사용했습니다. 빈번한 하드웨어 장애 중 일부는 이제 자동으로 처리되었습니다. 아직 Borg나 리눅스 컨테이너가 고안되지 않았기 때문에 서버 자체는 비교적 수동적인 방식으로 관리되었습니다. 마지막으로 중요한 점은, 이 논문이 전력과 에너지를 중요한 시스템 속성으로 논의한 최초의 논문 중 하나라는 것입니다.

**이 논문을 선정한 이유:** 웹 검색에 최적화된 최초의 실제 WSC 스택에 대한 짧지만 상세한 설명입니다.

## 12.3 구글 파일 시스템 (The Google File System)

*The Google File System, Sanjay Ghemawat, Howard Gobioff, Shun-Tak Leung, Proceedings of the 19th ACM Symposium on Operating Systems Principles, ACM, Bolton Landing, NY (2003), pp. 20-43 [3]*

구글 파일 시스템(GFS)은 구글에서 설계된 최초의 실제 스토리지 시스템이자, 대규모 데이터 집약적 애플리케이션을 위해 맞춤 제작된 최초의 수평적 확장 가능 파일 시스템이었습니다. GFS 이전에는 고용량 스토리지 시스템을 구축하려면 RAID 컨트롤러가 장착된 값비싼 고성능 디스크를 랙에 설치해야 했습니다. 대조적으로 GFS는 수천 대의 머신에 분산된 수많은 디스크(sea of disks)를 하나의 파일 시스템으로 조립했습니다. 파일은 개별 청크(chunk)로 나뉘고, 각 청크는 데이터 손실을 방지하고 읽기 처리량을 높이기 위해 여러 디스크에 배치되었습니다. 단일 마스터 노드가 메타데이터를 관리했습니다. GFS는 POSIX와 유사한 API를 제공했지만, 시스템을 단순화하고 대상 애플리케이션의 성능을 향상시키기 위해 일관성(consistency) 의미론을 완화했습니다. GFS는 구글이 검색 인덱스 크기를 다른 검색 엔진보다 훨씬 크게 늘릴 수 있게 해주었으며, Gmail이나 YouTube와 같은 미래의 스토리지 중점적인 애플리케이션을 위한 준비가 되게 했습니다.

**이 논문을 선정한 이유:** 수만 개의 신뢰할 수 없는 디스크를 관리하면서 초고속 처리량을 제공하는 파일 시스템을 어떻게 작성하며, 그것을 단 18개월 만에 어떻게 구현할 수 있을까요? 핵심적인 측면을 단순화하기 위해 현명한 선택을 함으로써 너무 많은 것을 포기하지 않고 이를 달성했습니다.

## 12.4 맵리듀스: 대규모 클러스터에서의 간소화된 데이터 처리

*MapReduce: Simplified Data Processing on Large Clusters, Jeffrey Dean, Sanjay Ghemawat, OSDI’04: Sixth Symposium on Operating System Design and Implementation, San Francisco, CA (2004), pp. 137-150 [4]*

맵리듀스(MapReduce)는 GFS를 보완합니다. GFS가 많은 장치에 대량의 데이터를 쉽게 저장할 수 있게 했다면, 맵리듀스는 많은 범용 서버에서 대규모 데이터셋을 쉽게 처리할 수 있게 했습니다. 사용자는 키/값 쌍을 처리하여 중간 쌍을 생성하는 "map" 함수와, 동일한 키와 연관된 중간 값들을 병합하는 "reduce" 함수를 지정합니다. 런타임 시스템은 병렬화, 결함 허용(fault tolerance), 데이터 분산 및 로드 밸런싱을 투명하게 처리합니다. 이러한 추상화를 통해 분산 시스템 전문 지식이 없는 프로그래머도 대규모 클러스터를 효과적으로 활용할 수 있게 되었습니다. 이 논문은 분산 정렬, 웹 액세스 로그 분석, 역색인 생성 등 다양한 작업에서 맵리듀스의 유용성을 입증하며 대규모 데이터 처리를 크게 단순화했습니다.

**이 논문을 선정한 이유:** Lisp가 맵-리듀스 패턴을 발명했고, 맵리듀스(나중에 하둡(Hadoop)으로 오픈 소스화됨)는 빠르게 대규모 배치(batch) 처리의 기본 선택지가 되었습니다.

## 12.5 빅테이블: 구조화된 데이터를 위한 분산 저장 시스템

*Bigtable: A Distributed Storage System for Structured Data, Fay Chang, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Mike Burrows, Tushar Chandra, Andrew Fikes, Robert E. Gruber, 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI), {USENIX} (2006), pp. 205-218 [5]*

GFS와 맵리듀스를 보완하는 빅테이블(Bigtable)은 하이퍼스케일 데이터베이스에 대한 초기 솔루션이었으며, 수평적 확장성을 대가로 강력한 트랜잭션 일관성을 희생하는 NoSQL 운동의 시작이었습니다. 빅테이블은 수천 대의 범용 서버에 데이터를 저장하며, 희소(sparse)하고 분산된, 영구적인 다차원 정렬 맵 데이터 모델을 노출합니다. 데이터는 행 키, 열 키, 타임스탬프로 인덱싱됩니다. 빅테이블은 기본 스토리지로 GFS를 사용하며 높은 확장성, 가용성 및 성능을 제공합니다. 대규모 데이터셋에 대한 지연 시간이 짧은 읽기/쓰기 및 효율적인 스캔을 제공하는 강점은 이후 NoSQL 설계에 큰 영향을 미쳤습니다.

**이 논문을 선정한 이유:** 이 논문은 오늘날까지도 강세를 보이고 있는 NoSQL 운동(예: MongoDB, Redis, Firestore, DynamoDB)을 시작했습니다. 그러나 지난 10년 동안 구글 내부의 많은 애플리케이션에서는 스패너(Spanner)가 비슷한 성능으로 완전한 트랜잭션 스토리지를 제공하기 때문에 빅테이블을 대체했습니다.

## 12.6 느슨하게 결합된 분산 시스템을 위한 Chubby 락 서비스

*The Chubby lock service for loosely-coupled distributed systems, Mike Burrows, 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI), {USENIX} (2006) [6]*

WSC 서비스는 구성 데이터를 부트스트랩할 방법이 필요하며, 단일 서비스의 여러 복제본은 누가 리더인지 합의해야 합니다. Chubby는 단일하고 간단한 API로 이 기능을 제공합니다. 자체 복제 서버 간의 결함 허용 합의를 위해 팩소스(Paxos)를 사용합니다. 클라이언트 측 캐싱 메커니즘은 킵얼라이브(keep-alive) 세션 및 이벤트 알림과 결합되어 일관성을 유지하면서 서버 부하를 최소화합니다.

**이 논문을 선정한 이유:** Chubby는 간단하지만 필수적인 API로 일반적인 문제를 해결하는 WSC 서비스의 고전적인 예입니다. 이는 오늘날 인기 있는 오픈소스 대응 제품인 Apache ZooKeeper와 Kubernetes 등에게 영감을 주었습니다.

## 12.7 에너지 비례 컴퓨팅을 위한 제언

*The Case for Energy-Proportional Computing, Luiz André Barroso, Urs Hölzle, IEEE Computer, 40 (2007) [7]*

컴퓨터 시스템, 특히 데이터 센터의 서버는 수행하는 작업량에 비례하여 전력을 소비해야 합니다. 대부분의 서버는 에너지 비례적이지 않습니다. 유휴 상태이거나 낮은 사용률로 작동할 때에도 피크 전력의 상당 부분을 소비합니다. 이 짧은 논문은 문제를 정의하고 더 나은 에너지 비례성이 데이터 센터의 운영 비용과 환경적 영향을 크게 줄일 것이라고 주장했습니다.

**이 논문을 선정한 이유:** 때로는 누구나 이해할 수 있는 방식으로 표현된 단순한 관찰이 산업의 방향에 영향을 줄 수 있습니다.

## 12.8 Dremel: 웹 규모 데이터셋의 대화형 분석

*Dremel: Interactive Analysis of Web-Scale Datasets, Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton, Theo Vassilakis, Proc. of the 36th Int’l Conf on Very Large Data Bases (2010), pp. 330-339 [8]*

Dremel은 매우 큰 읽기 전용 중첩 데이터셋에 대한 대화형, 임의 질의(ad-hoc querying)를 구현합니다. 높은 처리량의 병렬 쿼리 실행 엔진은 단일 쿼리에 수천 대의 서버를 사용할 수 있습니다. 컬럼형(columnar) 스토리지는 더 나은 압축을 허용하고, 쿼리가 부분적인 행만 읽고 사용하지 않는 레코드 필드를 건너뛰기 때문에 I/O 부하를 크게 줄입니다. 쿼리 엔진은 대규모 병렬 트리 아키텍처를 사용하여 페타바이트 규모의 데이터에 대해 몇 초 만에 SQL과 유사한 쿼리를 실행합니다. 많은 분석 작업에서 Dremel은 맵리듀스에 비해 프로그래밍 노력을 줄이고 실행 시간을 수십 배 단축했습니다.

**이 논문을 선정한 이유:** Dremel은 오늘날 선도적인 데이터 레이크하우스 제품인 BigQuery의 전신이었으며, 많은 사용 사례에서 맵리듀스를 빠르게 대체했습니다. 수평적 확장성을 이해하고 싶다면 이 논문을 읽어보세요.

## 12.9 Dapper, 대규모 분산 시스템 추적 인프라

*Dapper, a Large-Scale Distributed Systems Tracing Infrastructure, Benjamin H. Sigelman, Luiz André Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald Beaver, Saul Jaspan, Chandan Shanbhag, Google Technical Report dapper-2010-1, Apr. 2010 (2010) [9]*

Dapper 추적 인프라는 낮은 오버헤드, 애플리케이션 수준의 투명성, 확장성을 갖추고 클러스터 규모의 애플리케이션을 관찰합니다. 이는 개발자에게 대규모 마이크로서비스 기반 아키텍처를 통해 전파되는 요청에 대한 가시성을 제공합니다. 요청에 전역적으로 고유한 ID를 할당하고 컨텍스트를 전파함으로써 Dapper는 서로 다른 서비스 간의 인과 경로와 타이밍을 재구성합니다. 시스템은 적응형 샘플링을 사용하여 데이터 양을 관리하면서 대표적인 추적(trace)을 캡처합니다. Dapper(오픈 소스로 발전하여 OpenTelemetry가 됨)가 없다면 오늘 서비스가 왜 느린지, 또는 왜 일부 요청이 다른 요청보다 훨씬 느린지 이해하기 어려울 것입니다.

**이 논문을 선정한 이유:** Dapper는 처음 보기에 비해 해결하기 훨씬 어려운 문제에 대한 우아한 해결책입니다.

## 12.10 구글 전사적 프로파일링: 데이터 센터를 위한 연속적 프로파일링 인프라

*Google-Wide Profiling: A Continuous Profiling Infrastructure for Data Centers, Gang Ren, Eric Tune, Tipp Moseley, Yixin Shi, Silvius Rus, Robert Hundt, IEEE Micro (2010), pp. 65-79 [10]*

Google-Wide Profiling(GWP)은 WSC 애플리케이션을 위한 지속적이고 오버헤드가 낮은 프로파일링을 제공합니다. 샘플링을 사용하여 성능에 미치는 영향을 최소화하면서 실행 중인 작업에서 성능 데이터(CPU 사용량, 메모리 할당, 동기화 경합)를 수집합니다. 어디서나 이루어지는 데이터 수집은 성능 병목 현상을 식별하고, 서비스 전반 및 내부의 리소스 소비를 정확하게 귀속시키며, 시간 경과에 따른 성능 저하를 추적합니다. GWP는 애플리케이션별 계측(instrumentation)을 요구하지 않고도 웨어하우스 규모 애플리케이션의 효율성을 개선하는 데 필수적인 전체적인(fleet-wide) 가시성을 제공합니다. GWP의 심층적이고 정확한 프로파일링 데이터는 하드웨어 차이 이해, 피드백 기반 컴파일러 최적화 활성화, 모든 바이너리에서 자주 사용되는 라이브러리 코드 발견 등 다양한 사용 사례를 지원합니다.

**이 논문을 선정한 이유:** 이 정보가 얼마나 필수적인지, 그리고 단 0.01%의 오버헤드로 이를 수집할 수 있다는 사실에 놀라게 될 것입니다.

## 12.11 스패너: 구글의 전 세계 분산 데이터베이스

*Spanner: Google’s Globally-Distributed Database, James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes et al, Symposium on Operating Systems Design and Implementation (OSDI) (2012), [11]*

스패너(Spanner)는 지리적으로 분산된 데이터 센터 전반에 걸쳐 트랜잭션 일관성(외부 일관성, 가장 높은 일관성 표준)과 고가용성 및 수평적 확장성을 독특하게 결합한 최초의 데이터베이스입니다. 이는 자동 샤딩, 팩소스(Paxos)를 이용한 복제, 동기식 복제와 같은 기능을 통해 달성됩니다. 핵심 혁신은 GPS와 원자 시계를 사용하여 제한된 불확실성을 가진 긴밀하게 동기화된 타임스탬프를 생성하는 TrueTime API로, 이를 통해 일관된 분산 트랜잭션, 데이터베이스 전체에 걸친 일관된 스냅샷 읽기, 잠금 없는 읽기(lock-free reads)가 가능합니다. 스패너는 관계형 데이터 모델과 SQL 유사 쿼리 언어를 지원하여 글로벌 규모에서 친숙한 데이터베이스 의미론을 제공합니다.

**이 논문을 선정한 이유:** 언뜻 보기에 스패너는 가용성과 확장성을 얻기 위해 완전한 일관성을 포기하도록 NoSQL 시스템에 동기를 부여했던 CAP 정리를 위반하는 것처럼 보입니다. 많은 사람들이 이 논문을 지난 25년 동안 최고의 데이터베이스 논문으로 꼽습니다.

## 12.12 B4: 전 세계에 배포된 소프트웨어 정의 WAN 경험

*B4: Experience with a Globally Deployed Software Defined WAN, Sushant Jain, Alok Kumar, Subhasree Mandal, Joon Ong et al, Proceedings of the ACM SIGCOMM Conference, Hong Kong, China (2013), pp. 3–14 [12]*

B4 논문은 전 세계 데이터 센터를 연결하기 위해 사설 소프트웨어 정의 광역 네트워크(SDN WAN)를 설계, 배포 및 운영한 구글의 경험을 상세히 설명합니다. 이 시스템은 제어 평면(control plane)을 데이터 평면(data plane)과 분리하여, 중앙 집중식 트래픽 엔지니어링 컨트롤러를 사용해 물리적 네트워크 전반의 대역폭 할당 및 라우팅을 관리합니다. B4는 맞춤형 스위치 하드웨어와 OpenFlow 프로토콜을 채택하여, 기존 WAN 아키텍처에 비해 효율성, 유연성 및 비용 효율성을 개선하기 위해 크고 복잡한 WAN을 관리하는 데 있어 SDN의 효과를 입증했습니다.

**이 논문을 선정한 이유:** 여러 면에서 B4가 대규모 네트워크에 대해 갖는 의미는 Borg가 대규모 서버 풀에 대해 갖는 의미와 같습니다. 즉, 정신을 잃지 않고 수만 대의 장치를 관리할 수 있는 유일한 방법입니다.

## 12.13 대규모 시스템의 꼬리 지연시간 (The Tail at Scale)

*The Tail at Scale, Jeffrey Dean, Luiz André Barroso, Communications of the ACM, 56 (2013), pp. 74-80 [13]*

대규모 분산 시스템에서의 꼬리 지연시간(Tail latency)—높은 백분위수의 지연시간—은 사용자 경험에 불균형적으로 큰 영향을 미칩니다. 단일 요청에 수천 대의 서버가 관여하는 시스템에서는 개별 구성 요소의 드물고 작은 지연조차도 팬아웃(fan-out)으로 인해 전체 응답 시간을 지배하는 경우가 많습니다. 리소스 공유, 백그라운드 활동, 큐 대기 지연 등 많은 요인이 지연시간 변동성을 유발합니다. 이 논문은 헤징 요청(동일한 요청을 여러 복제본에 전송), 지연시간 유발 유예(probation), 마이크로 파티션 데이터 등 꼬리 지연시간을 완화하기 위한 몇 가지 기술을 설명합니다.

**이 논문을 선정한 이유:** 수평적 확장은 훌륭하지만 항상 그런 것은 아니며, 꼬리 지연시간이 파티를 망칠 수 있습니다.

## 12.14 웨어하우스 규모 컴퓨터 프로파일링

*Profiling a warehouse-scale computer, Svilen Kanev, Juan Darago, Kim Hazelwood, Parthasarathy Ranganathan, Tipp Moseley, Gu-Yeon Wei, David Brooks, ISCA ’15 Proceedings of the 42nd Annual International Symposium on Computer Architecture, ACM (2014), pp. 158-169 [14]*

이 연구는 주로 명령어 수준에서 훨씬 더 많은 데이터를 수집하여 Google-Wide Profiling을 보완했습니다. 저자들은 다양한 워크로드에 걸쳐 명령어 혼합, CPI(명령어당 사이클), 캐시 미스, 분기 예측 실패 및 프론트엔드 스톨(stall)을 분석합니다. 그들은 마이크로아키텍처 기능이 규모에 따라 소프트웨어와 어떻게 상호 작용하는지 보여주며, 소규모 연구에서는 종종 가려졌던 프론트엔드 스톨과 같은 병목 현상을 드러냅니다.

**이 논문을 선정한 이유:** 실제 WSC 부하는 일반적인 벤치마크와 전혀 다릅니다. 지난 10년 동안 하드웨어는 극적으로 변했지만, 이 논문에서 처음 관찰된 내용의 대부분은 오늘날에도 여전히 유효합니다.

## 12.15 Borg를 이용한 구글의 대규모 클러스터 관리

*Large-scale cluster management at Google with Borg, Abhishek Verma, Luis Pedrosa, Madhukar R. Korupolu, David Oppenheimer, Eric Tune, John Wilkes, Proceedings of the European Conference on Computer Systems (EuroSys), ACM, Bordeaux, France (2015) pp. 18:1–18:17 [15]*

Borg는 수만 대의 머신에 걸쳐 애플리케이션을 스케줄링, 배포, 모니터링 및 관리하는 대규모 클러스터 관리자입니다. 이는 장기 실행 서비스와 배치 작업을 모두 처리하며, 리소스 회수(reclamation) 및 세밀한 공유와 같은 기술을 통해 높은 신뢰성, 확장성 및 효율적인 리소스 활용에 중점을 둡니다. Borg는 결함 허용을 위해 복제되는 논리적으로 중앙 집중화된 기본 스케줄러를 사용합니다. 분산 에이전트(Borglets)는 각 머신에서 세밀한 리소스 관리를 제공합니다. Borg는 애플리케이션 우선순위와 할당량을 기반으로 리소스를 할당하고, 선언적 작업 사양을 지원하며, 롤링 업데이트 및 상태 확인과 같은 기능을 제공합니다.

**이 논문을 선정한 이유:** Borg는 Kubernetes의 토대를 마련했습니다. 수동으로 관리되던 서버들에게 보내는 메시지는 간단했습니다. "너희는 동화될 것이다!(You will be assimilated)"

## 12.16 사이트 신뢰성 공학: 구글이 운영 시스템을 실행하는 방법

*Site Reliability Engineering: How Google Runs Production Systems, Betsy Beyer, Chris Jones, Jennifer Petoff, Niall Richard Murphy, O’Reilly (2016) [16]*

수많은 서버에 걸친 수천 개의 서비스를 관리하는 것은 예술이 아니라 과학이어야 합니다. 사이트 신뢰성 공학(SRE)은 소프트웨어 엔지니어링 원칙을 인프라 및 운영 작업에 통합하여 자동화, 신뢰성 및 확장성에 중점을 둡니다. 이는 신뢰할 수 있고 확장 가능한 소프트웨어 시스템을 구축하고 유지하려는 조직을 위한 청사진을 제공합니다. 결정적으로, 절대적인 신뢰성은 목표가 아닙니다. 오히려 신뢰성과 속도(velocity)의 균형을 맞추고 비난 없는 포스트모텀(blameless postmortems)을 통해 실패로부터 배우는 것입니다.

**이 논문을 선정한 이유:** 모든 사람이 책 전체를 읽을 것이라고 기대하지는 않지만, Ben Treynor Sloss가 쓴 서문 [17] "희망은 전략이 아니다(Hope is not a strategy)"는 반드시 읽어야 합니다.

## 12.17 텐서 처리 장치(TPU)의 데이터 센터 내 성능 분석

*In-Data center Performance Analysis of a Tensor Processing Unit, Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, et al, ISCA (2017), pp. 1–12 [18]*

이 논문은 AI 추론 워크로드를 가속화하기 위해 대규모로 배포된 최초의 맞춤형 ASIC인 구글의 1세대 텐서 처리 장치(TPU)를 공개했습니다. 저자들은 TPU의 아키텍처, 특히 대형 행렬 곱셈 유닛과 딥러닝 계산에 최적화된 온칩 메모리에 대해 설명합니다. TPU는 동시대의 CPU 및 GPU에 비해 프로덕션 AI 워크로드에서 훨씬 더 높은 성능과 에너지 효율성을 보여줍니다. 이 분석은 중요한 대규모 워크로드에 대한 도메인 특화 하드웨어 가속의 이점을 강조하고, 머신러닝 추론 작업을 위한 처리량 및 에너지 효율성의 실질적인 개선을 뒷받침하는 주요 학습 내용을 요약합니다.

**이 논문을 선정한 이유:** 원래 TPU 논문의 결론은 다음과 같습니다. "제품 간의 자릿수(Order-of-magnitude) 차이는 컴퓨터 아키텍처에서 드문 일이며, 이는 TPU가 도메인 특화 아키텍처의 전형이 되는 결과를 낳을 수 있습니다."

## 12.18 킬러 마이크로세컨드의 습격

*Attack of the killer microseconds, Luiz André Barroso, Mike Marty, David Patterson, Parthasarathy Ranganathan, Communications of the ACM, 60(4) (2017), pp. 48-54 [19]*

WSC는 더 빠른 데이터 센터 네트워크, 새로운 메모리 계층 구조 및 가속기에 힘입어 마이크로초(microsecond) 수준의 지연시간에 최적화해야 합니다. 두 가지 사례 연구(빠른 데이터 센터 네트워크를 낭비하는 방법과 빠른 데이터 센터 프로세서를 낭비하는 방법)를 사용하여, 이 논문은 나노초 및 밀리초 규모의 이벤트를 대상으로 하는 시스템 최적화가 마이크로초 범위의 이벤트에는 어떻게 부적절한지 지적합니다. 새로운 기술은 마이크로초 지연시간에서 고성능을 달성할 수 있지만 하드웨어, 커널 및 애플리케이션 계층 전반에 걸친 공동 설계(co-design)가 필요합니다.

**이 논문을 선정한 이유:** 앞서 논의한 에너지 비례성 논문과 마찬가지로, 이 논문은 WSC에 대한 단순하지만 중요한 과제를 강조했으며 WSC 시스템 스택에서 많은 후속 최적화로 이어졌습니다.

## 12.19 안드로메다: 클라우드 네트워크 가상화에서의 성능, 격리, 속도

*Andromeda: Performance, Isolation, and Velocity at Scale in Cloud Network Virtualization, Mike Dalton, David Schultz, Ahsan Arefin, Alex Docauer et al, 15th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2018, pp. 373–387. [20]*

안드로메다(Andromeda)는 구글 클라우드 플랫폼의 네트워크 가상화 스택을 설명합니다. 그 SDN(소프트웨어 정의 네트워크) 설계는 가상 머신을 위한 고성능의 안전한 네트워킹을 제공합니다. 안드로메다는 제어 평면을, 일부는 호스트에 구현되고 일부는 하드웨어로 오프로드된 분산 데이터 평면과 분리합니다. 이 아키텍처는 네트워크 서비스(VPC, 방화벽, 로드 밸런서)의 신속한 프로비저닝을 가능하게 하고, 베어메탈 네트워크에 근접한 성능을 제공하며, 테넌트 간의 강력한 보안 격리를 보장합니다. 이 설계는 제품 속도와 운영 민첩성을 우선시하여 새로운 네트워크 기능을 단순화하고 인프라를 효율적으로 확장합니다.

**이 논문을 선정한 이유:** 네트워크 가상화는 고속 클러스터 네트워크의 모든 단일 패킷을 건드리기 때문에 하드웨어로 구현되기를 원합니다. 하드웨어와 소프트웨어 전반에 걸쳐 기능을 신중하게 분할함으로써 안드로메다는 두 가지의 장점을 결합합니다. 말처럼 쉬운 일은 아닙니다.

## 12.20 Snap: 호스트 네트워킹에 대한 마이크로커널 접근 방식

*Snap: a Microkernel Approach to Host Networking, Michael Marty, Marc de Kruijf, Jacob Adriaens, Christopher Alfeld et al, In ACM SIGOPS 27th Symposium on Operating Systems Principles, ACM, New York, NY, USA (2019), pp. 416–431 [21]*

네트워크 하드웨어는 수백 Gbps를 제공하지만 기존 커널 API는 이 용량을 실제로 사용하는 데 너무 높은 오버헤드를 부과합니다. Snap은 마이크로커널에서 영감을 받은 아키텍처를 사용하여 커널 외부에서 고성능 호스트 네트워킹 스택을 구현합니다. 이러한 모듈성은 구성 요소를 격리하여 보안을 강화하고 코어 커널을 수정하지 않고도 특수 네트워크 기능(예: 사용자 정의 프로토콜, 하드웨어 오프로드)을 배포할 수 있게 합니다.

**이 논문을 선정한 이유:** 현대 호스트 네트워킹에 대한 훌륭한 개요이며, 사용자 공간 패킷 처리가 대규모 환경에서 어떻게 사용될 수 있는지 보여줍니다.

## 12.21 썬더볼트: 대규모 처리량 최적화 및 서비스 품질 인식 전력 제한 (Power Capping)

*Thunderbolt: Throughput-Optimized, Quality-of-Service-Aware Power Capping at Scale, Shaohong Li, Xi Wang, Xiao Zhang, Vasileios Kontorinis, Sreekumar Kodakara, David Lo, Parthasarathy Ranganathan, 14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20), {USENIX} Association (2020), pp. 1241-1255 [22]*

썬더볼트(Thunderbolt)는 과도하게 오버서브스크립션(oversubscribed)된 대형 데이터 센터에서 전력 제한(power caps)을 시행합니다. 워크로드 분류 및 QoS 피드백을 사용하여 전력 제한을 지능적으로 적용하고, 지연 시간에 민감한 작업에 우선순위를 두며 백그라운드 또는 배치 작업을 선택적으로 조절합니다. 이 접근 방식은 유용한 작업을 극대화하고 가장 중요한 사용자에게 전력 제한의 존재를 숨겨, 필요할 때 전력 인프라를 한계치에 가깝게 안전하게 실행함으로써 데이터 센터 효율성을 향상시킵니다. (시간이 있다면 구글에서 전력 오버서브스크립션을 소개한 원래의 "웨어하우스 규모 컴퓨터를 위한 전력 프로비저닝(Power provisioning for a warehouse-sized computer)" 논문 [23]과 함께 읽어보는 것이 좋습니다.)

**이 논문을 선정한 이유:** 오버서브스크립션을 통해 두 마리 토끼를 다 잡을 수 있지만, 이를 위해서는 매우 신중하게 설계된 제어 평면이 필요합니다. 이는 획기적인 전력 활용률 개선으로 이어지므로 노력할 가치가 있습니다.

## 12.22 Swift: 데이터 센터 혼잡 제어를 위한 지연 시간의 단순성과 효과성

*Swift: Delay is Simple and Effective for Congestion Control in the Data center, Gautam Kumar, Nandita Dukkipati, Keon Jang, Hassan Wassel, et al, SIGCOMM 2020 (2020), pp. 583–598 [24]*

TCP는 패킷 손실을 사용하여 혼잡을 감지함으로써 인터넷 혼잡을 효과적으로 처리해 왔습니다. 그러나 매우 빠른 속도와 매우 낮은 지연 시간 및 작은 버퍼 스위치가 특징인 데이터 센터 네트워크에서는 잘 작동하지 않습니다. Swift는 패킷 지연에 대한 정확한 측정을 기반으로 하는 매우 간단한 프로토콜이 다른 모든 프로토콜보다 우수함을 보여줍니다. 간단한 엔드 호스트 기반 접근 방식은 극도로 낮은 네트워크 큐, 높은 링크 활용률 및 빠른 수렴을 달성합니다.

**이 논문을 선정한 이유:** Swift는 매우 단순하면서도 성능이 뛰어나 WSC 네트워크가 할 수 있는 일에 대한 사람들의 기대치를 완전히 바꿔 놓았습니다. 사실 처음에는 약간의 혼란을 야기하기도 했는데, 활용률이 높은 링크가 손실(loss)을 0으로 보고했기 때문에 모니터링 실패 버그로 잘못 신고된 적이 두 번이나 있었습니다.

## 12.23 웨어하우스 규모 비디오 가속: 야생에서의 공동 설계 및 배포

*Warehouse-Scale Video Acceleration: Co-design and Deployment in the Wild, Parthasarathy Ranganathan, Danner Stodolsky, Jeff Calow, Jeremy Dorfman et al, Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Association for Computing Machinery, New York, NY, USA (2021), pp. 600-615 [25]*

비디오 코딩 유닛(VCU)은 비디오 처리를 획기적으로 가속화하며, AI 가속기 다음으로 가장 인기 있는 WSC 가속기입니다. 저자들은 VCU 하드웨어, 소프트웨어 드라이버, 서버 통합 및 작업 관리 시스템뿐만 아니라 결함 허용, 모니터링 및 프로그래밍 모델을 포함하여 거대한 범용 컴퓨팅 환경에 특수 하드웨어를 통합하는 과제에 대해 설명합니다.

**이 논문을 선정한 이유:** VCU는 웨어하우스 규모 가속기를 위한 하드웨어-소프트웨어 공동 설계의 대표적인 예입니다. 그리고 기술 에미상을 수상했습니다!

## 12.24 주피터의 진화: 광 회로 스위치와 소프트웨어 정의 네트워킹을 통한 구글 데이터 센터 네트워크 변혁

*Jupiter Evolving: Transforming Google’s Data center Network via Optical Circuit Switches and Software-Defined Networking, Leon Poutievski, Omid Mashayekhi, Joon Ong, Arjun Singh, et al, Proceedings of ACM SIGCOMM 2022, pp. 1–17 [26]*

전통적인 Clos 토폴로지에서 시작하여 Jupiter 네트워크는 네트워크 토폴로지를 동적으로 재구성하기 위해 전용 광학 경로를 제공하는 MEMS 기반 광 회로 스위치(OCS)를 갖춘 보다 유연한 직접 연결 아키텍처로 진화했습니다. WSC 네트워크는 지속적으로 성장하고 변화하므로 Jupiter 설계의 많은 측면은 유연성, 증분 업데이트, 정교한 트래픽 엔지니어링 및 동적으로 재구성된 트래픽 흐름을 지원합니다. 자동화된 네트워크 운영은 라이브 서비스를 중단하지 않고 증분 용량 제공 및 토폴로지 엔지니어링을 지원합니다.

**이 논문을 선정한 이유:** 구글의 데이터 센터 네트워크에 관한 논문을 하나만 골라야 한다면 바로 이 논문입니다. 원래의 "Jupiter Rising" 논문 [27]에 이어, 이 논문은 기술적 세부 사항을 설명하는 것은 물론 디자인이 오늘날의 모습으로 진화한 과정을 설명하는 회고록으로서도 훌륭합니다.

## 12.25 Pathways: 머신러닝을 위한 비동기 분산 데이터플로우

*Pathways: Asynchronous Distributed Dataflow for ML, Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, et al, MLSys 2022 (2022) [28]*

전통적인 ML 훈련은 대규모 클러스터의 가속기를 매우 동기적이고 록스텝(lock-step) 방식으로 실행하며, 모든 가속기가 동일해야 합니다. 그러나 모델이 너무 커져서 단일 클러스터에 맞지 않게 되었습니다. Pathways는 가속기 시스템이 더 큰 규모의 AI 모델을 훈련하고 제공하기 위한 새로운 오케스트레이션 계층입니다. Pathways 아키텍처는 비동기 분산 데이터플로우를 활용하고 단일 컨트롤러 모델 및 중앙 집중식 스케줄링과 결합하여 이기종 하드웨어 가속기(CPU, GPU, TPU) 전반에 걸쳐 계산을 라우팅합니다. 이 접근 방식을 통해 현재의 조밀한(dense) 단일 작업 모델을 넘어, 동시에 많은 작업을 학습하고 효율성을 위해 희소성(sparsity)을 활용할 수 있는 희소(sparse) 다중 작업 모델로 이동할 수 있습니다.

**이 논문을 선정한 이유:** 이 논문은 딥러닝을 확장하는 데 필요한 모델-시스템 공동 설계 및 분산 시스템 혁신의 훌륭한 예입니다.

---

# 참고 문헌 (References)

1. S. Brin and L. Page, “The anatomy of a large-scale hypertextual web search engine,” *Computer Networks and ISDN Systems*, vol. 30, no. 1-7, pp. 107–117, 1998.
2. L. Barroso, J. Dean, and U. Hölzle, “Web search for a planet: The Google cluster architecture,” *IEEE Micro*, vol. 23, no. 2, pp. 22–28, 2003.
3. S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,” in *Proceedings of the nineteenth ACM Symposium on Operating systems principles*, ser. SOSP ’03, New York, NY, USA: Association for Computing Machinery, Oct. 19, 2003, pp. 29–43.
4. J. Dean and S. Ghemawat, “MapReduce: Simplified data processing on large clusters,” 1, vol. 51, New York, NY, USA: Association for Computing Machinery, Jan. 1, 2008, pp. 107–113.
5. F. Chang, J. Dean, S. Ghemawat, et al., “Bigtable: A distributed storage system for structured data,” Article 4, vol. 26, New York, NY, USA: Association for Computing Machinery, Jun. 1, 2008, pp. 1–26.
6. M. Burrows, “The Chubby lock service for loosely-coupled distributed systems,” in *Proceedings of the 7th Symposium on Operating Systems Design and Implementation*, ser. OSDI ’06, USA: USENIX Association, Nov. 6, 2006, pp. 335–350.
7. L. A. Barroso and U. Hölzle, “The case for energy-proportional computing,” *Computer*, vol. 40, no. 12, pp. 33–37, 2007.
8. S. Melnik, A. Gubarev, J. J. Long, et al., “Dremel: Interactive analysis of web-scale datasets,” 1-2, vol. 3, VLDB Endowment, Sep. 1, 2010, pp. 330–339.
9. B. H. Sigelman, L. A. Barroso, M. Burrows, et al., “Dapper, a large-scale distributed systems tracing infrastructure,” 2010.
10. G. Ren, E. Tune, T. Moseley, et al., “Google-Wide Profiling: A continuous profiling infrastructure for data centers,” *IEEE Micro*, vol. 30, no. 4, pp. 65–79, 2010.
11. J. C. Corbett, J. Dean, M. Epstein, et al., “Spanner: Google’s globally distributed database,” Article 8, vol. 31, New York, NY, USA: Association for Computing Machinery, Aug. 1, 2013, pp. 1–22.
12. S. Jain, A. Kumar, S. Mandal, et al., “B4: Experience with a globally-deployed software defined WAN,” in *Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM*, ser. SIGCOMM ’13, New York, NY, USA: Association for Computing Machinery, Aug. 27, 2013, pp. 3–14.
13. J. Dean and L. A. Barroso, “The tail at scale,” 2, vol. 56, New York, NY, USA: Association for Computing Machinery, Feb. 1, 2013, pp. 74–80.
14. S. Kanev, J. P. Darago, K. Hazelwood, et al., “Profiling a warehouse-scale computer,” in *Proceedings of the 42nd Annual International Symposium on Computer Architecture*, ser. ISCA ’15, New York, NY, USA: Association for Computing Machinery, Jun. 13, 2015, pp. 158–169.
15. A. Verma, L. Pedrosa, M. Korupolu, et al., “Large-scale cluster management at Google with Borg,” in *Proceedings of the Tenth European Conference on Computer Systems*, ser. EuroSys ’15, New York, NY, USA: Association for Computing Machinery, Apr. 17, 2015, pp. 1–17.
16. B. Beyer, C. Jones, J. Petoff, and N. R. Murphy, *Site Reliability Engineering: How Google runs production systems*. ” O’Reilly Media.”, 2016.
17. B. T. Sloss, *Introduction to Site Reliability Engineering*. ” O’Reilly Media.”, 2016.
18. N. P. Jouppi, C. Young, N. Patil, et al., “In-data center performance analysis of a Tensor Processing Unit,” in *Proceedings of the 44th Annual International Symposium on Computer Architecture*, ser. ISCA ’17, New York, NY, USA: Association for Computing Machinery, Jun. 24, 2017, pp. 1–12.
19. L. Barroso, M. Marty, D. Patterson, and P. Ranganathan, “Attack of the killer microseconds,” *Commun. ACM*, vol. 60, no. 4, pp. 48–54, Mar. 2017.
20. M. Dalton, D. Schultz, J. Adriaens, et al., “Andromeda: Performance, isolation, and velocity at scale in cloud network virtualization,” in *15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)*, 2018, pp. 373–387.
21. M. Marty, M. de Kruijf, J. Adriaens, et al., “Snap: A microkernel approach to host networking,” in *Proceedings of the 27th ACM Symposium on Operating Systems Principles*, ser. SOSP ’19, New York, NY, USA: Association for Computing Machinery, Oct. 27, 2019, pp. 399–413.
22. S. Li, X. Wang, F. Kalim, et al., “Thunderbolt: Throughput-optimized, quality-of-service-aware power capping at scale,” in *14th USENIX Symposium on Operating Systems Design and Implementation (OSDI20)*, 2020, pp. 1241–1255.
23. X. Fan, W.-D. Weber, and L. A. Barroso, “Power provisioning for a warehouse-sized computer,” in *Proceedings of the 34th Annual International Symposium on Computer Architecture*, ser. ISCA ’07, New York, NY, USA: Association for Computing Machinery, Jun. 9, 2007, pp. 13–23.
24. G. Kumar, N. Dukkipati, K. Jang, et al., “Swift: Delay is simple and effective for congestion control in the data center,” in *Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the applications, technologies, architectures, and protocols for computer communication*, ser. SIGCOMM ’20, New York, NY, USA: Association for Computing Machinery, Jul. 30, 2020, pp. 514–528.
25. P. Ranganathan, D. Stodolsky, J. Calow, et al., “Warehouse-scale video acceleration: Co-design and deployment in the wild,” in *Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems*, ser. ASPLOS ’21, New York, NY, USA: Association for Computing Machinery, Apr. 17, 2021, pp. 600–615.
26. L. Poutievski, O. Mashayekhi, J. Ong, et al., “Jupiter evolving: Transforming google’s data center network via optical circuit switches and software-defined networking,” in *Proceedings of the ACM SIGCOMM 2022 Conference*, ser. SIGCOMM ’22, New York, NY, USA: Association for Computing Machinery, Aug. 22, 2022, pp. 66–85.
27. A. Singh, J. Ong, A. Agarwal, et al., “Jupiter Rising: a decade of Clos topologies and centralized control in Google’s data center network,” 4, vol. 45, New York, NY, USA: Association for Computing Machinery, Aug. 17, 2015, pp. 183–197.
28. P. Barham, A. Chowdhery, J. Dean, et al., “Pathways: Asynchronous distributed dataflow for ML,” in *Proceedings of the Fifth Conference on Machine Learning and Systems, MLSys 2022, Santa Clara, CA, USA, August 29 - September 1, 2022*, D. Marculescu, Y. Chi, and C. Wu, Eds., mlsys.org, 2022.

---
**Open Access** This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.
The images or other third party material in this chapter are included in the chapter’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.

© The Author(s) 2026
L. A. Barroso et al., The Data Center as a Computer, Synthesis Lectures on Computer Architecture, https://doi.org/10.1007/978-3-031-99489-0_12